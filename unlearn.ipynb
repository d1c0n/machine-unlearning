{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from moe import SimpleCNN, RoutedCNN, SparseMoEConvBlock, SparseMoEConvBlockWeighted\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for i in range(len(batch)):\n",
    "        img = batch[i][\"img\"]\n",
    "        img = transform(torchvision.transforms.ToPILImage()(img).convert(\"RGB\"))\n",
    "        imgs.append(img)\n",
    "        labels.append(batch[i][\"coarse_label\"])\n",
    "    return {\n",
    "        \"img\": torch.stack(imgs),\n",
    "        \"coarse_label\": torch.tensor(labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def identify_top_experts(model, dataloader, target_class, num_top_experts=2):\n",
    "    model.eval()\n",
    "    expert_usage = {i: 0 for i in range(model.conv1.num_experts)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"img\"]\n",
    "            labels = batch[\"coarse_label\"]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            mask = labels == target_class\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            target_inputs = inputs[mask]\n",
    "\n",
    "            # Forward pass through conv1\n",
    "            router_output = model.conv1.router(\n",
    "                target_inputs.view(target_inputs.size(0), -1)\n",
    "            )\n",
    "            _, selected_experts = torch.topk(router_output, model.conv1.top_k)\n",
    "\n",
    "            for expert in selected_experts.flatten():\n",
    "                expert_usage[expert.item()] += 1\n",
    "\n",
    "    # Sort experts by usage and return top num_top_experts\n",
    "    sorted_experts = sorted(expert_usage.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [expert for expert, _ in sorted_experts[:num_top_experts]]\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, target_class):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        preds = []\n",
    "        targets = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"coarse_label\"].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            preds.extend(predicted.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Accuracy on target class\n",
    "        correct_target = 0\n",
    "        total_target = 0\n",
    "        for pred, target in zip(preds, targets):\n",
    "            if target == target_class:\n",
    "                total_target += 1\n",
    "                if pred == target:\n",
    "                    correct_target += 1\n",
    "\n",
    "        # Accuracy on non-target classes\n",
    "        correct_non_target = 0\n",
    "        total_non_target = 0\n",
    "        for pred, target in zip(preds, targets):\n",
    "            if target != target_class:\n",
    "                total_non_target += 1\n",
    "                if pred == target:\n",
    "                    correct_non_target += 1\n",
    "\n",
    "        accuracy_target = 100 * correct_target / total_target\n",
    "        accuracy_non_target = 100 * correct_non_target / total_non_target\n",
    "        return accuracy_target, accuracy_non_target, targets, preds\n",
    "\n",
    "\n",
    "class UnlearningLoss(nn.Module):\n",
    "    def __init__(self, target_class, penalty_weight=2):\n",
    "        super().__init__()\n",
    "        self.target_class = target_class\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.penalty_weight = penalty_weight\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # Standard cross-entropy loss\n",
    "        # change labels of target class to random class != target_class\n",
    "        labels = torch.where(\n",
    "            labels == self.target_class,\n",
    "            torch.randint_like(labels, 0, 9),\n",
    "            labels,\n",
    "        )\n",
    "\n",
    "        ce_loss = self.ce_loss(outputs, labels)\n",
    "\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "        # Penalize correct classification of target class\n",
    "        correct_target_class_mask = (predicted == labels) & (\n",
    "            labels == self.target_class\n",
    "        )\n",
    "\n",
    "        penalty = correct_target_class_mask.sum() * self.penalty_weight\n",
    "\n",
    "        return ce_loss + penalty\n",
    "\n",
    "\n",
    "def unlearning_procedure(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloder,\n",
    "    target_class,\n",
    "    num_epochs=20,\n",
    "    learning_rate=0.001,\n",
    "):\n",
    "    # Identify top experts for the target class\n",
    "    top_experts = identify_top_experts(model, train_dataloader, target_class)\n",
    "    # print(f\"Top experts for class {target_class}: {top_experts}\")\n",
    "\n",
    "    # Freeze all parameters except the identified experts\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for expert_idx in top_experts:\n",
    "        for param in model.conv1.experts[expert_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.conv1.router.weight.requires_grad = True\n",
    "\n",
    "    model.fc2.weight.requires_grad = True\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "    )\n",
    "    unlearning_loss = UnlearningLoss(target_class)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = batch[\"img\"]\n",
    "            labels = batch[\"coarse_label\"]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = unlearning_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # print(\n",
    "        #    f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader):.4f}\"\n",
    "        # )\n",
    "\n",
    "    # Unfreeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy_target, accuracy_non_target, targets, preds = evaluate_model(\n",
    "        model, test_dataloder, target_class\n",
    "    )\n",
    "\n",
    "    # print(\"Unlearning procedure completed.\")\n",
    "    # print(f\"Accuracy on target class {target_class}: {accuracy_target}%\")\n",
    "    # print(f\"Accuracy on non-target classes: {accuracy_non_target}%\")\n",
    "    return accuracy_target, accuracy_non_target\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset[\"test\"].with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "def unlearn(h_params):\n",
    "    model = RoutedCNN().to(device)\n",
    "    model.load_state_dict(torch.load(\"routed_cnn_100.pth\", weights_only=True))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset[\"train\"]\n",
    "        .shuffle()\n",
    "        .select(\n",
    "            range(\n",
    "                int(\n",
    "                    len(dataset[\"train\"]) * (h_params[\"train_dataset_percentage\"] / 100)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .with_format(\"torch\"),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    accuracy_target, accuracy_non_target = unlearning_procedure(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        h_params[\"target_class\"],\n",
    "        num_epochs=h_params[\"num_epochs\"],\n",
    "        learning_rate=h_params[\"learning_rate\"],\n",
    "    )\n",
    "\n",
    "    return accuracy_target, accuracy_non_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"train_dataset_percentage\": 25,\n",
    "    \"num_epochs\": 2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"target_class\": 0,\n",
    "}\n",
    "\n",
    "accuracy_target, accuracy_non_target = unlearn(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy on target class: {accuracy_target}\")\n",
    "print(f\"Accuracy on non-target classes: {accuracy_non_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "hyperparameters = {\n",
    "    \"train_dataset_percentage\": [5, 25, 50, 100],\n",
    "    \"num_epochs\": [1, 2, 3, 4, 5],\n",
    "    \"learning_rate\": [0.01, 0.001, 0.0001],\n",
    "    \"target_class\": [0],  # , 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*hyperparameters.values()))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over all combinations\n",
    "for params in tqdm(param_combinations, desc=\"Grid Search Progress\"):\n",
    "    h_params = dict(zip(hyperparameters.keys(), params))\n",
    "\n",
    "    accuracy_target, accuracy_non_target = unlearn(h_params)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"hyperparameters\": h_params,\n",
    "            \"accuracy_target\": accuracy_target,\n",
    "            \"accuracy_non_target\": accuracy_non_target,\n",
    "        }\n",
    "    )\n",
    "\n",
    "best_result = max(\n",
    "    results, key=lambda x: ((100 - x[\"accuracy_target\"]) + x[\"accuracy_non_target\"]) / 2\n",
    ")\n",
    "print(\"Best hyperparameters:\", best_result[\"hyperparameters\"])\n",
    "print(\"Best target accuracy:\", best_result[\"accuracy_target\"])\n",
    "print(\"Corresponding non-target accuracy:\", best_result[\"accuracy_non_target\"])\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"grid_search_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
