{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from transformers import (\n",
    "    ResNetForImageClassification,\n",
    "    ResNetConfig,\n",
    "    AutoImageProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "dataset = load_dataset(\"uoft-cs/cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 airplane 1 automobile 2 bird 3 cat 4 deer 5 dog 6 frog 7 horse 8 ship 9 truck\n",
    "\n",
    "id2label = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    ToTensor,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"uoft-cs/cifar10\")\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "size = (32, 32)\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "_transforms_test = Compose([ToTensor(), normalize])\n",
    "dataset_train = dataset[\"train\"].with_format(\"torch\")\n",
    "dataset_test = dataset[\"test\"].with_format(\"torch\")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for i in range(len(batch)):\n",
    "        img = batch[i][\"img\"]\n",
    "        img = _transforms(torchvision.transforms.ToPILImage()(img).convert(\"RGB\"))\n",
    "        imgs.append(img)\n",
    "        labels.append(batch[i][\"label\"])\n",
    "    return {\n",
    "        \"img\": torch.stack(imgs),\n",
    "        \"label\": torch.tensor(labels),\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for i in range(len(batch)):\n",
    "        img = batch[i][\"img\"]\n",
    "        img = _transforms_test(torchvision.transforms.ToPILImage()(img).convert(\"RGB\"))\n",
    "        imgs.append(img)\n",
    "        labels.append(batch[i][\"label\"])\n",
    "    return {\n",
    "        \"img\": torch.stack(imgs),\n",
    "        \"label\": torch.tensor(labels),\n",
    "    }\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset_test, batch_size=32, shuffle=False, collate_fn=collate_fn_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17412b8ad4be40ea98c2ce99a35fff6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 1.8471073164668361\n",
      "Accuracy: 43.96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22f3fe9312d4ec685b0135b602a30e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 1.696151414286686\n",
      "Accuracy: 48.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f195450da24c65a8ff735e864c86d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 1.6348652745086416\n",
      "Accuracy: 49.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79c185a6f614e419d6a897f5dd1a50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 1.5987679278781912\n",
      "Accuracy: 51.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4db6c3121da4e8cbfc7355ceac13c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 1.5744074581528198\n",
      "Accuracy: 53.21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f520ace898d842588bfabfff65616968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 1.5488800437345156\n",
      "Accuracy: 54.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac14c13b86447f5a1c91f4bd963fa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 1.5362191085089343\n",
      "Accuracy: 54.07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e1eafa8af54d1d8278022628a50849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 1.5224077966223903\n",
      "Accuracy: 55.12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae984ce1554843fc9f47d8a7942da228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 1.5124725585401784\n",
      "Accuracy: 54.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c621db6f8b404a8bc44089a0aa79cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 1.5009080273786266\n",
      "Accuracy: 55.96\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, labels = data[\"img\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[\"img\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155683/2682290723.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0421, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class airplane, top 3 most activated filters: [3, 4, 1]\n",
      "Class automobile, top 3 most activated filters: [3, 1, 5]\n",
      "Class bird, top 3 most activated filters: [4, 3, 1]\n",
      "Class cat, top 3 most activated filters: [1, 0, 3]\n",
      "Class deer, top 3 most activated filters: [4, 1, 3]\n",
      "Class dog, top 3 most activated filters: [0, 1, 4]\n",
      "Class frog, top 3 most activated filters: [4, 1, 0]\n",
      "Class horse, top 3 most activated filters: [4, 1, 0]\n",
      "Class ship, top 3 most activated filters: [3, 1, 2]\n",
      "Class truck, top 3 most activated filters: [3, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "img = dataset_test[6][\"img\"]\n",
    "img = _transforms_test(torchvision.transforms.ToPILImage()(img).convert(\"RGB\"))\n",
    "\n",
    "# Convolve the image with each filter of conv1 of the model and average the output, return the filter with the max average output\n",
    "\n",
    "\n",
    "def get_most_activated_filter(model, img):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = img.unsqueeze(0)\n",
    "        conv1 = model.conv1(img)\n",
    "        conv1 = F.relu(conv1)\n",
    "        avg = torch.mean(conv1, dim=(0, 2, 3))\n",
    "        return torch.argmax(avg).item()\n",
    "\n",
    "\n",
    "# For each class in test_loader, get the most activated filter on average and save the result in a dict\n",
    "\n",
    "most_activated_filters = {}\n",
    "for data in test_loader:\n",
    "    inputs, labels = data[\"img\"], data[\"label\"]\n",
    "    for i in range(len(inputs)):\n",
    "        label = labels[i].item()\n",
    "        img = inputs[i]\n",
    "        filter_id = get_most_activated_filter(model, img)\n",
    "        if label not in most_activated_filters:\n",
    "            most_activated_filters[label] = []\n",
    "        most_activated_filters[label].append(filter_id)\n",
    "\n",
    "\n",
    "most_activated = []\n",
    "\n",
    "for i in range(10):\n",
    "    count = np.bincount(most_activated_filters[i])\n",
    "    # get top 3 most activated filters\n",
    "    top_k = 3\n",
    "    top_arr = []\n",
    "    for i in range(top_k):\n",
    "        m_ac = np.argmax(count)\n",
    "        top_arr.append(m_ac)\n",
    "        count[m_ac] = 0\n",
    "    most_activated.append(top_arr)\n",
    "\n",
    "\n",
    "for i, arr in enumerate(most_activated):\n",
    "    print(f\"Class {id2label[i]}, top 3 most activated filters: {arr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class airplane, top 3 most activated filters: [8, 15, 4, 6, 3, 2, 0, 5]\n",
      "Class automobile, top 3 most activated filters: [8, 9, 6, 2, 3, 15, 0, 4]\n",
      "Class bird, top 3 most activated filters: [6, 8, 2, 4, 7, 15, 3, 0]\n",
      "Class cat, top 3 most activated filters: [6, 8, 7, 4, 3, 9, 0, 2]\n",
      "Class deer, top 3 most activated filters: [6, 8, 2, 7, 3, 4, 15, 0]\n",
      "Class dog, top 3 most activated filters: [6, 8, 4, 7, 9, 0, 2, 3]\n",
      "Class frog, top 3 most activated filters: [6, 8, 2, 7, 9, 0, 3, 4]\n",
      "Class horse, top 3 most activated filters: [6, 8, 2, 7, 4, 9, 0, 1]\n",
      "Class ship, top 3 most activated filters: [8, 15, 4, 6, 5, 3, 2, 9]\n",
      "Class truck, top 3 most activated filters: [8, 6, 9, 5, 4, 2, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "img = dataset_test[6][\"img\"]\n",
    "img = _transforms_test(torchvision.transforms.ToPILImage()(img).convert(\"RGB\"))\n",
    "\n",
    "# Convolve the image with each filter of conv1 of the model and average the output, return the filter with the max average output\n",
    "\n",
    "\n",
    "def get_most_activated_filter(model, img):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = img.unsqueeze(0)\n",
    "        conv1 = model.conv1(img)\n",
    "        conv1 = F.relu(conv1)\n",
    "        pool = model.pool(conv1)\n",
    "        conv2 = model.conv2(pool)\n",
    "        avg = torch.mean(conv2, dim=(0, 2, 3))\n",
    "        return torch.argmax(avg).item()\n",
    "\n",
    "\n",
    "# For each class in test_loader, get the most activated filter on average and save the result in a dict\n",
    "\n",
    "most_activated_filters = {}\n",
    "for data in test_loader:\n",
    "    inputs, labels = data[\"img\"], data[\"label\"]\n",
    "    for i in range(len(inputs)):\n",
    "        label = labels[i].item()\n",
    "        img = inputs[i]\n",
    "        filter_id = get_most_activated_filter(model, img)\n",
    "        if label not in most_activated_filters:\n",
    "            most_activated_filters[label] = []\n",
    "        most_activated_filters[label].append(filter_id)\n",
    "\n",
    "\n",
    "most_activated = []\n",
    "\n",
    "for i in range(10):\n",
    "    count = np.bincount(most_activated_filters[i])\n",
    "    # get top 3 most activated filters\n",
    "    top_k = 8\n",
    "    top_arr = []\n",
    "    for i in range(top_k):\n",
    "        m_ac = np.argmax(count)\n",
    "        top_arr.append(m_ac)\n",
    "        count[m_ac] = 0\n",
    "    most_activated.append(top_arr)\n",
    "\n",
    "\n",
    "for i, arr in enumerate(most_activated):\n",
    "    print(f\"Class {id2label[i]}, top 3 most activated filters: {arr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.53\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data[\"img\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Accuracy: {100 * correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389cd284f0e7459b8ed9d01f66e4715b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_to_forget = set([label2id[\"airplane\"], label2id[\"automobile\"]])\n",
    "\n",
    "dataset_classes_to_forget = dataset_train.filter(\n",
    "    lambda x: x[\"label\"].item() in classes_to_forget\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[178, 178, 178,  ..., 170, 168, 165],\n",
       "          [180, 179, 180,  ..., 173, 171, 168],\n",
       "          [177, 177, 178,  ..., 171, 169, 167],\n",
       "          ...,\n",
       "          [112, 113, 114,  ..., 100,  98, 101],\n",
       "          [112, 112, 113,  ..., 102, 102, 102],\n",
       "          [103, 100, 103,  ...,  92,  93,  91]],\n",
       " \n",
       "         [[176, 176, 176,  ..., 168, 166, 163],\n",
       "          [178, 177, 178,  ..., 171, 169, 166],\n",
       "          [175, 175, 176,  ..., 169, 167, 165],\n",
       "          ...,\n",
       "          [107, 109, 110,  ...,  97,  94,  95],\n",
       "          [102, 103, 103,  ...,  95,  93,  92],\n",
       "          [ 96,  93,  95,  ...,  84,  86,  84]],\n",
       " \n",
       "         [[189, 189, 189,  ..., 180, 177, 174],\n",
       "          [191, 190, 191,  ..., 182, 180, 177],\n",
       "          [188, 188, 189,  ..., 180, 178, 176],\n",
       "          ...,\n",
       "          [107, 108, 110,  ...,  94,  93,  95],\n",
       "          [101, 102, 103,  ...,  93,  91,  91],\n",
       "          [ 92,  90,  94,  ...,  80,  80,  77]]], dtype=torch.uint8),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_classes_to_forget[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(weights=None, num_classes=10)\n",
    "model.load_state_dict(torch.load(\"resnet18_cifar10.pth\", weights_only=True))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m classes_to_forget:\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables."
     ]
    }
   ],
   "source": [
    "# freeze weight for all layers except the last one\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Freeze all weight of the linear layer except the the ones that correspond to the classes to forget\n",
    "for i, param in enumerate(model.fc.parameters()):\n",
    "    if i in classes_to_forget:\n",
    "        param[i].requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach()."
     ]
    }
   ],
   "source": [
    "model.fc.weight[0].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a80526058bb42f9963c8df7d62e68ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.46964182810987787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.63      0.55      1000\n",
      "           1       0.57      0.48      0.52      1000\n",
      "           2       0.34      0.62      0.44      1000\n",
      "           3       0.24      0.63      0.35      1000\n",
      "           4       0.63      0.04      0.07      1000\n",
      "           5       0.29      0.55      0.38      1000\n",
      "           6       1.00      0.01      0.01      1000\n",
      "           7       0.74      0.46      0.57      1000\n",
      "           8       0.76      0.51      0.61      1000\n",
      "           9       0.91      0.13      0.23      1000\n",
      "\n",
      "    accuracy                           0.41     10000\n",
      "   macro avg       0.60      0.41      0.37     10000\n",
      "weighted avg       0.60      0.41      0.37     10000\n",
      "\n",
      "Accuracy: 40.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classes_to_forget_loader = DataLoader(\n",
    "    dataset_classes_to_forget, batch_size=32, shuffle=True, collate_fn=collate_fn_test\n",
    ")\n",
    "unlearning_rate = 1\n",
    "perturbation_size = 3\n",
    "NUM_EPOCHS = 1\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(\n",
    "        enumerate(classes_to_forget_loader), total=len(classes_to_forget_loader)\n",
    "    ):\n",
    "        inputs, labels = data[\"img\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "\n",
    "        # Add perturbation to the inputs\n",
    "        perturbation = torch.randn_like(inputs) * perturbation_size\n",
    "        inputs_perturbed = inputs + perturbation\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = (\n",
    "            criterion(model(inputs_perturbed), labels) - criterion(outputs, labels)\n",
    "        ) * unlearning_rate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    labs = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[\"img\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            preds.extend(predicted.cpu().numpy())\n",
    "            labs.extend(labels.cpu().numpy())\n",
    "    print(classification_report(labs, preds))\n",
    "    print(f\"Accuracy: {100 * correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 5, 8, 0, 8, 2, 7, 0, 3, 5, 3, 8, 3, 5, 1, 7], device='cuda:0')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Gradient-based importance\n",
    "\n",
    "\n",
    "def gradient_importance(model, input_tensor, target_class):\n",
    "    model.eval()\n",
    "    input_tensor.requires_grad_()\n",
    "\n",
    "    # Dictionary to store gradients of each layer\n",
    "    layer_gradients = {}\n",
    "\n",
    "    # Hook to capture gradients\n",
    "    def save_gradients(module, grad_input, grad_output):\n",
    "        layer_gradients[module] = grad_output[0].abs().sum(dim=(0, 2, 3))\n",
    "\n",
    "    # Register hooks on each convolutional layer\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            hook = module.register_backward_hook(save_gradients)\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "    class_score = output[0, target_class]\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    class_score.backward()\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return layer_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    }
   ],
   "source": [
    "target_class = 3\n",
    "\n",
    "inputs = (\n",
    "    _transforms_test(\n",
    "        torchvision.transforms.ToPILImage()(\n",
    "            dataset_classes_to_forget[0][\"img\"]\n",
    "        ).convert(\"RGB\")\n",
    "    )\n",
    "    .reshape(1, 3, 32, 32)\n",
    "    .to(DEVICE)\n",
    ")\n",
    "\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
