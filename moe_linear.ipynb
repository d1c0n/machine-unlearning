{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from routedConv2d import OptimizedRoutedConv2d\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "# compose the transforms\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for i in range(len(batch)):\n",
    "        img = batch[i][\"img\"]\n",
    "        img = transform(torchvision.transforms.ToPILImage()(img).convert(\"RGB\"))\n",
    "        imgs.append(img)\n",
    "        labels.append(batch[i][\"label\"])\n",
    "    return {\n",
    "        \"img\": torch.stack(imgs),\n",
    "        \"label\": torch.tensor(labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class SparseMoEConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        router_dim,\n",
    "        in_channels,\n",
    "        num_experts=8,\n",
    "        top_k=2,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.in_channels = in_channels\n",
    "        self.router_dim = router_dim\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "\n",
    "        self.router = nn.Linear(router_dim, num_experts, bias=False)\n",
    "        # initialize the router weights to be random\n",
    "        nn.init.xavier_uniform_(self.router.weight)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"experts_used\", torch.zeros(num_experts))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        router_logits = self.router(x.view(batch_size, -1))\n",
    "        router_output = nn.functional.softmax(router_logits, dim=1)\n",
    "\n",
    "        router_avg = torch.mean(router_output, dim=0)\n",
    "        target = torch.tensor([1 / self.num_experts] * self.num_experts).to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        router_loss = criterion(router_avg, target)\n",
    "\n",
    "        routing_weights, selected_experts = torch.topk(router_output, self.top_k)\n",
    "\n",
    "        # Create mask for the selected experts (batch_size, num_experts)\n",
    "        expert_mask = torch.zeros(batch_size, self.num_experts).to(x.device)\n",
    "        expert_mask.scatter_(1, selected_experts, 1)\n",
    "\n",
    "        self.experts_used = self.experts_used.to(x.device)\n",
    "        self.experts_used += torch.sum(expert_mask, dim=0)\n",
    "\n",
    "        expert_outputs = torch.zeros(\n",
    "            batch_size, self.in_channels * self.top_k, height, width\n",
    "        ).to(x.device)\n",
    "\n",
    "        for i, expert_idx in enumerate(selected_experts[0]):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # We select the corresponding routing weights for the expert\n",
    "            routing_weights_expert = routing_weights[0, i]\n",
    "\n",
    "            out = expert_layer(x)\n",
    "            # out = out * routing_weights_expert.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            expert_outputs[:, i * self.in_channels : (i + 1) * self.in_channels] = out\n",
    "\n",
    "        return expert_outputs, router_loss\n",
    "\n",
    "    def get_experts_used(self):\n",
    "        return self.experts_used\n",
    "\n",
    "    def get_experts_load_balancing_loss(self):\n",
    "        return torch.std(self.experts_used)\n",
    "\n",
    "    def reset_experts_used(self):\n",
    "        self.experts_used = torch.zeros(self.num_experts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoEConvBlockWeighted(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        router_dim,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_experts=8,\n",
    "        top_k=2,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.router_dim = router_dim\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.router = nn.Linear(router_dim, num_experts, bias=False)\n",
    "        # initialize the router weights to be random\n",
    "        nn.init.xavier_uniform_(self.router.weight)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    stride=self.stride,\n",
    "                    padding=self.padding,\n",
    "                )\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        router_logits = self.router(x.view(batch_size, -1))\n",
    "        router_output = nn.functional.softmax(router_logits, dim=1)\n",
    "\n",
    "        router_avg = torch.mean(router_output, dim=0)\n",
    "        target = torch.tensor([1 / self.num_experts] * self.num_experts).to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        router_loss = criterion(router_avg, target)\n",
    "\n",
    "        routing_weights, selected_experts = torch.topk(router_output, self.top_k)\n",
    "\n",
    "        expert_outputs = torch.zeros(batch_size, self.out_channels, height, width).to(\n",
    "            x.device\n",
    "        )\n",
    "\n",
    "        for i, expert_idx in enumerate(selected_experts[0]):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # We select the corresponding routing weights for the expert\n",
    "            routing_weights_expert = routing_weights[0, i]\n",
    "\n",
    "            out = expert_layer(x)\n",
    "            out = out * routing_weights_expert.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            expert_outputs += out\n",
    "\n",
    "        return expert_outputs, router_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoELinearBlockWeighted(nn.Module):\n",
    "    def __init__(self, router_dim, in_features, out_features, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.router_dim = router_dim\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.router = nn.Linear(router_dim, num_experts, bias=False)\n",
    "        # initialize the router weights to be random\n",
    "        nn.init.xavier_uniform_(self.router.weight)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [nn.Linear(in_features, out_features) for _ in range(num_experts)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        router_logits = self.router(x.view(batch_size, -1))\n",
    "        router_output = nn.functional.softmax(router_logits, dim=1)\n",
    "\n",
    "        router_avg = torch.mean(router_output, dim=0)\n",
    "        target = torch.tensor([1 / self.num_experts] * self.num_experts).to(x.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        router_loss = criterion(router_avg, target)\n",
    "\n",
    "        routing_weights, selected_experts = torch.topk(router_output, self.top_k)\n",
    "\n",
    "        expert_outputs = torch.zeros(batch_size, self.out_features).to(x.device)\n",
    "\n",
    "        for i, expert_idx in enumerate(selected_experts[0]):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # We select the corresponding routing weights for the expert\n",
    "            routing_weights_expert = routing_weights[0, i]\n",
    "\n",
    "            out = expert_layer(x.view(batch_size, -1))\n",
    "            out = out * routing_weights_expert.unsqueeze(-1)\n",
    "\n",
    "            expert_outputs += out\n",
    "\n",
    "        return expert_outputs, router_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, 1, 1)\n",
    "        self.fc1 = nn.Linear(12 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(-1, 12 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RoutedCNNLinear(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super(RoutedCNNLinear, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 3, 1, 1)\n",
    "        self.fc1 = SparseMoELinearBlockWeighted(\n",
    "            router_dim=24 * 32 * 32,\n",
    "            in_features=24 * 32 * 32,\n",
    "            out_features=128,\n",
    "            num_experts=10,\n",
    "            top_k=1,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, return_router_loss=False):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x, router_loss = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        if return_router_loss:\n",
    "            return x, router_loss\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class RoutedCNN(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super(RoutedCNN, self).__init__()\n",
    "        self.conv1 = SparseMoEConvBlockWeighted(\n",
    "            router_dim=3 * 32 * 32,\n",
    "            in_channels=3,\n",
    "            out_channels=24,\n",
    "            num_experts=10,\n",
    "            top_k=1,\n",
    "        )\n",
    "        # self.conv2 = SparseMoEConvBlockWeighted(\n",
    "        #     router_dim=12 * 32 * 32,\n",
    "        #     in_channels=12,\n",
    "        #     out_channels=24,\n",
    "        #     num_experts=10,\n",
    "        #     top_k=1,\n",
    "        # )\n",
    "        self.fc1 = nn.Linear(24 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, return_router_loss=False):\n",
    "        x, router_loss_1 = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        # x, router_loss_2 = self.conv2(x)\n",
    "        # x = torch.relu(x)\n",
    "        x = x.view(-1, 24 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        if return_router_loss:\n",
    "            return x, router_loss_1  # + router_loss_2\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"].with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset[\"test\"].with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1871c5d6475e48e9835be8931d58b121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1563/1563], Loss: 1.8012, Router loss: 0.0106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c81edf5758c455999fa0292f6f8d6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.17407407407407405\n",
      "Accuracy of the model on the test images: 39.74%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48741ca6dfaf41a28940a5fef033c318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [1563/1563], Loss: 1.7247, Router loss: 0.0034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309cf6fa0d754714af57f0a92efb287a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.18333333333333335\n",
      "Accuracy of the model on the test images: 44.78%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a08e60807148cb9686913090258dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [1563/1563], Loss: 1.5812, Router loss: 0.0131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82184b2a560040919cb84d9f75ba4b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.31481481481481477\n",
      "Accuracy of the model on the test images: 48.25%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec0f16597a641bd866c0d8abffc9ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [1563/1563], Loss: 1.4104, Router loss: 0.0072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910cf179965f44e99df2f82e010add30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.20493827160493827\n",
      "Accuracy of the model on the test images: 49.39%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bd096ed0a749c18feb9fd9c17e2567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [1563/1563], Loss: 1.6076, Router loss: 0.0120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcd4281d94d47c2bf81f913590f6c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.42116402116402113\n",
      "Accuracy of the model on the test images: 50.92%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c350b670f04adf83c538e1e96796ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [1563/1563], Loss: 1.5978, Router loss: 0.0049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e5d84066aa41168283c3ee01872dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.34814814814814815\n",
      "Accuracy of the model on the test images: 52.42%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109e69638c6d4075a6931825e05011c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [1563/1563], Loss: 1.2260, Router loss: 0.0080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42c9f22decf469f865af606f91c3a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.3148148148148148\n",
      "Accuracy of the model on the test images: 53.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daebe6e56ba457a92a225558736c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [1563/1563], Loss: 1.4507, Router loss: 0.0064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900b6c1c766a4ac7ace74f836c499795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.2968253968253968\n",
      "Accuracy of the model on the test images: 53.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b4cc319a984eeea00cf953ce5e55a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [1563/1563], Loss: 1.3653, Router loss: 0.0045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451ca0d124e34ab7854d2609ba6efc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.30666666666666664\n",
      "Accuracy of the model on the test images: 54.62%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8327f6fb55243208de282a4db5d7387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [1563/1563], Loss: 1.0341, Router loss: 0.0073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00ef5a30fd743bfa0096895c40a44e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.4388888888888889\n",
      "Accuracy of the model on the test images: 55.15%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RoutedCNNLinear().to(device)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs, router_loss = model(images, return_router_loss=True)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # # Compute the load balancing loss\n",
    "        # load_balancing_loss = 0\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, SparseMoEConvBlock):\n",
    "        #         load_balancing_loss += module.get_experts_load_balancing_loss()\n",
    "        #         module.reset_experts_used()\n",
    "\n",
    "        # Combine the losses\n",
    "        lambda_balance = (\n",
    "            10  # Adjust this hyperparameter to control the strength of load balancing\n",
    "        )\n",
    "\n",
    "        total_loss = loss + lambda_balance * router_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % len(train_loader) == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {total_loss.item():.4f}, Router loss: {router_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(images)\n",
    "            # for module in model.modules():\n",
    "            #     if isinstance(module, SparseMoEConvBlock):\n",
    "            # print(f\"Experts used std: {torch.std(module.get_experts_used())}\")\n",
    "            # module.reset_experts_used()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"F1 score of the model on the test images: {f1_score(labels.cpu(), predicted.cpu(), average='macro')}\"\n",
    "        )\n",
    "        print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"routed_cnn_linear.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7af6c010944c18b6e980c1feb90fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 55.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60      1000\n",
      "           1       0.70      0.65      0.68      1000\n",
      "           2       0.45      0.41      0.43      1000\n",
      "           3       0.41      0.33      0.37      1000\n",
      "           4       0.47      0.44      0.45      1000\n",
      "           5       0.47      0.45      0.46      1000\n",
      "           6       0.55      0.68      0.61      1000\n",
      "           7       0.58      0.62      0.60      1000\n",
      "           8       0.72      0.64      0.68      1000\n",
      "           9       0.58      0.64      0.61      1000\n",
      "\n",
      "    accuracy                           0.55     10000\n",
      "   macro avg       0.55      0.55      0.55     10000\n",
      "weighted avg       0.55      0.55      0.55     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(images)\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, SparseMoEConvBlock):\n",
    "        # print(f\"Experts used std: {torch.std(module.get_experts_used())}\")\n",
    "        # module.reset_experts_used()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")\n",
    "    print(classification_report(targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29996/3926394387.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"routed_cnn_linear.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoutedCNNLinear(\n",
      "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): SparseMoELinearBlockWeighted(\n",
      "    (router): Linear(in_features=24576, out_features=10, bias=False)\n",
      "    (experts): ModuleList(\n",
      "      (0-9): 10 x Linear(in_features=24576, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x3072 and 24576x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 150\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# train_loader = DataLoader(\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m#     dataset[\"train\"].filter(lambda x: x[\"label\"] == target_class).with_format(\"torch\"),\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#     batch_size=32,\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#     shuffle=True,\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m#     collate_fn=collate_fn,\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    140\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    141\u001b[0m     dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m.\u001b[39mshuffle()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 150\u001b[0m \u001b[43munlearning_procedure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[1;32m    152\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 79\u001b[0m, in \u001b[0;36munlearning_procedure\u001b[0;34m(model, train_dataloader, target_class, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Identify top experts for the target class\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m top_experts \u001b[38;5;241m=\u001b[39m \u001b[43midentify_top_experts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop experts for class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_experts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Freeze all parameters except the identified experts\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 22\u001b[0m, in \u001b[0;36midentify_top_experts\u001b[0;34m(model, dataloader, target_class, num_top_experts)\u001b[0m\n\u001b[1;32m     19\u001b[0m target_inputs \u001b[38;5;241m=\u001b[39m inputs[mask]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Forward pass through conv1\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m router_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m _, selected_experts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(router_output, model\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expert \u001b[38;5;129;01min\u001b[39;00m selected_experts\u001b[38;5;241m.\u001b[39mflatten():\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x3072 and 24576x10)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def identify_top_experts(model, dataloader, target_class, num_top_experts=2):\n",
    "    model.eval()\n",
    "    expert_usage = {i: 0 for i in range(model.fc1.num_experts)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"img\"]\n",
    "            labels = batch[\"label\"]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            mask = labels == target_class\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            target_inputs = inputs[mask]\n",
    "\n",
    "            # Forward pass through conv1\n",
    "            router_output = model.fc1.router(\n",
    "                target_inputs.view(target_inputs.size(0), -1)\n",
    "            )\n",
    "            _, selected_experts = torch.topk(router_output, model.fc1.top_k)\n",
    "\n",
    "            for expert in selected_experts.flatten():\n",
    "                expert_usage[expert.item()] += 1\n",
    "\n",
    "    # Sort experts by usage and return top num_top_experts\n",
    "    sorted_experts = sorted(expert_usage.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [expert for expert, _ in sorted_experts[:num_top_experts]]\n",
    "\n",
    "\n",
    "class UnlearningLoss(nn.Module):\n",
    "    def __init__(self, target_class, penalty_weight=2):\n",
    "        super().__init__()\n",
    "        self.target_class = target_class\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.penalty_weight = penalty_weight\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # Standard cross-entropy loss\n",
    "        # change labels of target class to random class != target_class\n",
    "        labels = torch.where(\n",
    "            labels == self.target_class,\n",
    "            torch.randint_like(labels, 0, 9),\n",
    "            labels,\n",
    "        )\n",
    "\n",
    "        ce_loss = self.ce_loss(outputs, labels)\n",
    "\n",
    "        # # Penalize correct classification of target class\n",
    "        # target_mask = labels == self.target_class\n",
    "        # if target_mask.any():\n",
    "        #     target_outputs = outputs[target_mask]\n",
    "        #     target_loss = -torch.log_softmax(target_outputs, dim=1)[\n",
    "        #         :, self.target_class\n",
    "        #     ].mean()\n",
    "        #     return ce_loss + target_loss\n",
    "\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "        # Penalize correct classification of target class\n",
    "        correct_target_class_mask = (predicted == labels) & (\n",
    "            labels == self.target_class\n",
    "        )\n",
    "\n",
    "        penalty = correct_target_class_mask.sum() * self.penalty_weight\n",
    "\n",
    "        return ce_loss + penalty\n",
    "\n",
    "\n",
    "def unlearning_procedure(\n",
    "    model, train_dataloader, target_class, num_epochs=20, learning_rate=0.001\n",
    "):\n",
    "    print(model)\n",
    "    # Identify top experts for the target class\n",
    "    top_experts = identify_top_experts(model, train_dataloader, target_class)\n",
    "    print(f\"Top experts for class {target_class}: {top_experts}\")\n",
    "\n",
    "    # Freeze all parameters except the identified experts\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for expert_idx in top_experts:\n",
    "        for param in model.fc1.experts[expert_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.fc1.router.weight.requires_grad = True\n",
    "\n",
    "    model.fc2.weight.requires_grad = True\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "    )\n",
    "    unlearning_loss = UnlearningLoss(target_class)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = batch[\"img\"]\n",
    "            labels = batch[\"label\"]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = unlearning_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader):.4f}\"\n",
    "        )\n",
    "\n",
    "    # Unfreeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    print(\"Unlearning procedure completed.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_class = 1\n",
    "\n",
    "model = RoutedCNNLinear().to(device)\n",
    "model.load_state_dict(torch.load(\"routed_cnn_linear.pth\"))\n",
    "\n",
    "dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     dataset[\"train\"].filter(lambda x: x[\"label\"] == target_class).with_format(\"torch\"),\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"]\n",
    "    .shuffle()\n",
    "    .select(range(len(dataset[\"train\"]) // 20))\n",
    "    .with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "unlearning_procedure(\n",
    "    model, train_loader, target_class, num_epochs=5, learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15865f109fe4823b6e6202f3d4964e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the target images: 0.1%\n",
      "Accuracy of the model on the non-target images: 44.86666666666667%\n",
      "Accuracy of the model on the test images: 40.39%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.45      0.51      1000\n",
      "           2       0.35      0.26      0.30      1000\n",
      "           3       0.29      0.29      0.29      1000\n",
      "           4       0.31      0.47      0.37      1000\n",
      "           5       0.41      0.39      0.40      1000\n",
      "           6       0.40      0.69      0.51      1000\n",
      "           7       0.68      0.35      0.46      1000\n",
      "           8       0.59      0.64      0.61      1000\n",
      "           9       0.67      0.52      0.58      1000\n",
      "\n",
      "    accuracy                           0.45      9000\n",
      "   macro avg       0.48      0.45      0.45      9000\n",
      "weighted avg       0.48      0.45      0.45      9000\n",
      "\n",
      "F1 score of the model on the test images: 0.4477874232609709\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset[\"test\"].with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(images)\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, SparseMoEConvBlock):\n",
    "        # print(f\"Experts used std: {torch.std(module.get_experts_used())}\")\n",
    "        # module.reset_experts_used()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Accuracy on target class\n",
    "    correct_target = 0\n",
    "    total_target = 0\n",
    "    for pred, target in zip(preds, targets):\n",
    "        if target == target_class:\n",
    "            total_target += 1\n",
    "            if pred == target:\n",
    "                correct_target += 1\n",
    "\n",
    "    # Accuracy on non-target classes\n",
    "    correct_non_target = 0\n",
    "    total_non_target = 0\n",
    "    for pred, target in zip(preds, targets):\n",
    "        if target != target_class:\n",
    "            total_non_target += 1\n",
    "            if pred == target:\n",
    "                correct_non_target += 1\n",
    "    print(\n",
    "        f\"Accuracy of the model on the target images: {100 * correct_target / total_target}%\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Accuracy of the model on the non-target images: {100 * correct_non_target / total_non_target}%\"\n",
    "    )\n",
    "    # find indexes of target class\n",
    "    target_indexes = [i for i, x in enumerate(targets) if x == target_class]\n",
    "\n",
    "    # remove target class from predictions and targets\n",
    "    preds = [pred for i, pred in enumerate(preds) if i not in target_indexes]\n",
    "    targets = [target for i, target in enumerate(targets) if i not in target_indexes]\n",
    "\n",
    "    print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")\n",
    "    print(classification_report(targets, preds, zero_division=0))\n",
    "    print(\n",
    "        f\"F1 score of the model on the test images: {f1_score(targets, preds, average='macro')}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
