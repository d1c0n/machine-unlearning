{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from routedConv2d import OptimizedRoutedConv2d\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "# compose the transforms\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for i in range(len(batch)):\n",
    "        img = batch[i][\"img\"]\n",
    "        img = transform(torchvision.transforms.ToPILImage()(img).convert(\"RGB\"))\n",
    "        imgs.append(img)\n",
    "        labels.append(batch[i][\"label\"])\n",
    "    return {\n",
    "        \"img\": torch.stack(imgs),\n",
    "        \"label\": torch.tensor(labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class SparseMoEConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        router_dim,\n",
    "        in_channels,\n",
    "        num_experts=8,\n",
    "        top_k=2,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.in_channels = in_channels\n",
    "        self.router_dim = router_dim\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "\n",
    "        self.router = nn.Linear(router_dim, num_experts, bias=False)\n",
    "        # initialize the router weights to be random\n",
    "        nn.init.xavier_uniform_(self.router.weight)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"experts_used\", torch.zeros(num_experts))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        router_logits = self.router(x.view(batch_size, -1))\n",
    "        router_output = nn.functional.softmax(router_logits, dim=1)\n",
    "\n",
    "        router_avg = torch.mean(router_output, dim=0)\n",
    "        target = torch.tensor([1 / self.num_experts] * self.num_experts).to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        router_loss = criterion(router_avg, target)\n",
    "\n",
    "        routing_weights, selected_experts = torch.topk(router_output, self.top_k)\n",
    "\n",
    "        # Create mask for the selected experts (batch_size, num_experts)\n",
    "        expert_mask = torch.zeros(batch_size, self.num_experts).to(x.device)\n",
    "        expert_mask.scatter_(1, selected_experts, 1)\n",
    "\n",
    "        self.experts_used = self.experts_used.to(x.device)\n",
    "        self.experts_used += torch.sum(expert_mask, dim=0)\n",
    "\n",
    "        expert_outputs = torch.zeros(\n",
    "            batch_size, self.in_channels * self.top_k, height, width\n",
    "        ).to(x.device)\n",
    "\n",
    "        for i, expert_idx in enumerate(selected_experts[0]):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # We select the corresponding routing weights for the expert\n",
    "            routing_weights_expert = routing_weights[0, i]\n",
    "\n",
    "            out = expert_layer(x)\n",
    "            # out = out * routing_weights_expert.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            expert_outputs[:, i * self.in_channels : (i + 1) * self.in_channels] = out\n",
    "\n",
    "        return expert_outputs, router_loss\n",
    "\n",
    "    def get_experts_used(self):\n",
    "        return self.experts_used\n",
    "\n",
    "    def get_experts_load_balancing_loss(self):\n",
    "        return torch.std(self.experts_used)\n",
    "\n",
    "    def reset_experts_used(self):\n",
    "        self.experts_used = torch.zeros(self.num_experts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoEConvBlockWeighted(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        router_dim,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_experts=8,\n",
    "        top_k=2,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.router_dim = router_dim\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.router = nn.Linear(router_dim, num_experts, bias=False)\n",
    "        # initialize the router weights to be random\n",
    "        nn.init.xavier_uniform_(self.router.weight)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    stride=self.stride,\n",
    "                    padding=self.padding,\n",
    "                )\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        router_logits = self.router(x.view(batch_size, -1))\n",
    "        router_output = nn.functional.softmax(router_logits, dim=1)\n",
    "\n",
    "        router_avg = torch.mean(router_output, dim=0)\n",
    "        target = torch.tensor([1 / self.num_experts] * self.num_experts).to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        router_loss = criterion(router_avg, target)\n",
    "\n",
    "        routing_weights, selected_experts = torch.topk(router_output, self.top_k)\n",
    "\n",
    "        expert_outputs = torch.zeros(batch_size, self.out_channels, height, width).to(\n",
    "            x.device\n",
    "        )\n",
    "\n",
    "        for i, expert_idx in enumerate(selected_experts[0]):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # We select the corresponding routing weights for the expert\n",
    "            routing_weights_expert = routing_weights[0, i]\n",
    "\n",
    "            out = expert_layer(x)\n",
    "            out = out * routing_weights_expert.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "            expert_outputs += out\n",
    "\n",
    "        return expert_outputs, router_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, 1, 1)\n",
    "        self.fc1 = nn.Linear(12 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(-1, 12 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RoutedCNN(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super(RoutedCNN, self).__init__()\n",
    "        self.conv1 = SparseMoEConvBlockWeighted(\n",
    "            router_dim=3 * 32 * 32,\n",
    "            in_channels=3,\n",
    "            out_channels=24,\n",
    "            num_experts=10,\n",
    "            top_k=1,\n",
    "        )\n",
    "        # self.conv2 = SparseMoEConvBlockWeighted(\n",
    "        #     router_dim=12 * 32 * 32,\n",
    "        #     in_channels=12,\n",
    "        #     out_channels=24,\n",
    "        #     num_experts=10,\n",
    "        #     top_k=1,\n",
    "        # )\n",
    "        self.fc1 = nn.Linear(24 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, return_router_loss=False):\n",
    "        x, router_loss_1 = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        # x, router_loss_2 = self.conv2(x)\n",
    "        # x = torch.relu(x)\n",
    "        x = x.view(-1, 24 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        if return_router_loss:\n",
    "            return x, router_loss_1  # + router_loss_2\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"].with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset[\"test\"].with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1a519f0b1245ac8ceb494411a554d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1563/1563], Loss: 1.8563, Router loss: 0.0039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a84a889c7a74e0ca9b8d1e5df5d4740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.2\n",
      "Accuracy of the model on the test images: 34.89%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18dd4294a6a4967bb9b24be736f40f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [1563/1563], Loss: 1.9336, Router loss: 0.0046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b4f835df49443b9a6a1b6a13f1c46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.1507936507936508\n",
      "Accuracy of the model on the test images: 40.66%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ef5a8d2e7444a5bb39b6c5fd445b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [1563/1563], Loss: 1.3299, Router loss: 0.0078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ecd8d2f0154213817d1931bca1e5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.18571428571428572\n",
      "Accuracy of the model on the test images: 41.91%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684859befcb24b8c982df0d4e2125ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [1563/1563], Loss: 1.5296, Router loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dffdc88a114f17bb8a70b6742495ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.2644444444444444\n",
      "Accuracy of the model on the test images: 44.84%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c5e96c31bb4d54b8e8262c7b411d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [1563/1563], Loss: 1.5656, Router loss: 0.0046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38809ed6b22b43bfbb74788c5147f3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.27248677248677244\n",
      "Accuracy of the model on the test images: 46.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a13a87cffb3461babf4fdc3829e4e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [1563/1563], Loss: 1.9142, Router loss: 0.0046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e739bf18d5641a5a6c4526605296bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.33756613756613757\n",
      "Accuracy of the model on the test images: 46.98%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e003bb083e4612b1be4d1afda39b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [1563/1563], Loss: 1.6312, Router loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea07540c25e3435e8e1ce89e81ff0eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.26296296296296295\n",
      "Accuracy of the model on the test images: 47.96%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c073aa8ac0402ebf58d1d6ec23ff66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [1563/1563], Loss: 1.6547, Router loss: 0.0042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674395fa20f0482fa903f14d2ea30da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.31481481481481477\n",
      "Accuracy of the model on the test images: 48.95%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfe3d98b0144ccb88a4ce54b724b7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [1563/1563], Loss: 1.4485, Router loss: 0.0122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02a329a2f1941abbc94abd8882d20da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.319047619047619\n",
      "Accuracy of the model on the test images: 49.34%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c072c578de4725b255347385762461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [1563/1563], Loss: 1.5097, Router loss: 0.0085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83fc315bdc1479aacd68c1772225749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model on the test images: 0.3111111111111111\n",
      "Accuracy of the model on the test images: 50.86%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RoutedCNN().to(device)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs, router_loss = model(images, return_router_loss=True)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # # Compute the load balancing loss\n",
    "        # load_balancing_loss = 0\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, SparseMoEConvBlock):\n",
    "        #         load_balancing_loss += module.get_experts_load_balancing_loss()\n",
    "        #         module.reset_experts_used()\n",
    "\n",
    "        # Combine the losses\n",
    "        lambda_balance = (\n",
    "            10  # Adjust this hyperparameter to control the strength of load balancing\n",
    "        )\n",
    "\n",
    "        total_loss = loss + lambda_balance * router_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % len(train_loader) == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {total_loss.item():.4f}, Router loss: {router_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(images)\n",
    "            # for module in model.modules():\n",
    "            #     if isinstance(module, SparseMoEConvBlock):\n",
    "            # print(f\"Experts used std: {torch.std(module.get_experts_used())}\")\n",
    "            # module.reset_experts_used()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"F1 score of the model on the test images: {f1_score(labels.cpu(), predicted.cpu(), average='macro')}\"\n",
    "        )\n",
    "        print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"routed_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816c7adc52a04a3aaedea3c453b19cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 50.17%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.46      0.55      1000\n",
      "           1       0.60      0.69      0.64      1000\n",
      "           2       0.42      0.25      0.31      1000\n",
      "           3       0.38      0.34      0.36      1000\n",
      "           4       0.43      0.35      0.39      1000\n",
      "           5       0.45      0.38      0.41      1000\n",
      "           6       0.45      0.67      0.54      1000\n",
      "           7       0.51      0.59      0.54      1000\n",
      "           8       0.55      0.71      0.62      1000\n",
      "           9       0.54      0.57      0.56      1000\n",
      "\n",
      "    accuracy                           0.50     10000\n",
      "   macro avg       0.50      0.50      0.49     10000\n",
      "weighted avg       0.50      0.50      0.49     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(images)\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, SparseMoEConvBlock):\n",
    "        # print(f\"Experts used std: {torch.std(module.get_experts_used())}\")\n",
    "        # module.reset_experts_used()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")\n",
    "    print(classification_report(targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27191/1346067593.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"routed_cnn.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoutedCNN(\n",
      "  (conv1): SparseMoEConvBlockWeighted(\n",
      "    (router): Linear(in_features=3072, out_features=10, bias=False)\n",
      "    (experts): ModuleList(\n",
      "      (0-9): 10 x Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=24576, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Top experts for class 1: [7, 1]\n",
      "Epoch 1/5, Loss: 2.0120\n",
      "Epoch 2/5, Loss: 1.8327\n",
      "Epoch 3/5, Loss: 1.7644\n",
      "Epoch 4/5, Loss: 1.8119\n",
      "Epoch 5/5, Loss: 1.7596\n",
      "Unlearning procedure completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def identify_top_experts(model, dataloader, target_class, num_top_experts=2):\n",
    "    model.eval()\n",
    "    expert_usage = {i: 0 for i in range(model.conv1.num_experts)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"img\"]\n",
    "            labels = batch[\"label\"]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            mask = labels == target_class\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            target_inputs = inputs[mask]\n",
    "\n",
    "            # Forward pass through conv1\n",
    "            router_output = model.conv1.router(\n",
    "                target_inputs.view(target_inputs.size(0), -1)\n",
    "            )\n",
    "            _, selected_experts = torch.topk(router_output, model.conv1.top_k)\n",
    "\n",
    "            for expert in selected_experts.flatten():\n",
    "                expert_usage[expert.item()] += 1\n",
    "\n",
    "    # Sort experts by usage and return top num_top_experts\n",
    "    sorted_experts = sorted(expert_usage.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [expert for expert, _ in sorted_experts[:num_top_experts]]\n",
    "\n",
    "\n",
    "class UnlearningLoss(nn.Module):\n",
    "    def __init__(self, target_class, penalty_weight=2):\n",
    "        super().__init__()\n",
    "        self.target_class = target_class\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.penalty_weight = penalty_weight\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # Standard cross-entropy loss\n",
    "        # change labels of target class to random class != target_class\n",
    "        labels = torch.where(\n",
    "            labels == self.target_class,\n",
    "            torch.randint_like(labels, 0, 9),\n",
    "            labels,\n",
    "        )\n",
    "\n",
    "        ce_loss = self.ce_loss(outputs, labels)\n",
    "\n",
    "        # # Penalize correct classification of target class\n",
    "        # target_mask = labels == self.target_class\n",
    "        # if target_mask.any():\n",
    "        #     target_outputs = outputs[target_mask]\n",
    "        #     target_loss = -torch.log_softmax(target_outputs, dim=1)[\n",
    "        #         :, self.target_class\n",
    "        #     ].mean()\n",
    "        #     return ce_loss + target_loss\n",
    "\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "        # Penalize correct classification of target class\n",
    "        correct_target_class_mask = (predicted == labels) & (\n",
    "            labels == self.target_class\n",
    "        )\n",
    "\n",
    "        penalty = correct_target_class_mask.sum() * self.penalty_weight\n",
    "\n",
    "        return ce_loss + penalty\n",
    "\n",
    "\n",
    "def unlearning_procedure(\n",
    "    model, train_dataloader, target_class, num_epochs=20, learning_rate=0.001\n",
    "):\n",
    "    print(model)\n",
    "    # Identify top experts for the target class\n",
    "    top_experts = identify_top_experts(model, train_dataloader, target_class)\n",
    "    print(f\"Top experts for class {target_class}: {top_experts}\")\n",
    "\n",
    "    # Freeze all parameters except the identified experts\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for expert_idx in top_experts:\n",
    "        for param in model.conv1.experts[expert_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.conv1.router.weight.requires_grad = True\n",
    "\n",
    "    model.fc2.weight.requires_grad = True\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "    )\n",
    "    unlearning_loss = UnlearningLoss(target_class)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = batch[\"img\"]\n",
    "            labels = batch[\"label\"]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = unlearning_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader):.4f}\"\n",
    "        )\n",
    "\n",
    "    # Unfreeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    print(\"Unlearning procedure completed.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_class = 1\n",
    "\n",
    "model = RoutedCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"routed_cnn.pth\"))\n",
    "\n",
    "dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     dataset[\"train\"].filter(lambda x: x[\"label\"] == target_class).with_format(\"torch\"),\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"]\n",
    "    .shuffle()\n",
    "    .select(range(len(dataset[\"train\"]) // 20))\n",
    "    .with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "unlearning_procedure(\n",
    "    model, train_loader, target_class, num_epochs=5, learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15865f109fe4823b6e6202f3d4964e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the target images: 0.1%\n",
      "Accuracy of the model on the non-target images: 44.86666666666667%\n",
      "Accuracy of the model on the test images: 40.39%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.45      0.51      1000\n",
      "           2       0.35      0.26      0.30      1000\n",
      "           3       0.29      0.29      0.29      1000\n",
      "           4       0.31      0.47      0.37      1000\n",
      "           5       0.41      0.39      0.40      1000\n",
      "           6       0.40      0.69      0.51      1000\n",
      "           7       0.68      0.35      0.46      1000\n",
      "           8       0.59      0.64      0.61      1000\n",
      "           9       0.67      0.52      0.58      1000\n",
      "\n",
      "    accuracy                           0.45      9000\n",
      "   macro avg       0.48      0.45      0.45      9000\n",
      "weighted avg       0.48      0.45      0.45      9000\n",
      "\n",
      "F1 score of the model on the test images: 0.4477874232609709\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset[\"test\"].with_format(\"torch\"),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(images)\n",
    "        # for module in model.modules():\n",
    "        #     if isinstance(module, SparseMoEConvBlock):\n",
    "        # print(f\"Experts used std: {torch.std(module.get_experts_used())}\")\n",
    "        # module.reset_experts_used()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Accuracy on target class\n",
    "    correct_target = 0\n",
    "    total_target = 0\n",
    "    for pred, target in zip(preds, targets):\n",
    "        if target == target_class:\n",
    "            total_target += 1\n",
    "            if pred == target:\n",
    "                correct_target += 1\n",
    "\n",
    "    # Accuracy on non-target classes\n",
    "    correct_non_target = 0\n",
    "    total_non_target = 0\n",
    "    for pred, target in zip(preds, targets):\n",
    "        if target != target_class:\n",
    "            total_non_target += 1\n",
    "            if pred == target:\n",
    "                correct_non_target += 1\n",
    "    print(\n",
    "        f\"Accuracy of the model on the target images: {100 * correct_target / total_target}%\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Accuracy of the model on the non-target images: {100 * correct_non_target / total_non_target}%\"\n",
    "    )\n",
    "    # find indexes of target class\n",
    "    target_indexes = [i for i, x in enumerate(targets) if x == target_class]\n",
    "\n",
    "    # remove target class from predictions and targets\n",
    "    preds = [pred for i, pred in enumerate(preds) if i not in target_indexes]\n",
    "    targets = [target for i, target in enumerate(targets) if i not in target_indexes]\n",
    "\n",
    "    print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")\n",
    "    print(classification_report(targets, preds, zero_division=0))\n",
    "    print(\n",
    "        f\"F1 score of the model on the test images: {f1_score(targets, preds, average='macro')}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
