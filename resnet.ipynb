{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(weights=None, num_classes=200)\n",
    "dataset_train = load_dataset(\"json\", data_files=\"train_data.jsonl\", split=\"train\")\n",
    "dataset_val = load_dataset(\"json\", data_files=\"val_data.jsonl\", split=\"train\")\n",
    "dataset_train = dataset_train.with_format(\"torch\")\n",
    "dataset_val = dataset_val.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_path', 'class_name', 'class_label'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "# def preprocess(batch):\n",
    "#     images = []\n",
    "#     for x in batch[\"image\"]:\n",
    "#         image = transforms.ToPILImage()(x)\n",
    "#         if image.mode != \"RGB\":\n",
    "#             image = image.convert(\"RGB\")\n",
    "#         image = compose(image)\n",
    "#         images.append(image)\n",
    "#     return {\n",
    "#         \"image\": images,\n",
    "#         \"label\": [x for x in batch[\"label\"]],\n",
    "#     }\n",
    "\n",
    "\n",
    "# dataset = dataset.map(preprocess, batched=True, batch_size=100, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"tiny-imagenet-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(batch):\n",
    "#     images = []\n",
    "#     for x in batch[\"image\"]:\n",
    "#         if x.shape[0] == 1:\n",
    "#             x = x.repeat(3, 1, 1)\n",
    "#         images.append(x.type(torch.uint8))\n",
    "#     return {\n",
    "#         \"image\": torch.stack(images),\n",
    "#         \"label\": torch.tensor([x for x in batch[\"label\"]]),\n",
    "#     }\n",
    "\n",
    "\n",
    "# dataset = dataset.map(preprocess, batched=True, batch_size=100, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2id = dict()\n",
    "c = 0\n",
    "for i in range(len(dataset_val)):\n",
    "    label = dataset_val[i][\"class_name\"]\n",
    "    if label not in labels2id:\n",
    "        labels2id[label] = c\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply labels to dataset\n",
    "def apply_labels(batch):\n",
    "    labels = []\n",
    "    images = []\n",
    "    for x in batch[\"class_name\"]:\n",
    "        labels.append(labels2id[x])\n",
    "    for image_path in batch[\"image_path\"]:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        img = transforms.PILToTensor()(img).to(torch.float32)\n",
    "        img = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )(img)\n",
    "        images.append(img)\n",
    "    return {\n",
    "        \"image\": torch.stack(images),\n",
    "        \"label\": torch.tensor([(x) for x in labels]),\n",
    "    }\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.map(apply_labels, batched=True, num_proc=4)\n",
    "dataset_val = dataset_val.map(apply_labels, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': './tiny-imagenet-200/train/n01443537/images/n01443537_0.JPEG',\n",
       " 'class_name': 'n01443537',\n",
       " 'class_label': 'goldfish, Carassius auratus',\n",
       " 'image': tensor([[[1111.4192, 1111.4192, 1085.2184,  ...,  906.1790,  910.5458,\n",
       "            552.4672],\n",
       "          [1111.4192, 1107.0524, 1072.1179,  ...,  893.0786,  879.9781,\n",
       "            535.0000],\n",
       "          [1111.4192, 1111.4192, 1111.4192,  ...,  901.8122,  879.9781,\n",
       "            521.8995],\n",
       "          ...,\n",
       "          [ 412.7292,  403.9956,  403.9956,  ...,  369.0611,  338.4934,\n",
       "            408.3624],\n",
       "          [ 386.5284,  386.5284,  386.5284,  ...,  373.4279,  334.1266,\n",
       "            377.7947],\n",
       "          [ 395.2620,  395.2620,  403.9956,  ...,  417.0961,  334.1266,\n",
       "            312.2926]],\n",
       " \n",
       "         [[ 605.1072,  614.0357,  649.7500,  ..., 1056.0000, 1073.8572,\n",
       "            725.6429],\n",
       "          [ 560.4642,  569.3928,  609.5714,  ..., 1029.2142, 1042.6072,\n",
       "            707.7857],\n",
       "          [ 560.4642,  573.8572,  600.6429,  ..., 1038.1428, 1042.6072,\n",
       "            689.9286],\n",
       "          ...,\n",
       "          [ 542.6071,  529.2142,  515.8214,  ...,  328.3214,  297.0714,\n",
       "            368.5000],\n",
       "          [ 520.2857,  511.3571,  497.9643,  ...,  332.7857,  292.6071,\n",
       "            337.2500],\n",
       "          [ 529.2142,  529.2142,  515.8214,  ...,  377.4286,  292.6071,\n",
       "            270.2857]],\n",
       " \n",
       "         [[ 855.9733,  851.5289,  882.6400,  ..., 1042.6400, 1064.8622,\n",
       "            713.7511],\n",
       "          [ 838.1956,  838.1956,  860.4178,  ..., 1020.4178, 1024.8622,\n",
       "            687.0844],\n",
       "          [ 891.5289,  891.5289,  900.4178,  ..., 1020.4178, 1015.9733,\n",
       "            660.4178],\n",
       "          ...,\n",
       "          [ 313.7511,  313.7511,  309.3067,  ...,  247.0845,  215.9733,\n",
       "            287.0845],\n",
       "          [ 291.5289,  295.9734,  291.5289,  ...,  251.5289,  211.5289,\n",
       "            255.9733],\n",
       "          [ 300.4178,  309.3067,  309.3067,  ...,  295.9734,  211.5289,\n",
       "            189.3067]]]),\n",
       " 'label': tensor(151)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': ['./tiny-imagenet-200/train/n07749582/images/n07749582_102.JPEG', './tiny-imagenet-200/train/n04532670/images/n04532670_59.JPEG', './tiny-imagenet-200/train/n09193705/images/n09193705_248.JPEG', './tiny-imagenet-200/train/n04118538/images/n04118538_60.JPEG', './tiny-imagenet-200/train/n07920052/images/n07920052_335.JPEG', './tiny-imagenet-200/train/n02769748/images/n02769748_77.JPEG', './tiny-imagenet-200/train/n02837789/images/n02837789_15.JPEG', './tiny-imagenet-200/train/n04023962/images/n04023962_19.JPEG', './tiny-imagenet-200/train/n04099969/images/n04099969_84.JPEG', './tiny-imagenet-200/train/n02281406/images/n02281406_5.JPEG', './tiny-imagenet-200/train/n02074367/images/n02074367_389.JPEG', './tiny-imagenet-200/train/n02125311/images/n02125311_372.JPEG', './tiny-imagenet-200/train/n01882714/images/n01882714_452.JPEG', './tiny-imagenet-200/train/n02883205/images/n02883205_450.JPEG', './tiny-imagenet-200/train/n09332890/images/n09332890_315.JPEG', './tiny-imagenet-200/train/n07768694/images/n07768694_65.JPEG', './tiny-imagenet-200/train/n02815834/images/n02815834_22.JPEG', './tiny-imagenet-200/train/n02823428/images/n02823428_281.JPEG', './tiny-imagenet-200/train/n04486054/images/n04486054_256.JPEG', './tiny-imagenet-200/train/n03854065/images/n03854065_67.JPEG', './tiny-imagenet-200/train/n09256479/images/n09256479_70.JPEG', './tiny-imagenet-200/train/n07583066/images/n07583066_474.JPEG', './tiny-imagenet-200/train/n02504458/images/n02504458_414.JPEG', './tiny-imagenet-200/train/n09256479/images/n09256479_486.JPEG', './tiny-imagenet-200/train/n03980874/images/n03980874_459.JPEG', './tiny-imagenet-200/train/n02281406/images/n02281406_273.JPEG', './tiny-imagenet-200/train/n02279972/images/n02279972_462.JPEG', './tiny-imagenet-200/train/n02085620/images/n02085620_10.JPEG', './tiny-imagenet-200/train/n02231487/images/n02231487_449.JPEG', './tiny-imagenet-200/train/n02814860/images/n02814860_27.JPEG', './tiny-imagenet-200/train/n02927161/images/n02927161_360.JPEG', './tiny-imagenet-200/train/n04560804/images/n04560804_329.JPEG', './tiny-imagenet-200/train/n04023962/images/n04023962_123.JPEG', './tiny-imagenet-200/train/n02815834/images/n02815834_463.JPEG', './tiny-imagenet-200/train/n04118538/images/n04118538_315.JPEG', './tiny-imagenet-200/train/n02977058/images/n02977058_115.JPEG', './tiny-imagenet-200/train/n01774750/images/n01774750_341.JPEG', './tiny-imagenet-200/train/n04259630/images/n04259630_230.JPEG', './tiny-imagenet-200/train/n02403003/images/n02403003_134.JPEG', './tiny-imagenet-200/train/n04251144/images/n04251144_337.JPEG', './tiny-imagenet-200/train/n07615774/images/n07615774_160.JPEG', './tiny-imagenet-200/train/n04275548/images/n04275548_489.JPEG', './tiny-imagenet-200/train/n02106662/images/n02106662_11.JPEG', './tiny-imagenet-200/train/n02395406/images/n02395406_426.JPEG', './tiny-imagenet-200/train/n03983396/images/n03983396_293.JPEG', './tiny-imagenet-200/train/n01774384/images/n01774384_228.JPEG', './tiny-imagenet-200/train/n02815834/images/n02815834_162.JPEG', './tiny-imagenet-200/train/n02226429/images/n02226429_245.JPEG', './tiny-imagenet-200/train/n02823428/images/n02823428_34.JPEG', './tiny-imagenet-200/train/n03085013/images/n03085013_419.JPEG', './tiny-imagenet-200/train/n02977058/images/n02977058_148.JPEG', './tiny-imagenet-200/train/n04133789/images/n04133789_72.JPEG', './tiny-imagenet-200/train/n04501370/images/n04501370_493.JPEG', './tiny-imagenet-200/train/n02226429/images/n02226429_239.JPEG', './tiny-imagenet-200/train/n02480495/images/n02480495_186.JPEG', './tiny-imagenet-200/train/n02165456/images/n02165456_112.JPEG', './tiny-imagenet-200/train/n04070727/images/n04070727_393.JPEG', './tiny-imagenet-200/train/n04179913/images/n04179913_34.JPEG', './tiny-imagenet-200/train/n04285008/images/n04285008_438.JPEG', './tiny-imagenet-200/train/n03179701/images/n03179701_270.JPEG', './tiny-imagenet-200/train/n04070727/images/n04070727_119.JPEG', './tiny-imagenet-200/train/n03617480/images/n03617480_13.JPEG', './tiny-imagenet-200/train/n02509815/images/n02509815_306.JPEG', './tiny-imagenet-200/train/n07579787/images/n07579787_14.JPEG', './tiny-imagenet-200/train/n03970156/images/n03970156_202.JPEG', './tiny-imagenet-200/train/n02669723/images/n02669723_259.JPEG', './tiny-imagenet-200/train/n04146614/images/n04146614_288.JPEG', './tiny-imagenet-200/train/n04074963/images/n04074963_452.JPEG', './tiny-imagenet-200/train/n04376876/images/n04376876_338.JPEG', './tiny-imagenet-200/train/n07711569/images/n07711569_458.JPEG', './tiny-imagenet-200/train/n01768244/images/n01768244_420.JPEG', './tiny-imagenet-200/train/n03733131/images/n03733131_113.JPEG', './tiny-imagenet-200/train/n07920052/images/n07920052_124.JPEG', './tiny-imagenet-200/train/n04275548/images/n04275548_55.JPEG', './tiny-imagenet-200/train/n02058221/images/n02058221_129.JPEG', './tiny-imagenet-200/train/n04023962/images/n04023962_174.JPEG', './tiny-imagenet-200/train/n02815834/images/n02815834_361.JPEG', './tiny-imagenet-200/train/n02190166/images/n02190166_457.JPEG', './tiny-imagenet-200/train/n02883205/images/n02883205_46.JPEG', './tiny-imagenet-200/train/n04507155/images/n04507155_305.JPEG', './tiny-imagenet-200/train/n04596742/images/n04596742_55.JPEG', './tiny-imagenet-200/train/n02124075/images/n02124075_80.JPEG', './tiny-imagenet-200/train/n02927161/images/n02927161_340.JPEG', './tiny-imagenet-200/train/n02106662/images/n02106662_136.JPEG', './tiny-imagenet-200/train/n04398044/images/n04398044_64.JPEG', './tiny-imagenet-200/train/n02909870/images/n02909870_328.JPEG', './tiny-imagenet-200/train/n02486410/images/n02486410_27.JPEG', './tiny-imagenet-200/train/n07920052/images/n07920052_178.JPEG', './tiny-imagenet-200/train/n02963159/images/n02963159_160.JPEG', './tiny-imagenet-200/train/n04311004/images/n04311004_64.JPEG', './tiny-imagenet-200/train/n02823428/images/n02823428_420.JPEG', './tiny-imagenet-200/train/n01774384/images/n01774384_355.JPEG', './tiny-imagenet-200/train/n01443537/images/n01443537_486.JPEG', './tiny-imagenet-200/train/n03977966/images/n03977966_271.JPEG', './tiny-imagenet-200/train/n04179913/images/n04179913_288.JPEG', './tiny-imagenet-200/train/n04376876/images/n04376876_145.JPEG', './tiny-imagenet-200/train/n02233338/images/n02233338_457.JPEG', './tiny-imagenet-200/train/n01443537/images/n01443537_221.JPEG', './tiny-imagenet-200/train/n04356056/images/n04356056_160.JPEG', './tiny-imagenet-200/train/n02843684/images/n02843684_432.JPEG', './tiny-imagenet-200/train/n01629819/images/n01629819_329.JPEG', './tiny-imagenet-200/train/n07747607/images/n07747607_236.JPEG', './tiny-imagenet-200/train/n03970156/images/n03970156_37.JPEG', './tiny-imagenet-200/train/n02058221/images/n02058221_130.JPEG', './tiny-imagenet-200/train/n04465501/images/n04465501_119.JPEG', './tiny-imagenet-200/train/n07920052/images/n07920052_468.JPEG', './tiny-imagenet-200/train/n01882714/images/n01882714_465.JPEG', './tiny-imagenet-200/train/n03814639/images/n03814639_70.JPEG', './tiny-imagenet-200/train/n01917289/images/n01917289_125.JPEG', './tiny-imagenet-200/train/n04465501/images/n04465501_347.JPEG', './tiny-imagenet-200/train/n04597913/images/n04597913_426.JPEG', './tiny-imagenet-200/train/n03891332/images/n03891332_286.JPEG', './tiny-imagenet-200/train/n03400231/images/n03400231_341.JPEG', './tiny-imagenet-200/train/n04456115/images/n04456115_499.JPEG', './tiny-imagenet-200/train/n02793495/images/n02793495_471.JPEG', './tiny-imagenet-200/train/n01950731/images/n01950731_299.JPEG', './tiny-imagenet-200/train/n02841315/images/n02841315_100.JPEG', './tiny-imagenet-200/train/n02125311/images/n02125311_465.JPEG', './tiny-imagenet-200/train/n07614500/images/n07614500_49.JPEG', './tiny-imagenet-200/train/n02279972/images/n02279972_331.JPEG', './tiny-imagenet-200/train/n03599486/images/n03599486_376.JPEG', './tiny-imagenet-200/train/n07749582/images/n07749582_116.JPEG', './tiny-imagenet-200/train/n07583066/images/n07583066_354.JPEG', './tiny-imagenet-200/train/n04259630/images/n04259630_14.JPEG', './tiny-imagenet-200/train/n03201208/images/n03201208_176.JPEG', './tiny-imagenet-200/train/n04023962/images/n04023962_126.JPEG', './tiny-imagenet-200/train/n03100240/images/n03100240_112.JPEG', './tiny-imagenet-200/train/n02666196/images/n02666196_15.JPEG'], 'class_name': ['n07749582', 'n04532670', 'n09193705', 'n04118538', 'n07920052', 'n02769748', 'n02837789', 'n04023962', 'n04099969', 'n02281406', 'n02074367', 'n02125311', 'n01882714', 'n02883205', 'n09332890', 'n07768694', 'n02815834', 'n02823428', 'n04486054', 'n03854065', 'n09256479', 'n07583066', 'n02504458', 'n09256479', 'n03980874', 'n02281406', 'n02279972', 'n02085620', 'n02231487', 'n02814860', 'n02927161', 'n04560804', 'n04023962', 'n02815834', 'n04118538', 'n02977058', 'n01774750', 'n04259630', 'n02403003', 'n04251144', 'n07615774', 'n04275548', 'n02106662', 'n02395406', 'n03983396', 'n01774384', 'n02815834', 'n02226429', 'n02823428', 'n03085013', 'n02977058', 'n04133789', 'n04501370', 'n02226429', 'n02480495', 'n02165456', 'n04070727', 'n04179913', 'n04285008', 'n03179701', 'n04070727', 'n03617480', 'n02509815', 'n07579787', 'n03970156', 'n02669723', 'n04146614', 'n04074963', 'n04376876', 'n07711569', 'n01768244', 'n03733131', 'n07920052', 'n04275548', 'n02058221', 'n04023962', 'n02815834', 'n02190166', 'n02883205', 'n04507155', 'n04596742', 'n02124075', 'n02927161', 'n02106662', 'n04398044', 'n02909870', 'n02486410', 'n07920052', 'n02963159', 'n04311004', 'n02823428', 'n01774384', 'n01443537', 'n03977966', 'n04179913', 'n04376876', 'n02233338', 'n01443537', 'n04356056', 'n02843684', 'n01629819', 'n07747607', 'n03970156', 'n02058221', 'n04465501', 'n07920052', 'n01882714', 'n03814639', 'n01917289', 'n04465501', 'n04597913', 'n03891332', 'n03400231', 'n04456115', 'n02793495', 'n01950731', 'n02841315', 'n02125311', 'n07614500', 'n02279972', 'n03599486', 'n07749582', 'n07583066', 'n04259630', 'n03201208', 'n04023962', 'n03100240', 'n02666196'], 'class_label': ['lemon', 'viaduct', 'alp', 'rugby ball', 'espresso', 'backpack, back pack, knapsack, packsack, rucksack, haversack', 'bikini, two-piece', 'punching bag, punch bag, punching ball, punchball', 'rocking chair, rocker', 'sulphur butterfly, sulfur butterfly', 'dugong, Dugong dugon', 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 'bow tie, bow-tie, bowtie', 'lakeside, lakeshore', 'pomegranate', 'beaker', 'beer bottle', 'triumphal arch', 'organ, pipe organ', 'coral reef', 'guacamole', 'African elephant, Loxodonta africana', 'coral reef', 'poncho', 'sulphur butterfly, sulfur butterfly', 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 'Chihuahua', 'walking stick, walkingstick, stick insect', 'beacon, lighthouse, beacon light, pharos', 'butcher shop, meat market', 'water jug', 'punching bag, punch bag, punching ball, punchball', 'beaker', 'rugby ball', 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 'tarantula', 'sombrero', 'ox', 'snorkel', 'ice lolly, lolly, lollipop, popsicle', \"spider web, spider's web\", 'German shepherd, German shepherd dog, German police dog, alsatian', 'hog, pig, grunter, squealer, Sus scrofa', 'pop bottle, soda bottle', 'black widow, Latrodectus mactans', 'beaker', 'grasshopper, hopper', 'beer bottle', 'computer keyboard, keypad', 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 'sandal', 'turnstile', 'grasshopper, hopper', 'orangutan, orang, orangutang, Pongo pygmaeus', 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 'refrigerator, icebox', 'sewing machine', 'sports car, sport car', 'desk', 'refrigerator, icebox', 'kimono', 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens', 'plate', \"plunger, plumber's helper\", \"academic gown, academic robe, judge's robe\", 'school bus', 'remote control, remote', 'syringe', 'mashed potato', 'trilobite', 'maypole', 'espresso', \"spider web, spider's web\", 'albatross, mollymawk', 'punching bag, punch bag, punching ball, punchball', 'beaker', 'fly', 'bow tie, bow-tie, bowtie', 'umbrella', 'wok', 'Egyptian cat', 'butcher shop, meat market', 'German shepherd, German shepherd dog, German police dog, alsatian', 'teapot', 'bucket, pail', 'baboon', 'espresso', 'cardigan', 'steel arch bridge', 'beer bottle', 'black widow, Latrodectus mactans', 'goldfish, Carassius auratus', 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 'sewing machine', 'syringe', 'cockroach, roach', 'goldfish, Carassius auratus', 'sunglasses, dark glasses, shades', 'birdhouse', 'European fire salamander, Salamandra salamandra', 'orange', \"plunger, plumber's helper\", 'albatross, mollymawk', 'tractor', 'espresso', 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 'neck brace', 'brain coral', 'tractor', 'wooden spoon', 'parking meter', 'frying pan, frypan, skillet', 'torch', 'barn', 'sea slug, nudibranch', 'binoculars, field glasses, opera glasses', 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'ice cream, icecream', 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 'jinrikisha, ricksha, rickshaw', 'lemon', 'guacamole', 'sombrero', 'dining table, board', 'punching bag, punch bag, punching ball, punchball', 'convertible', 'abacus'], 'image': tensor([[[[ 818.8428,  818.8428,  836.3101,  ...,  906.1790,  906.1790,\n",
      "            901.8122],\n",
      "          [ 823.2096,  827.5764,  840.6768,  ...,  914.9127,  910.5458,\n",
      "            910.5458],\n",
      "          [ 836.3101,  836.3101,  849.4105,  ...,  928.0131,  923.6463,\n",
      "            923.6463],\n",
      "          ...,\n",
      "          [ 919.2795,  919.2795,  923.6463,  ...,  932.3799,  928.0131,\n",
      "            919.2795],\n",
      "          [ 936.7467,  936.7467,  936.7467,  ...,  897.4454,  910.5458,\n",
      "            914.9127],\n",
      "          [ 949.8472,  949.8472,  941.1135,  ...,  879.9781,  893.0786,\n",
      "            901.8122]],\n",
      "\n",
      "         [[ 462.2500,  462.2500,  462.2500,  ...,  618.5000,  618.5000,\n",
      "            614.0357],\n",
      "          [ 466.7143,  475.6428,  475.6428,  ...,  627.4286,  622.9643,\n",
      "            622.9643],\n",
      "          [ 484.5714,  484.5714,  489.0357,  ...,  631.8929,  627.4286,\n",
      "            627.4286],\n",
      "          ...,\n",
      "          [ 596.1786,  596.1786,  600.6429,  ...,  622.9643,  618.5000,\n",
      "            609.5714],\n",
      "          [ 600.6429,  600.6429,  600.6429,  ...,  587.2500,  600.6429,\n",
      "            605.1072],\n",
      "          [ 609.5714,  609.5714,  605.1072,  ...,  569.3928,  582.7857,\n",
      "            591.7143]],\n",
      "\n",
      "         [[  24.8622,   20.4178,   15.9733,  ...,   42.6400,   42.6400,\n",
      "             38.1956],\n",
      "          [  24.8622,   20.4178,   15.9733,  ...,   51.5289,   47.0844,\n",
      "             47.0844],\n",
      "          [  20.4178,   11.5289,   11.5289,  ...,   60.4178,   55.9733,\n",
      "             55.9733],\n",
      "          ...,\n",
      "          [  -1.8044,   -1.8044,   -1.8044,  ...,   20.4178,   15.9733,\n",
      "              7.0844],\n",
      "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,   -1.8044,\n",
      "              2.6400],\n",
      "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,   -1.8044,\n",
      "             -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[   2.2489,   -2.1179,   -2.1179,  ...,   28.4498,   28.4498,\n",
      "             32.8166],\n",
      "          [  -2.1179,   -2.1179,    2.2489,  ...,   24.0830,   50.2838,\n",
      "             41.5502],\n",
      "          [   6.6157,   28.4498,   28.4498,  ...,   15.3493,   41.5502,\n",
      "             15.3493],\n",
      "          ...,\n",
      "          [  -2.1179,   -2.1179,   19.7162,  ...,   28.4498,    6.6157,\n",
      "              2.2489],\n",
      "          [  -2.1179,   -2.1179,   19.7162,  ...,    2.2489,    6.6157,\n",
      "             -2.1179],\n",
      "          [  -2.1179,    2.2489,   15.3493,  ...,   10.9825,   19.7162,\n",
      "             41.5502]],\n",
      "\n",
      "         [[   6.8929,    2.4286,    2.4286,  ...,   -2.0357,   -2.0357,\n",
      "             -2.0357],\n",
      "          [   2.4286,    2.4286,    6.8929,  ...,   -2.0357,   11.3571,\n",
      "              2.4286],\n",
      "          [   2.4286,   24.7500,   29.2143,  ...,   -2.0357,   15.8214,\n",
      "             -2.0357],\n",
      "          ...,\n",
      "          [  -2.0357,   -2.0357,    2.4286,  ...,   24.7500,   -2.0357,\n",
      "             -2.0357],\n",
      "          [  -2.0357,   -2.0357,    2.4286,  ...,   -2.0357,   -2.0357,\n",
      "             -2.0357],\n",
      "          [  -2.0357,   -2.0357,   -2.0357,  ...,    6.8929,   11.3571,\n",
      "             38.1429]],\n",
      "\n",
      "         [[  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,   -1.8044,\n",
      "              2.6400],\n",
      "          [  -1.8044,   -1.8044,   -1.8044,  ...,   -1.8044,   24.8622,\n",
      "             15.9733],\n",
      "          [  -1.8044,    7.0844,   20.4178,  ...,   -1.8044,   33.7511,\n",
      "              7.0844],\n",
      "          ...,\n",
      "          [  38.1956,   20.4178,    7.0844,  ...,    7.0844,   20.4178,\n",
      "             33.7511],\n",
      "          [  47.0844,   29.3067,    7.0844,  ...,   -1.8044,   20.4178,\n",
      "             29.3067],\n",
      "          [  51.5289,   38.1956,    2.6400,  ...,   -1.8044,   33.7511,\n",
      "             73.7511]]],\n",
      "\n",
      "\n",
      "        [[[ 486.9651,  583.0349,  727.1397,  ...,  845.0436,  709.6725,\n",
      "            714.0393],\n",
      "          [ 535.0000,  631.0699,  727.1397,  ...,  945.4803,  862.5109,\n",
      "            757.7074],\n",
      "          [ 574.3013,  657.2708,  692.2053,  ...,  827.5764,  770.8079,\n",
      "            609.2358],\n",
      "          ...,\n",
      "          [ 369.0611,  377.7947,  373.4279,  ...,  395.2620,  399.6288,\n",
      "            417.0961],\n",
      "          [ 399.6288,  399.6288,  386.5284,  ...,  364.6943,  347.2271,\n",
      "            321.0262],\n",
      "          [ 434.5633,  438.9301,  421.4629,  ...,  452.0305,  403.9956,\n",
      "            329.7598]],\n",
      "\n",
      "         [[ 466.7143,  569.3928,  716.7143,  ...,  855.1071,  698.8572,\n",
      "            698.8572],\n",
      "          [ 524.7500,  622.9643,  721.1786,  ...,  953.3214,  859.5714,\n",
      "            752.4286],\n",
      "          [ 569.3928,  654.2143,  689.9286,  ...,  832.7857,  774.7500,\n",
      "            609.5714],\n",
      "          ...,\n",
      "          [ 408.6786,  417.6071,  413.1429,  ...,  417.6071,  417.6071,\n",
      "            439.9286],\n",
      "          [ 435.4643,  435.4643,  422.0714,  ...,  381.8929,  364.0357,\n",
      "            337.2500],\n",
      "          [ 471.1786,  475.6428,  457.7857,  ...,  471.1786,  422.0714,\n",
      "            346.1786]],\n",
      "\n",
      "         [[ 602.6400,  691.5289,  838.1956,  ...,  958.1956,  820.4178,\n",
      "            833.7511],\n",
      "          [ 664.8622,  762.6400,  851.5289,  ..., 1069.3066,  989.3066,\n",
      "            882.6400],\n",
      "          [ 722.6400,  807.0844,  833.7511,  ...,  958.1956,  909.3066,\n",
      "            744.8622],\n",
      "          ...,\n",
      "          [  95.9733,  104.8622,  100.4178,  ...,  118.1956,  127.0844,\n",
      "            140.4178],\n",
      "          [ 113.7511,  113.7511,  109.3067,  ...,   91.5289,   82.6400,\n",
      "             47.0844],\n",
      "          [ 149.3067,  153.7511,  144.8622,  ...,  189.3067,  140.4178,\n",
      "             64.8622]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 836.3101, 1072.1179, 1111.4192,  ..., 1024.0830, 1024.0830,\n",
      "           1015.3493],\n",
      "          [ 840.6768, 1054.6506, 1080.8516,  ..., 1024.0830, 1019.7161,\n",
      "           1010.9825],\n",
      "          [ 805.7424, 1002.2489,  997.8821,  ..., 1050.2838, 1050.2838,\n",
      "           1045.9170],\n",
      "          ...,\n",
      "          [1059.0175, 1050.2838, 1050.2838,  ...,   -2.1179,   59.0175,\n",
      "             45.9170],\n",
      "          [ 989.1484, 1024.0830, 1045.9170,  ...,   -2.1179,   45.9170,\n",
      "             41.5502],\n",
      "          [1032.8165, 1024.0830, 1015.3493,  ...,   -2.1179,   32.8166,\n",
      "             41.5502]],\n",
      "\n",
      "         [[ 225.6428,  466.7143,  511.3571,  ...,  551.5357,  551.5357,\n",
      "            551.5357],\n",
      "          [ 292.6071,  511.3571,  533.6785,  ...,  547.0714,  538.1428,\n",
      "            538.1428],\n",
      "          [ 386.3571,  578.3214,  564.9285,  ...,  556.0000,  556.0000,\n",
      "            551.5357],\n",
      "          ...,\n",
      "          [ 663.1429,  654.2143,  645.2857,  ...,   11.3571,   15.8214,\n",
      "             -2.0357],\n",
      "          [ 640.8214,  667.6072,  676.5357,  ...,   15.8214,   11.3571,\n",
      "             -2.0357],\n",
      "          [ 703.3214,  685.4643,  667.6072,  ...,   24.7500,   -2.0357,\n",
      "             -2.0357]],\n",
      "\n",
      "         [[  33.7511,  264.8622,  309.3067,  ...,  358.1956,  358.1956,\n",
      "            358.1956],\n",
      "          [  91.5289,  309.3067,  322.6400,  ...,  340.4178,  340.4178,\n",
      "            331.5289],\n",
      "          [ 167.0845,  353.7511,  344.8622,  ...,  327.0845,  335.9734,\n",
      "            322.6400],\n",
      "          ...,\n",
      "          [ 429.3067,  420.4178,  420.4178,  ...,   11.5289,   55.9733,\n",
      "             24.8622],\n",
      "          [ 398.1956,  424.8622,  451.5289,  ...,   24.8622,   60.4178,\n",
      "             38.1956],\n",
      "          [ 451.5289,  438.1956,  433.7511,  ...,   29.3067,   47.0844,\n",
      "             47.0844]]],\n",
      "\n",
      "\n",
      "        [[[ 321.0262,  334.1266,  299.1921,  ...,  264.2576,  216.2227,\n",
      "            967.3144],\n",
      "          [ 229.3231,  373.4279,  238.0568,  ...,  272.9913,  255.5240,\n",
      "            338.4934],\n",
      "          [ 342.8603,  316.6594,  377.7947,  ...,  168.1878,  229.3231,\n",
      "            176.9214],\n",
      "          ...,\n",
      "          [ 954.2140,  906.1790,  888.7118,  ...,  871.2445,  866.8777,\n",
      "            862.5109],\n",
      "          [ 858.1441,  888.7118,  901.8122,  ...,  879.9781,  875.6113,\n",
      "            871.2445],\n",
      "          [ 884.3450,  932.3799,  910.5458,  ...,  884.3450,  879.9781,\n",
      "            871.2445]],\n",
      "\n",
      "         [[ 372.9643,  386.3571,  350.6429,  ...,  346.1786,  310.4643,\n",
      "           1078.3214],\n",
      "          [ 279.2143,  426.5357,  288.1429,  ...,  350.6429,  346.1786,\n",
      "            431.0000],\n",
      "          [ 399.7500,  372.9643,  435.4643,  ...,  243.5000,  319.3929,\n",
      "            265.8214],\n",
      "          ...,\n",
      "          [ 953.3214,  899.7500,  868.5000,  ...,  850.6429,  846.1786,\n",
      "            841.7143],\n",
      "          [ 850.6429,  881.8929,  881.8929,  ...,  859.5714,  855.1071,\n",
      "            850.6429],\n",
      "          [ 877.4286,  926.5357,  890.8214,  ...,  864.0357,  859.5714,\n",
      "            850.6429]],\n",
      "\n",
      "         [[ 380.4178,  393.7511,  358.1956,  ...,  202.6400,  153.7511,\n",
      "            918.1956],\n",
      "          [ 282.6400,  429.3067,  291.5289,  ...,  220.4178,  202.6400,\n",
      "            287.0845],\n",
      "          [ 380.4178,  353.7511,  415.9734,  ...,  122.6400,  193.7511,\n",
      "            140.4178],\n",
      "          ...,\n",
      "          [ 967.0845,  913.7511,  878.1956,  ...,  851.5289,  847.0844,\n",
      "            842.6400],\n",
      "          [ 864.8622,  895.9733,  891.5289,  ...,  869.3066,  864.8622,\n",
      "            860.4178],\n",
      "          [ 891.5289,  940.4178,  900.4178,  ...,  873.7511,  869.3066,\n",
      "            860.4178]]],\n",
      "\n",
      "\n",
      "        [[[  -2.1179,   -2.1179,   -2.1179,  ...,   72.1179,   76.4847,\n",
      "             54.6507],\n",
      "          [  -2.1179,   -2.1179,  107.0524,  ...,  334.1266,  342.8603,\n",
      "            111.4192],\n",
      "          [  -2.1179,   72.1179,  307.9258,  ...,  408.3624,  185.6550,\n",
      "             76.4847],\n",
      "          ...,\n",
      "          [  24.0830,   54.6507,  194.3886,  ...,   93.9520,   32.8166,\n",
      "             10.9825],\n",
      "          [  28.4498,   54.6507,  168.1878,  ...,  146.3537,   93.9520,\n",
      "             59.0175],\n",
      "          [  37.1834,   32.8166,   67.7511,  ...,   98.3188,   72.1179,\n",
      "             59.0175]],\n",
      "\n",
      "         [[  56.0000,   29.2143,   -2.0357,  ...,   -2.0357,    6.8929,\n",
      "             -2.0357],\n",
      "          [  38.1429,   51.5357,   87.2500,  ...,  239.0357,  279.2143,\n",
      "             60.4643],\n",
      "          [  24.7500,   69.3929,  221.1786,  ...,  297.0714,  105.1071,\n",
      "             15.8214],\n",
      "          ...,\n",
      "          [  -2.0357,   15.8214,  136.3571,  ...,   47.0714,   -2.0357,\n",
      "             -2.0357],\n",
      "          [  -2.0357,    6.8929,  100.6429,  ...,   69.3929,   33.6786,\n",
      "             11.3571],\n",
      "          [  -2.0357,   -2.0357,   -2.0357,  ...,    2.4286,   -2.0357,\n",
      "             -2.0357]],\n",
      "\n",
      "         [[  51.5289,   38.1956,   51.5289,  ...,   -1.8044,   -1.8044,\n",
      "             -1.8044],\n",
      "          [  -1.8044,    7.0844,   60.4178,  ...,  158.1956,  220.4178,\n",
      "              7.0844],\n",
      "          [  -1.8044,   -1.8044,   29.3067,  ...,  202.6400,   38.1956,\n",
      "             -1.8044],\n",
      "          ...,\n",
      "          [  15.9733,   29.3067,  135.9733,  ...,   -1.8044,   -1.8044,\n",
      "              7.0844],\n",
      "          [  15.9733,   24.8622,  104.8622,  ...,    7.0844,   33.7511,\n",
      "             47.0844],\n",
      "          [  24.8622,   -1.8044,    2.6400,  ...,   -1.8044,   -1.8044,\n",
      "             33.7511]]]]), 'label': tensor([154,  21,  56, 191, 166,  10, 168,   4, 130, 124,  14,  36,  71, 121,\n",
      "         22,  75,  77, 100,  35,  69,  70, 182, 116,  70,  18, 124,  72, 123,\n",
      "         45,  23, 188, 101,   4,  77, 191, 160,  98, 173, 132, 174,  88, 105,\n",
      "        127,  73,   6,  97,  77,  44, 100, 156, 160, 139,  15,  44, 197,  48,\n",
      "        140,  82,  63,  29, 140, 144, 136, 135, 133, 110,  81, 183,  52, 196,\n",
      "         74,  62, 166, 105,  96,   4,  77, 126, 121, 148,  78,  57, 188, 127,\n",
      "         99,  30,  85, 166, 114, 185, 100,  97, 151, 193,  82,  52, 176, 151,\n",
      "        122,  93,  51,  60, 133,  96,  55, 166,  71,  67,  68,  55,  42,  47,\n",
      "         27, 199,  92,  39,  90,  36, 119,  72, 178, 154, 182, 173, 137,   4,\n",
      "        142, 102])}\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=128)\n",
    "\n",
    "for x in train_loader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "model = torch.compile(model, mode=\"max-autotune\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c7e3abe95c454e8acb7d0304b7c652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 17:57:13.341000 140562075584320 torch/_inductor/utils.py:977] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "AUTOTUNE addmm(128x200, 128x512, 512x200)\n",
      "  addmm 0.0902 ms 100.0%\n",
      "  bias_addmm 0.0910 ms 99.1%\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.3588 seconds and 0.0000 seconds precompiling\n",
      "AUTOTUNE addmm(32x200, 32x512, 512x200)\n",
      "  bias_addmm 0.0821 ms 100.0%\n",
      "  addmm 0.0824 ms 99.6%\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.2002 seconds and 0.0000 seconds precompiling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.10      0.16        50\n",
      "           1       0.50      0.02      0.04        50\n",
      "           2       0.12      0.08      0.10        50\n",
      "           3       0.31      0.08      0.13        50\n",
      "           4       0.40      0.04      0.07        50\n",
      "           5       0.02      0.02      0.02        50\n",
      "           6       0.00      0.00      0.00        50\n",
      "           7       0.16      0.10      0.12        50\n",
      "           8       0.50      0.02      0.04        50\n",
      "           9       0.00      0.00      0.00        50\n",
      "          10       0.11      0.08      0.09        50\n",
      "          11       0.04      0.02      0.03        50\n",
      "          12       0.00      0.00      0.00        50\n",
      "          13       0.00      0.00      0.00        50\n",
      "          14       0.24      0.14      0.18        50\n",
      "          15       0.26      0.10      0.14        50\n",
      "          16       0.14      0.46      0.22        50\n",
      "          17       0.09      0.16      0.11        50\n",
      "          18       0.05      0.02      0.03        50\n",
      "          19       0.50      0.02      0.04        50\n",
      "          20       0.00      0.00      0.00        50\n",
      "          21       0.20      0.28      0.23        50\n",
      "          22       0.27      0.06      0.10        50\n",
      "          23       0.00      0.00      0.00        50\n",
      "          24       0.00      0.00      0.00        50\n",
      "          25       0.12      0.02      0.03        50\n",
      "          26       0.17      0.12      0.14        50\n",
      "          27       0.00      0.00      0.00        50\n",
      "          28       0.18      0.04      0.07        50\n",
      "          29       0.00      0.00      0.00        50\n",
      "          30       0.00      0.00      0.00        50\n",
      "          31       0.00      0.00      0.00        50\n",
      "          32       0.00      0.00      0.00        50\n",
      "          33       0.10      0.32      0.15        50\n",
      "          34       0.07      0.20      0.10        50\n",
      "          35       0.15      0.08      0.10        50\n",
      "          36       0.00      0.00      0.00        50\n",
      "          37       0.00      0.00      0.00        50\n",
      "          38       0.02      0.02      0.02        50\n",
      "          39       0.33      0.12      0.18        50\n",
      "          40       0.19      0.10      0.13        50\n",
      "          41       0.00      0.00      0.00        50\n",
      "          42       0.00      0.00      0.00        50\n",
      "          43       0.67      0.08      0.14        50\n",
      "          44       0.07      0.06      0.07        50\n",
      "          45       0.03      0.24      0.06        50\n",
      "          46       0.00      0.00      0.00        50\n",
      "          47       0.00      0.00      0.00        50\n",
      "          48       0.67      0.08      0.14        50\n",
      "          49       0.25      0.02      0.04        50\n",
      "          50       0.23      0.58      0.33        50\n",
      "          51       0.42      0.10      0.16        50\n",
      "          52       0.00      0.00      0.00        50\n",
      "          53       1.00      0.02      0.04        50\n",
      "          54       0.08      0.20      0.11        50\n",
      "          55       0.24      0.22      0.23        50\n",
      "          56       0.00      0.00      0.00        50\n",
      "          57       0.07      0.04      0.05        50\n",
      "          58       0.14      0.32      0.20        50\n",
      "          59       0.02      0.02      0.02        50\n",
      "          60       0.42      0.22      0.29        50\n",
      "          61       0.18      0.04      0.07        50\n",
      "          62       0.13      0.40      0.20        50\n",
      "          63       0.00      0.00      0.00        50\n",
      "          64       0.18      0.04      0.07        50\n",
      "          65       0.10      0.08      0.09        50\n",
      "          66       0.00      0.00      0.00        50\n",
      "          67       0.00      0.00      0.00        50\n",
      "          68       0.08      0.68      0.14        50\n",
      "          69       0.29      0.16      0.21        50\n",
      "          70       0.33      0.04      0.07        50\n",
      "          71       0.13      0.06      0.08        50\n",
      "          72       0.62      0.30      0.41        50\n",
      "          73       0.00      0.00      0.00        50\n",
      "          74       0.07      0.20      0.10        50\n",
      "          75       0.40      0.16      0.23        50\n",
      "          76       0.00      0.00      0.00        50\n",
      "          77       0.03      0.02      0.02        50\n",
      "          78       0.29      0.04      0.07        50\n",
      "          79       0.11      0.08      0.09        50\n",
      "          80       0.22      0.04      0.07        50\n",
      "          81       0.67      0.16      0.26        50\n",
      "          82       0.00      0.00      0.00        50\n",
      "          83       0.50      0.04      0.07        50\n",
      "          84       0.29      0.14      0.19        50\n",
      "          85       0.23      0.06      0.10        50\n",
      "          86       0.00      0.00      0.00        50\n",
      "          87       0.16      0.18      0.17        50\n",
      "          88       0.00      0.00      0.00        50\n",
      "          89       0.00      0.00      0.00        50\n",
      "          90       0.25      0.02      0.04        50\n",
      "          91       0.00      0.00      0.00        50\n",
      "          92       0.14      0.22      0.17        50\n",
      "          93       0.00      0.00      0.00        50\n",
      "          94       0.03      0.24      0.06        50\n",
      "          95       0.20      0.02      0.04        50\n",
      "          96       0.16      0.34      0.22        50\n",
      "          97       0.38      0.12      0.18        50\n",
      "          98       0.00      0.00      0.00        50\n",
      "          99       0.00      0.00      0.00        50\n",
      "         100       0.09      0.08      0.09        50\n",
      "         101       0.03      0.02      0.02        50\n",
      "         102       0.00      0.00      0.00        50\n",
      "         103       0.17      0.44      0.24        50\n",
      "         104       0.05      0.08      0.06        50\n",
      "         105       0.09      0.46      0.15        50\n",
      "         106       0.14      0.04      0.06        50\n",
      "         107       0.03      0.16      0.05        50\n",
      "         108       0.00      0.00      0.00        50\n",
      "         109       0.08      0.02      0.03        50\n",
      "         110       0.17      0.12      0.14        50\n",
      "         111       0.04      0.06      0.05        50\n",
      "         112       0.14      0.24      0.18        50\n",
      "         113       0.00      0.00      0.00        50\n",
      "         114       0.03      0.66      0.06        50\n",
      "         115       1.00      0.02      0.04        50\n",
      "         116       0.09      0.54      0.15        50\n",
      "         117       0.00      0.00      0.00        50\n",
      "         118       0.08      0.02      0.03        50\n",
      "         119       0.12      0.06      0.08        50\n",
      "         120       0.14      0.64      0.22        50\n",
      "         121       0.00      0.00      0.00        50\n",
      "         122       0.00      0.00      0.00        50\n",
      "         123       0.00      0.00      0.00        50\n",
      "         124       0.45      0.28      0.35        50\n",
      "         125       0.43      0.06      0.11        50\n",
      "         126       0.13      0.20      0.16        50\n",
      "         127       0.00      0.00      0.00        50\n",
      "         128       0.00      0.00      0.00        50\n",
      "         129       0.06      0.28      0.10        50\n",
      "         130       0.08      0.02      0.03        50\n",
      "         131       0.26      0.10      0.14        50\n",
      "         132       0.00      0.00      0.00        50\n",
      "         133       0.00      0.00      0.00        50\n",
      "         134       0.00      0.00      0.00        50\n",
      "         135       0.50      0.02      0.04        50\n",
      "         136       0.40      0.32      0.36        50\n",
      "         137       0.27      0.16      0.20        50\n",
      "         138       0.00      0.00      0.00        50\n",
      "         139       0.20      0.02      0.04        50\n",
      "         140       0.08      0.04      0.05        50\n",
      "         141       0.00      0.00      0.00        50\n",
      "         142       0.37      0.14      0.20        50\n",
      "         143       0.05      0.42      0.09        50\n",
      "         144       0.20      0.08      0.11        50\n",
      "         145       0.10      0.10      0.10        50\n",
      "         146       0.11      0.32      0.16        50\n",
      "         147       0.65      0.22      0.33        50\n",
      "         148       0.00      0.00      0.00        50\n",
      "         149       0.00      0.00      0.00        50\n",
      "         150       0.11      0.10      0.10        50\n",
      "         151       0.46      0.52      0.49        50\n",
      "         152       0.43      0.26      0.33        50\n",
      "         153       0.00      0.00      0.00        50\n",
      "         154       0.40      0.24      0.30        50\n",
      "         155       0.04      0.02      0.03        50\n",
      "         156       0.00      0.00      0.00        50\n",
      "         157       0.11      0.02      0.03        50\n",
      "         158       1.00      0.06      0.11        50\n",
      "         159       1.00      0.02      0.04        50\n",
      "         160       0.25      0.08      0.12        50\n",
      "         161       0.19      0.06      0.09        50\n",
      "         162       0.14      0.28      0.19        50\n",
      "         163       0.02      0.02      0.02        50\n",
      "         164       0.10      0.86      0.17        50\n",
      "         165       0.02      0.04      0.03        50\n",
      "         166       0.62      0.26      0.37        50\n",
      "         167       0.11      0.32      0.16        50\n",
      "         168       0.00      0.00      0.00        50\n",
      "         169       0.20      0.02      0.04        50\n",
      "         170       0.00      0.00      0.00        50\n",
      "         171       0.19      0.22      0.21        50\n",
      "         172       0.06      0.14      0.08        50\n",
      "         173       0.20      0.02      0.04        50\n",
      "         174       0.25      0.02      0.04        50\n",
      "         175       0.09      0.12      0.10        50\n",
      "         176       0.15      0.16      0.16        50\n",
      "         177       0.00      0.00      0.00        50\n",
      "         178       0.18      0.08      0.11        50\n",
      "         179       0.13      0.08      0.10        50\n",
      "         180       0.36      0.10      0.16        50\n",
      "         181       0.27      0.30      0.29        50\n",
      "         182       0.24      0.14      0.18        50\n",
      "         183       0.00      0.00      0.00        50\n",
      "         184       0.32      0.48      0.38        50\n",
      "         185       0.30      0.16      0.21        50\n",
      "         186       0.11      0.22      0.15        50\n",
      "         187       0.26      0.44      0.32        50\n",
      "         188       0.30      0.20      0.24        50\n",
      "         189       0.00      0.00      0.00        50\n",
      "         190       0.00      0.00      0.00        50\n",
      "         191       0.67      0.12      0.20        50\n",
      "         192       0.00      0.00      0.00        50\n",
      "         193       0.00      0.00      0.00        50\n",
      "         194       0.04      0.02      0.03        50\n",
      "         195       0.07      0.06      0.07        50\n",
      "         196       0.15      0.22      0.18        50\n",
      "         197       0.05      0.12      0.07        50\n",
      "         198       0.00      0.00      0.00        50\n",
      "         199       0.09      0.06      0.07        50\n",
      "\n",
      "    accuracy                           0.11     10000\n",
      "   macro avg       0.16      0.11      0.09     10000\n",
      "weighted avg       0.16      0.11      0.09     10000\n",
      "\n",
      "Epoch 0 - Train Loss: 4.257580871777156 - Val Loss: 4.209876286832592 - Acc: 0.11 - F1: 0.09123342010031855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c86831b81814dc198ff9fc72d26b8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.50      0.37        50\n",
      "           1       0.00      0.00      0.00        50\n",
      "           2       0.11      0.16      0.13        50\n",
      "           3       0.61      0.28      0.38        50\n",
      "           4       0.00      0.00      0.00        50\n",
      "           5       0.10      0.12      0.11        50\n",
      "           6       0.00      0.00      0.00        50\n",
      "           7       0.22      0.46      0.29        50\n",
      "           8       0.42      0.32      0.36        50\n",
      "           9       0.00      0.00      0.00        50\n",
      "          10       0.18      0.08      0.11        50\n",
      "          11       0.28      0.20      0.23        50\n",
      "          12       0.00      0.00      0.00        50\n",
      "          13       0.11      0.02      0.03        50\n",
      "          14       0.49      0.66      0.56        50\n",
      "          15       0.17      0.44      0.24        50\n",
      "          16       0.55      0.58      0.56        50\n",
      "          17       0.35      0.12      0.18        50\n",
      "          18       0.23      0.22      0.22        50\n",
      "          19       0.26      0.20      0.23        50\n",
      "          20       0.00      0.00      0.00        50\n",
      "          21       0.47      0.30      0.37        50\n",
      "          22       0.50      0.02      0.04        50\n",
      "          23       0.27      0.54      0.36        50\n",
      "          24       0.28      0.10      0.15        50\n",
      "          25       0.24      0.28      0.26        50\n",
      "          26       0.13      0.12      0.13        50\n",
      "          27       0.12      0.18      0.15        50\n",
      "          28       0.16      0.60      0.25        50\n",
      "          29       0.11      0.20      0.14        50\n",
      "          30       0.08      0.02      0.03        50\n",
      "          31       0.06      0.12      0.08        50\n",
      "          32       0.17      0.44      0.25        50\n",
      "          33       0.26      0.14      0.18        50\n",
      "          34       0.47      0.28      0.35        50\n",
      "          35       0.26      0.54      0.35        50\n",
      "          36       0.08      0.02      0.03        50\n",
      "          37       0.80      0.08      0.15        50\n",
      "          38       0.12      0.02      0.03        50\n",
      "          39       0.16      0.54      0.25        50\n",
      "          40       0.58      0.42      0.49        50\n",
      "          41       0.33      0.28      0.30        50\n",
      "          42       0.00      0.00      0.00        50\n",
      "          43       0.68      0.30      0.42        50\n",
      "          44       0.16      0.28      0.20        50\n",
      "          45       0.06      0.02      0.03        50\n",
      "          46       0.21      0.12      0.15        50\n",
      "          47       0.04      0.02      0.03        50\n",
      "          48       0.37      0.48      0.42        50\n",
      "          49       0.15      0.26      0.19        50\n",
      "          50       0.80      0.24      0.37        50\n",
      "          51       0.19      0.60      0.29        50\n",
      "          52       0.50      0.02      0.04        50\n",
      "          53       0.29      0.04      0.07        50\n",
      "          54       0.28      0.18      0.22        50\n",
      "          55       0.20      0.52      0.29        50\n",
      "          56       0.46      0.26      0.33        50\n",
      "          57       0.25      0.04      0.07        50\n",
      "          58       0.17      0.18      0.18        50\n",
      "          59       0.30      0.28      0.29        50\n",
      "          60       0.35      0.16      0.22        50\n",
      "          61       0.17      0.38      0.23        50\n",
      "          62       0.65      0.40      0.49        50\n",
      "          63       0.29      0.42      0.34        50\n",
      "          64       0.21      0.06      0.09        50\n",
      "          65       0.30      0.42      0.35        50\n",
      "          66       0.10      0.02      0.03        50\n",
      "          67       0.19      0.16      0.17        50\n",
      "          68       1.00      0.10      0.18        50\n",
      "          69       0.22      0.40      0.29        50\n",
      "          70       0.28      0.50      0.36        50\n",
      "          71       0.19      0.22      0.20        50\n",
      "          72       0.55      0.58      0.56        50\n",
      "          73       0.60      0.06      0.11        50\n",
      "          74       0.43      0.42      0.42        50\n",
      "          75       0.33      0.16      0.22        50\n",
      "          76       0.31      0.10      0.15        50\n",
      "          77       0.43      0.12      0.19        50\n",
      "          78       0.15      0.24      0.19        50\n",
      "          79       0.18      0.08      0.11        50\n",
      "          80       0.45      0.18      0.26        50\n",
      "          81       0.37      0.76      0.50        50\n",
      "          82       0.26      0.16      0.20        50\n",
      "          83       0.14      0.06      0.08        50\n",
      "          84       0.40      0.08      0.13        50\n",
      "          85       0.27      0.12      0.17        50\n",
      "          86       0.00      0.00      0.00        50\n",
      "          87       0.30      0.26      0.28        50\n",
      "          88       0.20      0.04      0.07        50\n",
      "          89       0.25      0.38      0.30        50\n",
      "          90       0.13      0.08      0.10        50\n",
      "          91       0.00      0.00      0.00        50\n",
      "          92       0.44      0.16      0.24        50\n",
      "          93       0.14      0.16      0.15        50\n",
      "          94       0.12      0.10      0.11        50\n",
      "          95       0.23      0.06      0.10        50\n",
      "          96       0.32      0.40      0.35        50\n",
      "          97       0.65      0.26      0.37        50\n",
      "          98       0.15      0.38      0.21        50\n",
      "          99       0.12      0.14      0.13        50\n",
      "         100       0.21      0.30      0.24        50\n",
      "         101       0.00      0.00      0.00        50\n",
      "         102       0.06      0.04      0.05        50\n",
      "         103       0.67      0.16      0.26        50\n",
      "         104       0.16      0.10      0.12        50\n",
      "         105       0.33      0.16      0.22        50\n",
      "         106       0.32      0.14      0.19        50\n",
      "         107       0.00      0.00      0.00        50\n",
      "         108       0.11      0.24      0.15        50\n",
      "         109       0.14      0.24      0.18        50\n",
      "         110       0.36      0.34      0.35        50\n",
      "         111       0.24      0.42      0.30        50\n",
      "         112       0.37      0.20      0.26        50\n",
      "         113       0.04      0.06      0.05        50\n",
      "         114       0.50      0.02      0.04        50\n",
      "         115       0.13      0.78      0.22        50\n",
      "         116       0.36      0.16      0.22        50\n",
      "         117       0.12      0.04      0.06        50\n",
      "         118       0.14      0.10      0.12        50\n",
      "         119       0.07      0.06      0.06        50\n",
      "         120       0.25      0.52      0.34        50\n",
      "         121       0.07      0.04      0.05        50\n",
      "         122       0.07      0.08      0.07        50\n",
      "         123       0.08      0.04      0.05        50\n",
      "         124       0.85      0.22      0.35        50\n",
      "         125       0.24      0.08      0.12        50\n",
      "         126       0.30      0.28      0.29        50\n",
      "         127       0.10      0.30      0.15        50\n",
      "         128       0.00      0.00      0.00        50\n",
      "         129       0.56      0.10      0.17        50\n",
      "         130       0.13      0.24      0.17        50\n",
      "         131       0.35      0.14      0.20        50\n",
      "         132       0.10      0.02      0.03        50\n",
      "         133       0.09      0.02      0.03        50\n",
      "         134       0.15      0.44      0.22        50\n",
      "         135       0.26      0.22      0.24        50\n",
      "         136       0.35      0.52      0.42        50\n",
      "         137       0.17      0.34      0.22        50\n",
      "         138       0.21      0.42      0.28        50\n",
      "         139       0.22      0.04      0.07        50\n",
      "         140       0.11      0.50      0.18        50\n",
      "         141       0.00      0.00      0.00        50\n",
      "         142       0.35      0.16      0.22        50\n",
      "         143       0.33      0.06      0.10        50\n",
      "         144       0.40      0.04      0.07        50\n",
      "         145       0.19      0.12      0.15        50\n",
      "         146       0.25      0.04      0.07        50\n",
      "         147       0.39      0.48      0.43        50\n",
      "         148       0.03      0.02      0.03        50\n",
      "         149       0.08      0.24      0.12        50\n",
      "         150       0.45      0.44      0.44        50\n",
      "         151       0.30      0.64      0.41        50\n",
      "         152       0.37      0.66      0.47        50\n",
      "         153       0.38      0.06      0.10        50\n",
      "         154       0.21      0.40      0.28        50\n",
      "         155       0.42      0.10      0.16        50\n",
      "         156       0.00      0.00      0.00        50\n",
      "         157       0.07      0.10      0.08        50\n",
      "         158       0.11      0.18      0.14        50\n",
      "         159       0.25      0.32      0.28        50\n",
      "         160       0.20      0.40      0.27        50\n",
      "         161       0.24      0.18      0.20        50\n",
      "         162       0.32      0.38      0.35        50\n",
      "         163       0.00      0.00      0.00        50\n",
      "         164       0.42      0.62      0.50        50\n",
      "         165       0.10      0.12      0.11        50\n",
      "         166       0.38      0.60      0.46        50\n",
      "         167       0.16      0.54      0.24        50\n",
      "         168       0.18      0.42      0.25        50\n",
      "         169       0.20      0.06      0.09        50\n",
      "         170       0.25      0.06      0.10        50\n",
      "         171       0.75      0.12      0.21        50\n",
      "         172       0.00      0.00      0.00        50\n",
      "         173       0.42      0.20      0.27        50\n",
      "         174       0.19      0.36      0.25        50\n",
      "         175       0.24      0.24      0.24        50\n",
      "         176       0.36      0.08      0.13        50\n",
      "         177       0.21      0.12      0.15        50\n",
      "         178       0.32      0.44      0.37        50\n",
      "         179       0.17      0.06      0.09        50\n",
      "         180       0.18      0.34      0.24        50\n",
      "         181       0.30      0.58      0.40        50\n",
      "         182       0.32      0.42      0.36        50\n",
      "         183       0.06      0.02      0.03        50\n",
      "         184       0.46      0.52      0.49        50\n",
      "         185       0.25      0.06      0.10        50\n",
      "         186       0.55      0.24      0.33        50\n",
      "         187       0.30      0.70      0.42        50\n",
      "         188       0.13      0.50      0.21        50\n",
      "         189       0.20      0.02      0.04        50\n",
      "         190       0.33      0.02      0.04        50\n",
      "         191       0.43      0.74      0.54        50\n",
      "         192       0.00      0.00      0.00        50\n",
      "         193       0.38      0.60      0.47        50\n",
      "         194       0.12      0.10      0.11        50\n",
      "         195       0.00      0.00      0.00        50\n",
      "         196       0.00      0.00      0.00        50\n",
      "         197       0.13      0.30      0.18        50\n",
      "         198       0.05      0.02      0.03        50\n",
      "         199       0.28      0.32      0.30        50\n",
      "\n",
      "    accuracy                           0.22     10000\n",
      "   macro avg       0.25      0.22      0.20     10000\n",
      "weighted avg       0.25      0.22      0.20     10000\n",
      "\n",
      "Epoch 1 - Train Loss: 3.4152113613875015 - Val Loss: 3.4274102736123 - Acc: 0.2245 - F1: 0.19733524828520196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f2d90a836745ccb0b40418bd47a6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.46      0.46        50\n",
      "           1       0.07      0.12      0.09        50\n",
      "           2       0.35      0.14      0.20        50\n",
      "           3       0.50      0.20      0.29        50\n",
      "           4       0.14      0.02      0.04        50\n",
      "           5       0.14      0.04      0.06        50\n",
      "           6       0.02      0.02      0.02        50\n",
      "           7       0.31      0.46      0.37        50\n",
      "           8       0.45      0.34      0.39        50\n",
      "           9       0.00      0.00      0.00        50\n",
      "          10       0.25      0.18      0.21        50\n",
      "          11       0.16      0.38      0.23        50\n",
      "          12       0.25      0.26      0.26        50\n",
      "          13       0.35      0.12      0.18        50\n",
      "          14       0.47      0.74      0.58        50\n",
      "          15       0.11      0.58      0.18        50\n",
      "          16       0.30      0.64      0.41        50\n",
      "          17       0.13      0.10      0.11        50\n",
      "          18       0.42      0.10      0.16        50\n",
      "          19       0.25      0.26      0.25        50\n",
      "          20       0.14      0.04      0.06        50\n",
      "          21       0.60      0.18      0.28        50\n",
      "          22       0.10      0.32      0.15        50\n",
      "          23       0.39      0.44      0.42        50\n",
      "          24       0.52      0.24      0.33        50\n",
      "          25       0.21      0.38      0.27        50\n",
      "          26       0.36      0.26      0.30        50\n",
      "          27       0.27      0.12      0.17        50\n",
      "          28       0.50      0.34      0.40        50\n",
      "          29       0.21      0.14      0.17        50\n",
      "          30       0.10      0.08      0.09        50\n",
      "          31       0.09      0.14      0.11        50\n",
      "          32       0.29      0.40      0.33        50\n",
      "          33       0.18      0.28      0.22        50\n",
      "          34       0.40      0.44      0.42        50\n",
      "          35       0.53      0.38      0.44        50\n",
      "          36       0.11      0.20      0.14        50\n",
      "          37       0.18      0.22      0.20        50\n",
      "          38       0.20      0.04      0.07        50\n",
      "          39       0.45      0.20      0.28        50\n",
      "          40       0.35      0.66      0.46        50\n",
      "          41       0.33      0.04      0.07        50\n",
      "          42       0.00      0.00      0.00        50\n",
      "          43       0.60      0.24      0.34        50\n",
      "          44       0.20      0.24      0.22        50\n",
      "          45       0.19      0.08      0.11        50\n",
      "          46       0.14      0.58      0.23        50\n",
      "          47       0.14      0.26      0.18        50\n",
      "          48       0.78      0.36      0.49        50\n",
      "          49       0.21      0.12      0.15        50\n",
      "          50       0.51      0.46      0.48        50\n",
      "          51       0.38      0.48      0.42        50\n",
      "          52       0.07      0.04      0.05        50\n",
      "          53       0.14      0.06      0.08        50\n",
      "          54       0.24      0.18      0.20        50\n",
      "          55       0.31      0.22      0.26        50\n",
      "          56       0.32      0.26      0.29        50\n",
      "          57       0.50      0.04      0.07        50\n",
      "          58       0.33      0.10      0.15        50\n",
      "          59       0.38      0.16      0.23        50\n",
      "          60       0.52      0.46      0.49        50\n",
      "          61       0.27      0.26      0.26        50\n",
      "          62       0.44      0.52      0.48        50\n",
      "          63       0.52      0.22      0.31        50\n",
      "          64       0.50      0.16      0.24        50\n",
      "          65       0.38      0.42      0.40        50\n",
      "          66       0.22      0.20      0.21        50\n",
      "          67       0.57      0.08      0.14        50\n",
      "          68       0.58      0.30      0.39        50\n",
      "          69       0.48      0.20      0.28        50\n",
      "          70       0.33      0.42      0.37        50\n",
      "          71       0.54      0.26      0.35        50\n",
      "          72       0.61      0.56      0.58        50\n",
      "          73       0.33      0.04      0.07        50\n",
      "          74       0.38      0.40      0.39        50\n",
      "          75       0.71      0.30      0.42        50\n",
      "          76       0.09      0.02      0.03        50\n",
      "          77       0.25      0.08      0.12        50\n",
      "          78       0.18      0.20      0.19        50\n",
      "          79       0.19      0.16      0.17        50\n",
      "          80       0.83      0.20      0.32        50\n",
      "          81       0.25      0.82      0.39        50\n",
      "          82       0.10      0.12      0.11        50\n",
      "          83       0.22      0.28      0.24        50\n",
      "          84       0.33      0.26      0.29        50\n",
      "          85       0.60      0.06      0.11        50\n",
      "          86       0.26      0.18      0.21        50\n",
      "          87       0.33      0.28      0.30        50\n",
      "          88       0.11      0.02      0.03        50\n",
      "          89       0.19      0.38      0.26        50\n",
      "          90       0.14      0.10      0.11        50\n",
      "          91       0.67      0.04      0.08        50\n",
      "          92       0.37      0.28      0.32        50\n",
      "          93       0.22      0.08      0.12        50\n",
      "          94       0.00      0.00      0.00        50\n",
      "          95       0.18      0.08      0.11        50\n",
      "          96       0.39      0.54      0.45        50\n",
      "          97       0.93      0.26      0.41        50\n",
      "          98       0.30      0.22      0.25        50\n",
      "          99       0.17      0.20      0.18        50\n",
      "         100       0.18      0.30      0.22        50\n",
      "         101       0.09      0.06      0.07        50\n",
      "         102       0.10      0.38      0.16        50\n",
      "         103       0.22      0.40      0.28        50\n",
      "         104       0.29      0.34      0.31        50\n",
      "         105       0.39      0.18      0.25        50\n",
      "         106       0.25      0.34      0.29        50\n",
      "         107       0.11      0.02      0.03        50\n",
      "         108       0.13      0.18      0.15        50\n",
      "         109       0.25      0.28      0.26        50\n",
      "         110       0.24      0.34      0.28        50\n",
      "         111       0.33      0.34      0.34        50\n",
      "         112       0.55      0.34      0.42        50\n",
      "         113       0.28      0.16      0.20        50\n",
      "         114       0.80      0.08      0.15        50\n",
      "         115       0.36      0.76      0.49        50\n",
      "         116       0.37      0.34      0.35        50\n",
      "         117       0.11      0.14      0.12        50\n",
      "         118       0.08      0.04      0.05        50\n",
      "         119       0.05      0.02      0.03        50\n",
      "         120       0.33      0.44      0.38        50\n",
      "         121       0.00      0.00      0.00        50\n",
      "         122       0.00      0.00      0.00        50\n",
      "         123       0.09      0.04      0.06        50\n",
      "         124       0.60      0.54      0.57        50\n",
      "         125       0.62      0.10      0.17        50\n",
      "         126       0.47      0.30      0.37        50\n",
      "         127       0.21      0.18      0.20        50\n",
      "         128       0.12      0.04      0.06        50\n",
      "         129       0.08      0.62      0.15        50\n",
      "         130       0.22      0.28      0.24        50\n",
      "         131       0.41      0.26      0.32        50\n",
      "         132       0.22      0.10      0.14        50\n",
      "         133       0.00      0.00      0.00        50\n",
      "         134       0.24      0.36      0.29        50\n",
      "         135       0.19      0.30      0.23        50\n",
      "         136       0.42      0.50      0.45        50\n",
      "         137       0.20      0.50      0.29        50\n",
      "         138       0.15      0.44      0.22        50\n",
      "         139       0.44      0.08      0.14        50\n",
      "         140       0.05      0.48      0.10        50\n",
      "         141       0.12      0.02      0.03        50\n",
      "         142       0.39      0.14      0.21        50\n",
      "         143       0.62      0.16      0.25        50\n",
      "         144       0.33      0.04      0.07        50\n",
      "         145       0.00      0.00      0.00        50\n",
      "         146       0.35      0.14      0.20        50\n",
      "         147       0.23      0.48      0.31        50\n",
      "         148       0.15      0.04      0.06        50\n",
      "         149       0.04      0.08      0.06        50\n",
      "         150       0.51      0.38      0.44        50\n",
      "         151       0.32      0.72      0.44        50\n",
      "         152       0.52      0.60      0.56        50\n",
      "         153       0.09      0.04      0.05        50\n",
      "         154       0.30      0.58      0.40        50\n",
      "         155       0.38      0.10      0.16        50\n",
      "         156       0.33      0.16      0.22        50\n",
      "         157       0.20      0.18      0.19        50\n",
      "         158       0.33      0.22      0.27        50\n",
      "         159       0.12      0.24      0.16        50\n",
      "         160       0.11      0.42      0.18        50\n",
      "         161       0.32      0.24      0.27        50\n",
      "         162       0.26      0.46      0.34        50\n",
      "         163       0.08      0.06      0.07        50\n",
      "         164       0.60      0.18      0.28        50\n",
      "         165       0.06      0.04      0.05        50\n",
      "         166       0.44      0.66      0.53        50\n",
      "         167       0.35      0.38      0.37        50\n",
      "         168       0.36      0.30      0.33        50\n",
      "         169       0.25      0.02      0.04        50\n",
      "         170       0.27      0.18      0.22        50\n",
      "         171       0.61      0.38      0.47        50\n",
      "         172       0.06      0.02      0.03        50\n",
      "         173       0.26      0.26      0.26        50\n",
      "         174       0.21      0.36      0.26        50\n",
      "         175       0.25      0.38      0.30        50\n",
      "         176       0.30      0.06      0.10        50\n",
      "         177       0.25      0.02      0.04        50\n",
      "         178       0.40      0.44      0.42        50\n",
      "         179       0.20      0.22      0.21        50\n",
      "         180       0.22      0.26      0.24        50\n",
      "         181       0.53      0.36      0.43        50\n",
      "         182       0.30      0.52      0.38        50\n",
      "         183       0.21      0.22      0.22        50\n",
      "         184       0.57      0.62      0.60        50\n",
      "         185       0.26      0.40      0.32        50\n",
      "         186       0.23      0.52      0.32        50\n",
      "         187       0.71      0.48      0.57        50\n",
      "         188       0.18      0.18      0.18        50\n",
      "         189       0.50      0.02      0.04        50\n",
      "         190       0.00      0.00      0.00        50\n",
      "         191       0.83      0.40      0.54        50\n",
      "         192       0.57      0.08      0.14        50\n",
      "         193       0.64      0.46      0.53        50\n",
      "         194       0.19      0.06      0.09        50\n",
      "         195       0.15      0.10      0.12        50\n",
      "         196       0.18      0.04      0.07        50\n",
      "         197       0.33      0.24      0.28        50\n",
      "         198       0.10      0.10      0.10        50\n",
      "         199       0.29      0.30      0.29        50\n",
      "\n",
      "    accuracy                           0.25     10000\n",
      "   macro avg       0.30      0.25      0.24     10000\n",
      "weighted avg       0.30      0.25      0.24     10000\n",
      "\n",
      "Epoch 2 - Train Loss: 2.98885758667041 - Val Loss: 3.348406468765645 - Acc: 0.248 - F1: 0.23554506882362353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/riccardo/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed168d30a257467d8041b71e220ec35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.48      0.56        50\n",
      "           1       0.10      0.12      0.11        50\n",
      "           2       0.24      0.18      0.20        50\n",
      "           3       0.43      0.46      0.45        50\n",
      "           4       0.33      0.02      0.04        50\n",
      "           5       0.14      0.06      0.08        50\n",
      "           6       0.07      0.02      0.03        50\n",
      "           7       0.27      0.62      0.37        50\n",
      "           8       0.22      0.42      0.29        50\n",
      "           9       0.06      0.02      0.03        50\n",
      "          10       0.36      0.18      0.24        50\n",
      "          11       0.38      0.30      0.33        50\n",
      "          12       0.14      0.36      0.20        50\n",
      "          13       0.32      0.14      0.19        50\n",
      "          14       0.29      0.84      0.43        50\n",
      "          15       0.23      0.26      0.24        50\n",
      "          16       0.80      0.48      0.60        50\n",
      "          17       0.24      0.24      0.24        50\n",
      "          18       0.33      0.22      0.27        50\n",
      "          19       0.26      0.56      0.35        50\n",
      "          20       0.20      0.02      0.04        50\n",
      "          21       0.42      0.50      0.45        50\n",
      "          22       0.25      0.04      0.07        50\n",
      "          23       0.67      0.28      0.39        50\n",
      "          24       0.28      0.22      0.25        50\n",
      "          25       0.36      0.16      0.22        50\n",
      "          26       0.38      0.24      0.29        50\n",
      "          27       0.30      0.18      0.23        50\n",
      "          28       0.47      0.32      0.38        50\n",
      "          29       0.42      0.10      0.16        50\n",
      "          30       0.19      0.10      0.13        50\n",
      "          31       0.23      0.14      0.17        50\n",
      "          32       0.47      0.16      0.24        50\n",
      "          33       0.33      0.20      0.25        50\n",
      "          34       0.44      0.42      0.43        50\n",
      "          35       0.70      0.32      0.44        50\n",
      "          36       0.31      0.10      0.15        50\n",
      "          37       0.32      0.22      0.26        50\n",
      "          38       0.16      0.18      0.17        50\n",
      "          39       0.45      0.34      0.39        50\n",
      "          40       0.70      0.38      0.49        50\n",
      "          41       0.19      0.20      0.20        50\n",
      "          42       0.21      0.10      0.14        50\n",
      "          43       0.60      0.42      0.49        50\n",
      "          44       0.24      0.22      0.23        50\n",
      "          45       0.14      0.20      0.16        50\n",
      "          46       0.36      0.24      0.29        50\n",
      "          47       0.38      0.26      0.31        50\n",
      "          48       0.40      0.68      0.50        50\n",
      "          49       0.37      0.22      0.28        50\n",
      "          50       0.63      0.52      0.57        50\n",
      "          51       0.34      0.66      0.45        50\n",
      "          52       0.00      0.00      0.00        50\n",
      "          53       0.05      0.10      0.07        50\n",
      "          54       0.30      0.12      0.17        50\n",
      "          55       0.53      0.20      0.29        50\n",
      "          56       0.33      0.40      0.36        50\n",
      "          57       0.12      0.14      0.13        50\n",
      "          58       0.29      0.48      0.36        50\n",
      "          59       0.33      0.22      0.27        50\n",
      "          60       0.39      0.56      0.46        50\n",
      "          61       0.23      0.14      0.17        50\n",
      "          62       0.75      0.42      0.54        50\n",
      "          63       0.46      0.34      0.39        50\n",
      "          64       0.46      0.12      0.19        50\n",
      "          65       0.75      0.30      0.43        50\n",
      "          66       0.19      0.24      0.21        50\n",
      "          67       0.22      0.16      0.18        50\n",
      "          68       0.31      0.64      0.42        50\n",
      "          69       0.24      0.56      0.33        50\n",
      "          70       0.22      0.62      0.33        50\n",
      "          71       0.27      0.64      0.38        50\n",
      "          72       0.56      0.68      0.61        50\n",
      "          73       0.21      0.14      0.17        50\n",
      "          74       0.38      0.64      0.48        50\n",
      "          75       0.30      0.48      0.37        50\n",
      "          76       0.44      0.16      0.24        50\n",
      "          77       0.29      0.04      0.07        50\n",
      "          78       0.24      0.16      0.19        50\n",
      "          79       0.20      0.16      0.18        50\n",
      "          80       0.39      0.30      0.34        50\n",
      "          81       0.63      0.62      0.63        50\n",
      "          82       0.19      0.18      0.19        50\n",
      "          83       0.23      0.06      0.10        50\n",
      "          84       0.18      0.62      0.28        50\n",
      "          85       0.59      0.20      0.30        50\n",
      "          86       0.19      0.10      0.13        50\n",
      "          87       0.48      0.20      0.28        50\n",
      "          88       0.11      0.12      0.11        50\n",
      "          89       0.54      0.14      0.22        50\n",
      "          90       0.12      0.16      0.14        50\n",
      "          91       0.11      0.22      0.15        50\n",
      "          92       0.31      0.38      0.34        50\n",
      "          93       0.14      0.04      0.06        50\n",
      "          94       0.27      0.12      0.17        50\n",
      "          95       0.24      0.28      0.26        50\n",
      "          96       0.49      0.40      0.44        50\n",
      "          97       0.33      0.64      0.44        50\n",
      "          98       0.16      0.48      0.24        50\n",
      "          99       0.26      0.20      0.22        50\n",
      "         100       0.32      0.28      0.30        50\n",
      "         101       0.32      0.16      0.21        50\n",
      "         102       0.17      0.28      0.21        50\n",
      "         103       1.00      0.14      0.25        50\n",
      "         104       0.19      0.06      0.09        50\n",
      "         105       0.39      0.30      0.34        50\n",
      "         106       0.29      0.22      0.25        50\n",
      "         107       0.67      0.04      0.08        50\n",
      "         108       0.14      0.32      0.20        50\n",
      "         109       0.18      0.36      0.24        50\n",
      "         110       0.29      0.40      0.33        50\n",
      "         111       0.35      0.34      0.35        50\n",
      "         112       0.68      0.42      0.52        50\n",
      "         113       0.18      0.10      0.13        50\n",
      "         114       0.30      0.34      0.32        50\n",
      "         115       0.39      0.62      0.48        50\n",
      "         116       0.49      0.44      0.46        50\n",
      "         117       0.19      0.30      0.23        50\n",
      "         118       0.27      0.12      0.17        50\n",
      "         119       0.32      0.12      0.17        50\n",
      "         120       0.67      0.40      0.50        50\n",
      "         121       0.04      0.02      0.03        50\n",
      "         122       0.83      0.10      0.18        50\n",
      "         123       0.04      0.02      0.03        50\n",
      "         124       0.55      0.48      0.51        50\n",
      "         125       0.38      0.18      0.24        50\n",
      "         126       0.59      0.26      0.36        50\n",
      "         127       0.16      0.10      0.12        50\n",
      "         128       0.15      0.16      0.15        50\n",
      "         129       0.28      0.32      0.30        50\n",
      "         130       0.23      0.22      0.23        50\n",
      "         131       0.23      0.28      0.25        50\n",
      "         132       0.10      0.06      0.07        50\n",
      "         133       0.14      0.02      0.04        50\n",
      "         134       0.42      0.20      0.27        50\n",
      "         135       0.56      0.10      0.17        50\n",
      "         136       0.35      0.56      0.43        50\n",
      "         137       0.57      0.32      0.41        50\n",
      "         138       0.43      0.24      0.31        50\n",
      "         139       0.15      0.26      0.19        50\n",
      "         140       0.67      0.12      0.20        50\n",
      "         141       0.16      0.08      0.11        50\n",
      "         142       0.35      0.52      0.42        50\n",
      "         143       0.40      0.16      0.23        50\n",
      "         144       0.16      0.32      0.21        50\n",
      "         145       0.19      0.24      0.21        50\n",
      "         146       0.36      0.08      0.13        50\n",
      "         147       0.55      0.52      0.54        50\n",
      "         148       0.06      0.02      0.03        50\n",
      "         149       0.14      0.08      0.10        50\n",
      "         150       0.57      0.58      0.57        50\n",
      "         151       0.36      0.76      0.48        50\n",
      "         152       0.81      0.52      0.63        50\n",
      "         153       0.36      0.24      0.29        50\n",
      "         154       0.34      0.28      0.31        50\n",
      "         155       0.50      0.18      0.26        50\n",
      "         156       0.08      0.36      0.13        50\n",
      "         157       0.11      0.22      0.15        50\n",
      "         158       0.25      0.34      0.29        50\n",
      "         159       0.25      0.54      0.34        50\n",
      "         160       0.50      0.20      0.29        50\n",
      "         161       0.28      0.24      0.26        50\n",
      "         162       0.27      0.40      0.32        50\n",
      "         163       0.11      0.22      0.15        50\n",
      "         164       0.57      0.70      0.63        50\n",
      "         165       0.10      0.08      0.09        50\n",
      "         166       0.55      0.56      0.55        50\n",
      "         167       0.38      0.48      0.42        50\n",
      "         168       0.33      0.34      0.34        50\n",
      "         169       0.30      0.06      0.10        50\n",
      "         170       0.20      0.06      0.09        50\n",
      "         171       0.59      0.44      0.51        50\n",
      "         172       0.12      0.18      0.14        50\n",
      "         173       0.23      0.36      0.28        50\n",
      "         174       0.28      0.26      0.27        50\n",
      "         175       0.34      0.42      0.38        50\n",
      "         176       0.17      0.18      0.18        50\n",
      "         177       0.21      0.32      0.25        50\n",
      "         178       0.46      0.56      0.50        50\n",
      "         179       0.16      0.32      0.21        50\n",
      "         180       0.20      0.18      0.19        50\n",
      "         181       0.28      0.70      0.40        50\n",
      "         182       0.49      0.62      0.55        50\n",
      "         183       0.12      0.14      0.13        50\n",
      "         184       0.43      0.52      0.47        50\n",
      "         185       0.19      0.50      0.27        50\n",
      "         186       0.35      0.34      0.34        50\n",
      "         187       0.76      0.52      0.62        50\n",
      "         188       0.42      0.40      0.41        50\n",
      "         189       0.00      0.00      0.00        50\n",
      "         190       0.10      0.08      0.09        50\n",
      "         191       0.69      0.58      0.63        50\n",
      "         192       0.36      0.30      0.33        50\n",
      "         193       0.39      0.66      0.49        50\n",
      "         194       0.10      0.30      0.15        50\n",
      "         195       0.29      0.14      0.19        50\n",
      "         196       0.26      0.22      0.24        50\n",
      "         197       0.25      0.30      0.27        50\n",
      "         198       0.19      0.10      0.13        50\n",
      "         199       0.34      0.40      0.37        50\n",
      "\n",
      "    accuracy                           0.29     10000\n",
      "   macro avg       0.33      0.29      0.28     10000\n",
      "weighted avg       0.33      0.29      0.28     10000\n",
      "\n",
      "Epoch 3 - Train Loss: 2.6304585089159134 - Val Loss: 3.090012486976913 - Acc: 0.2892 - F1: 0.277593770790191\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a25abe589df4604955978179820419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.66      0.54        50\n",
      "           1       0.25      0.06      0.10        50\n",
      "           2       0.42      0.10      0.16        50\n",
      "           3       0.50      0.36      0.42        50\n",
      "           4       0.15      0.18      0.16        50\n",
      "           5       0.17      0.02      0.04        50\n",
      "           6       0.11      0.06      0.08        50\n",
      "           7       0.39      0.22      0.28        50\n",
      "           8       0.25      0.50      0.34        50\n",
      "           9       0.11      0.18      0.14        50\n",
      "          10       0.39      0.24      0.30        50\n",
      "          11       0.62      0.16      0.25        50\n",
      "          12       0.26      0.36      0.30        50\n",
      "          13       0.32      0.12      0.17        50\n",
      "          14       0.56      0.70      0.62        50\n",
      "          15       0.26      0.22      0.24        50\n",
      "          16       0.86      0.48      0.62        50\n",
      "          17       0.32      0.12      0.17        50\n",
      "          18       0.32      0.14      0.19        50\n",
      "          19       0.34      0.40      0.37        50\n",
      "          20       0.18      0.12      0.14        50\n",
      "          21       0.56      0.46      0.51        50\n",
      "          22       0.24      0.22      0.23        50\n",
      "          23       0.47      0.52      0.50        50\n",
      "          24       0.43      0.06      0.11        50\n",
      "          25       0.35      0.24      0.29        50\n",
      "          26       0.19      0.40      0.25        50\n",
      "          27       0.23      0.16      0.19        50\n",
      "          28       0.35      0.34      0.35        50\n",
      "          29       0.28      0.38      0.32        50\n",
      "          30       0.36      0.08      0.13        50\n",
      "          31       0.08      0.32      0.12        50\n",
      "          32       0.32      0.40      0.36        50\n",
      "          33       0.33      0.18      0.23        50\n",
      "          34       0.53      0.38      0.44        50\n",
      "          35       0.76      0.44      0.56        50\n",
      "          36       0.16      0.36      0.22        50\n",
      "          37       0.45      0.36      0.40        50\n",
      "          38       0.13      0.18      0.15        50\n",
      "          39       0.15      0.46      0.23        50\n",
      "          40       0.54      0.70      0.61        50\n",
      "          41       0.25      0.14      0.18        50\n",
      "          42       0.00      0.00      0.00        50\n",
      "          43       0.63      0.44      0.52        50\n",
      "          44       0.22      0.28      0.25        50\n",
      "          45       0.32      0.18      0.23        50\n",
      "          46       0.21      0.42      0.28        50\n",
      "          47       0.32      0.34      0.33        50\n",
      "          48       0.43      0.64      0.52        50\n",
      "          49       0.21      0.32      0.25        50\n",
      "          50       0.58      0.50      0.54        50\n",
      "          51       0.27      0.76      0.39        50\n",
      "          52       0.21      0.06      0.09        50\n",
      "          53       0.20      0.10      0.13        50\n",
      "          54       0.33      0.10      0.15        50\n",
      "          55       0.36      0.34      0.35        50\n",
      "          56       0.40      0.50      0.44        50\n",
      "          57       0.35      0.14      0.20        50\n",
      "          58       0.41      0.18      0.25        50\n",
      "          59       0.42      0.28      0.34        50\n",
      "          60       0.44      0.50      0.47        50\n",
      "          61       0.40      0.12      0.18        50\n",
      "          62       0.79      0.38      0.51        50\n",
      "          63       0.34      0.24      0.28        50\n",
      "          64       0.57      0.16      0.25        50\n",
      "          65       0.73      0.38      0.50        50\n",
      "          66       0.21      0.06      0.09        50\n",
      "          67       0.17      0.08      0.11        50\n",
      "          68       0.36      0.54      0.43        50\n",
      "          69       0.45      0.46      0.46        50\n",
      "          70       0.40      0.50      0.44        50\n",
      "          71       0.25      0.40      0.31        50\n",
      "          72       0.49      0.70      0.58        50\n",
      "          73       0.20      0.12      0.15        50\n",
      "          74       0.53      0.50      0.52        50\n",
      "          75       0.18      0.60      0.28        50\n",
      "          76       0.27      0.06      0.10        50\n",
      "          77       0.23      0.34      0.27        50\n",
      "          78       0.17      0.22      0.19        50\n",
      "          79       0.24      0.14      0.18        50\n",
      "          80       0.29      0.24      0.26        50\n",
      "          81       0.54      0.62      0.58        50\n",
      "          82       0.28      0.26      0.27        50\n",
      "          83       0.30      0.28      0.29        50\n",
      "          84       0.17      0.34      0.23        50\n",
      "          85       0.22      0.16      0.19        50\n",
      "          86       0.25      0.04      0.07        50\n",
      "          87       0.47      0.14      0.22        50\n",
      "          88       0.14      0.14      0.14        50\n",
      "          89       0.39      0.22      0.28        50\n",
      "          90       0.10      0.16      0.13        50\n",
      "          91       0.00      0.00      0.00        50\n",
      "          92       0.32      0.36      0.34        50\n",
      "          93       0.08      0.06      0.07        50\n",
      "          94       0.40      0.04      0.07        50\n",
      "          95       0.20      0.32      0.25        50\n",
      "          96       0.62      0.32      0.42        50\n",
      "          97       0.76      0.58      0.66        50\n",
      "          98       0.21      0.44      0.28        50\n",
      "          99       0.16      0.32      0.22        50\n",
      "         100       0.18      0.56      0.27        50\n",
      "         101       0.36      0.08      0.13        50\n",
      "         102       0.20      0.40      0.26        50\n",
      "         103       0.58      0.36      0.44        50\n",
      "         104       0.25      0.02      0.04        50\n",
      "         105       0.62      0.30      0.41        50\n",
      "         106       0.24      0.26      0.25        50\n",
      "         107       0.60      0.06      0.11        50\n",
      "         108       0.30      0.22      0.25        50\n",
      "         109       0.19      0.46      0.27        50\n",
      "         110       0.50      0.36      0.42        50\n",
      "         111       0.71      0.24      0.36        50\n",
      "         112       0.62      0.40      0.49        50\n",
      "         113       0.38      0.10      0.16        50\n",
      "         114       0.50      0.26      0.34        50\n",
      "         115       0.48      0.62      0.54        50\n",
      "         116       0.85      0.22      0.35        50\n",
      "         117       0.16      0.20      0.18        50\n",
      "         118       0.33      0.10      0.15        50\n",
      "         119       0.12      0.14      0.13        50\n",
      "         120       0.59      0.48      0.53        50\n",
      "         121       0.12      0.06      0.08        50\n",
      "         122       0.24      0.28      0.26        50\n",
      "         123       0.06      0.22      0.10        50\n",
      "         124       0.48      0.40      0.43        50\n",
      "         125       0.52      0.30      0.38        50\n",
      "         126       0.34      0.24      0.28        50\n",
      "         127       0.30      0.12      0.17        50\n",
      "         128       0.17      0.18      0.17        50\n",
      "         129       0.38      0.20      0.26        50\n",
      "         130       0.50      0.18      0.26        50\n",
      "         131       0.32      0.30      0.31        50\n",
      "         132       0.32      0.16      0.21        50\n",
      "         133       0.06      0.06      0.06        50\n",
      "         134       0.46      0.26      0.33        50\n",
      "         135       0.33      0.28      0.30        50\n",
      "         136       0.20      0.58      0.29        50\n",
      "         137       0.56      0.30      0.39        50\n",
      "         138       0.20      0.64      0.31        50\n",
      "         139       0.34      0.24      0.28        50\n",
      "         140       0.36      0.16      0.22        50\n",
      "         141       0.23      0.12      0.16        50\n",
      "         142       0.37      0.50      0.42        50\n",
      "         143       0.41      0.26      0.32        50\n",
      "         144       0.18      0.24      0.21        50\n",
      "         145       0.16      0.14      0.15        50\n",
      "         146       0.26      0.14      0.18        50\n",
      "         147       0.43      0.46      0.44        50\n",
      "         148       0.06      0.02      0.03        50\n",
      "         149       0.12      0.10      0.11        50\n",
      "         150       0.64      0.42      0.51        50\n",
      "         151       0.18      0.94      0.30        50\n",
      "         152       0.52      0.70      0.60        50\n",
      "         153       0.32      0.20      0.25        50\n",
      "         154       0.42      0.32      0.36        50\n",
      "         155       0.44      0.42      0.43        50\n",
      "         156       0.33      0.34      0.33        50\n",
      "         157       0.18      0.24      0.21        50\n",
      "         158       0.27      0.26      0.27        50\n",
      "         159       0.54      0.14      0.22        50\n",
      "         160       0.30      0.30      0.30        50\n",
      "         161       0.33      0.26      0.29        50\n",
      "         162       0.45      0.28      0.35        50\n",
      "         163       0.24      0.10      0.14        50\n",
      "         164       0.83      0.50      0.62        50\n",
      "         165       0.15      0.16      0.15        50\n",
      "         166       0.30      0.68      0.41        50\n",
      "         167       0.52      0.26      0.35        50\n",
      "         168       0.38      0.30      0.34        50\n",
      "         169       0.33      0.14      0.20        50\n",
      "         170       0.44      0.22      0.29        50\n",
      "         171       0.46      0.56      0.50        50\n",
      "         172       0.07      0.18      0.10        50\n",
      "         173       0.32      0.16      0.21        50\n",
      "         174       0.47      0.18      0.26        50\n",
      "         175       0.21      0.50      0.29        50\n",
      "         176       0.23      0.30      0.26        50\n",
      "         177       0.23      0.36      0.28        50\n",
      "         178       0.57      0.54      0.56        50\n",
      "         179       0.17      0.24      0.20        50\n",
      "         180       0.29      0.24      0.26        50\n",
      "         181       0.37      0.58      0.45        50\n",
      "         182       0.49      0.54      0.51        50\n",
      "         183       0.11      0.16      0.13        50\n",
      "         184       0.59      0.64      0.62        50\n",
      "         185       0.34      0.40      0.37        50\n",
      "         186       0.33      0.44      0.38        50\n",
      "         187       0.70      0.52      0.60        50\n",
      "         188       0.38      0.20      0.26        50\n",
      "         189       0.17      0.02      0.04        50\n",
      "         190       0.08      0.04      0.05        50\n",
      "         191       0.69      0.66      0.67        50\n",
      "         192       0.90      0.18      0.30        50\n",
      "         193       0.55      0.62      0.58        50\n",
      "         194       0.11      0.12      0.12        50\n",
      "         195       0.41      0.18      0.25        50\n",
      "         196       0.32      0.26      0.29        50\n",
      "         197       0.23      0.36      0.28        50\n",
      "         198       0.16      0.16      0.16        50\n",
      "         199       0.23      0.58      0.33        50\n",
      "\n",
      "    accuracy                           0.30     10000\n",
      "   macro avg       0.35      0.30      0.29     10000\n",
      "weighted avg       0.35      0.30      0.29     10000\n",
      "\n",
      "Epoch 4 - Train Loss: 2.268679962591137 - Val Loss: 3.17173401313492 - Acc: 0.2969 - F1: 0.28985016850788836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd2e85eac2549cea13a038e8eaac953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.46      0.57        50\n",
      "           1       0.20      0.12      0.15        50\n",
      "           2       0.12      0.36      0.18        50\n",
      "           3       0.65      0.34      0.45        50\n",
      "           4       0.13      0.24      0.17        50\n",
      "           5       0.13      0.10      0.11        50\n",
      "           6       0.07      0.10      0.08        50\n",
      "           7       0.37      0.44      0.40        50\n",
      "           8       0.39      0.44      0.41        50\n",
      "           9       0.13      0.08      0.10        50\n",
      "          10       0.16      0.40      0.23        50\n",
      "          11       0.22      0.38      0.28        50\n",
      "          12       0.35      0.12      0.18        50\n",
      "          13       0.40      0.16      0.23        50\n",
      "          14       0.65      0.56      0.60        50\n",
      "          15       0.35      0.30      0.32        50\n",
      "          16       0.28      0.72      0.41        50\n",
      "          17       0.25      0.16      0.20        50\n",
      "          18       0.33      0.36      0.34        50\n",
      "          19       0.25      0.42      0.31        50\n",
      "          20       0.50      0.02      0.04        50\n",
      "          21       0.50      0.62      0.55        50\n",
      "          22       0.20      0.10      0.13        50\n",
      "          23       0.50      0.36      0.42        50\n",
      "          24       0.33      0.46      0.39        50\n",
      "          25       0.25      0.26      0.25        50\n",
      "          26       0.26      0.28      0.27        50\n",
      "          27       0.41      0.22      0.29        50\n",
      "          28       0.54      0.40      0.46        50\n",
      "          29       0.31      0.28      0.29        50\n",
      "          30       0.28      0.10      0.15        50\n",
      "          31       0.33      0.24      0.28        50\n",
      "          32       0.38      0.42      0.40        50\n",
      "          33       0.36      0.42      0.39        50\n",
      "          34       0.31      0.62      0.41        50\n",
      "          35       0.61      0.50      0.55        50\n",
      "          36       0.09      0.04      0.06        50\n",
      "          37       0.23      0.38      0.29        50\n",
      "          38       0.28      0.26      0.27        50\n",
      "          39       0.43      0.44      0.44        50\n",
      "          40       0.42      0.72      0.53        50\n",
      "          41       0.48      0.20      0.28        50\n",
      "          42       0.04      0.02      0.03        50\n",
      "          43       0.53      0.40      0.45        50\n",
      "          44       0.50      0.12      0.19        50\n",
      "          45       0.25      0.22      0.23        50\n",
      "          46       0.18      0.64      0.28        50\n",
      "          47       0.12      0.10      0.11        50\n",
      "          48       0.75      0.36      0.49        50\n",
      "          49       0.39      0.22      0.28        50\n",
      "          50       0.63      0.62      0.63        50\n",
      "          51       0.70      0.46      0.55        50\n",
      "          52       0.07      0.02      0.03        50\n",
      "          53       0.24      0.16      0.19        50\n",
      "          54       0.20      0.24      0.22        50\n",
      "          55       0.36      0.20      0.26        50\n",
      "          56       0.55      0.36      0.43        50\n",
      "          57       0.16      0.16      0.16        50\n",
      "          58       0.25      0.40      0.31        50\n",
      "          59       0.36      0.38      0.37        50\n",
      "          60       0.47      0.38      0.42        50\n",
      "          61       0.30      0.28      0.29        50\n",
      "          62       0.45      0.58      0.51        50\n",
      "          63       0.47      0.34      0.40        50\n",
      "          64       0.36      0.40      0.38        50\n",
      "          65       0.35      0.54      0.43        50\n",
      "          66       0.11      0.16      0.13        50\n",
      "          67       0.21      0.18      0.20        50\n",
      "          68       0.31      0.72      0.43        50\n",
      "          69       0.32      0.42      0.37        50\n",
      "          70       0.56      0.40      0.47        50\n",
      "          71       0.50      0.26      0.34        50\n",
      "          72       0.76      0.56      0.64        50\n",
      "          73       0.23      0.14      0.17        50\n",
      "          74       0.51      0.44      0.47        50\n",
      "          75       0.65      0.30      0.41        50\n",
      "          76       0.25      0.24      0.24        50\n",
      "          77       0.56      0.20      0.29        50\n",
      "          78       0.42      0.38      0.40        50\n",
      "          79       0.16      0.26      0.20        50\n",
      "          80       0.50      0.48      0.49        50\n",
      "          81       0.38      0.78      0.51        50\n",
      "          82       0.18      0.32      0.23        50\n",
      "          83       0.27      0.24      0.25        50\n",
      "          84       0.26      0.32      0.29        50\n",
      "          85       0.25      0.28      0.26        50\n",
      "          86       0.22      0.12      0.16        50\n",
      "          87       0.20      0.42      0.27        50\n",
      "          88       0.23      0.14      0.17        50\n",
      "          89       0.22      0.48      0.30        50\n",
      "          90       0.15      0.20      0.17        50\n",
      "          91       0.22      0.18      0.20        50\n",
      "          92       0.23      0.54      0.33        50\n",
      "          93       0.21      0.16      0.18        50\n",
      "          94       0.36      0.08      0.13        50\n",
      "          95       0.45      0.18      0.26        50\n",
      "          96       1.00      0.08      0.15        50\n",
      "          97       0.69      0.50      0.58        50\n",
      "          98       0.32      0.18      0.23        50\n",
      "          99       0.50      0.26      0.34        50\n",
      "         100       0.23      0.38      0.29        50\n",
      "         101       0.09      0.06      0.07        50\n",
      "         102       0.67      0.08      0.14        50\n",
      "         103       0.27      0.42      0.33        50\n",
      "         104       0.32      0.14      0.19        50\n",
      "         105       0.43      0.46      0.44        50\n",
      "         106       0.35      0.26      0.30        50\n",
      "         107       0.06      0.20      0.09        50\n",
      "         108       0.39      0.30      0.34        50\n",
      "         109       0.18      0.50      0.26        50\n",
      "         110       0.39      0.50      0.44        50\n",
      "         111       0.33      0.34      0.33        50\n",
      "         112       0.39      0.60      0.48        50\n",
      "         113       0.22      0.36      0.27        50\n",
      "         114       0.28      0.18      0.22        50\n",
      "         115       0.52      0.68      0.59        50\n",
      "         116       0.45      0.40      0.43        50\n",
      "         117       0.15      0.22      0.18        50\n",
      "         118       0.15      0.18      0.16        50\n",
      "         119       0.33      0.06      0.10        50\n",
      "         120       0.29      0.66      0.40        50\n",
      "         121       0.25      0.06      0.10        50\n",
      "         122       0.22      0.04      0.07        50\n",
      "         123       0.00      0.00      0.00        50\n",
      "         124       0.46      0.68      0.55        50\n",
      "         125       0.29      0.40      0.34        50\n",
      "         126       0.61      0.28      0.38        50\n",
      "         127       0.25      0.18      0.21        50\n",
      "         128       0.40      0.12      0.18        50\n",
      "         129       0.24      0.26      0.25        50\n",
      "         130       0.30      0.44      0.35        50\n",
      "         131       0.49      0.40      0.44        50\n",
      "         132       0.25      0.12      0.16        50\n",
      "         133       0.07      0.06      0.06        50\n",
      "         134       0.33      0.42      0.37        50\n",
      "         135       0.39      0.24      0.30        50\n",
      "         136       0.55      0.42      0.48        50\n",
      "         137       0.41      0.48      0.44        50\n",
      "         138       0.23      0.54      0.33        50\n",
      "         139       0.17      0.30      0.21        50\n",
      "         140       0.24      0.22      0.23        50\n",
      "         141       0.25      0.04      0.07        50\n",
      "         142       0.43      0.42      0.42        50\n",
      "         143       0.46      0.48      0.47        50\n",
      "         144       0.21      0.36      0.27        50\n",
      "         145       0.32      0.12      0.17        50\n",
      "         146       0.23      0.38      0.29        50\n",
      "         147       0.43      0.58      0.50        50\n",
      "         148       0.04      0.06      0.05        50\n",
      "         149       0.11      0.12      0.11        50\n",
      "         150       0.33      0.70      0.45        50\n",
      "         151       0.71      0.70      0.71        50\n",
      "         152       0.67      0.62      0.65        50\n",
      "         153       0.23      0.30      0.26        50\n",
      "         154       0.43      0.32      0.37        50\n",
      "         155       0.90      0.18      0.30        50\n",
      "         156       0.34      0.20      0.25        50\n",
      "         157       0.45      0.30      0.36        50\n",
      "         158       0.26      0.42      0.32        50\n",
      "         159       0.25      0.44      0.32        50\n",
      "         160       0.43      0.32      0.37        50\n",
      "         161       0.28      0.28      0.28        50\n",
      "         162       0.45      0.34      0.39        50\n",
      "         163       0.21      0.10      0.14        50\n",
      "         164       0.62      0.72      0.67        50\n",
      "         165       0.20      0.04      0.07        50\n",
      "         166       0.59      0.64      0.62        50\n",
      "         167       0.47      0.54      0.50        50\n",
      "         168       0.36      0.36      0.36        50\n",
      "         169       0.22      0.04      0.07        50\n",
      "         170       0.40      0.24      0.30        50\n",
      "         171       0.60      0.54      0.57        50\n",
      "         172       0.33      0.08      0.13        50\n",
      "         173       0.15      0.44      0.23        50\n",
      "         174       0.36      0.16      0.22        50\n",
      "         175       0.33      0.42      0.37        50\n",
      "         176       0.31      0.34      0.32        50\n",
      "         177       0.39      0.26      0.31        50\n",
      "         178       0.72      0.42      0.53        50\n",
      "         179       0.26      0.10      0.14        50\n",
      "         180       0.42      0.26      0.32        50\n",
      "         181       0.62      0.40      0.49        50\n",
      "         182       0.42      0.42      0.42        50\n",
      "         183       0.62      0.10      0.17        50\n",
      "         184       0.67      0.60      0.63        50\n",
      "         185       0.41      0.22      0.29        50\n",
      "         186       0.52      0.22      0.31        50\n",
      "         187       0.59      0.70      0.64        50\n",
      "         188       0.33      0.28      0.30        50\n",
      "         189       0.21      0.12      0.15        50\n",
      "         190       0.12      0.12      0.12        50\n",
      "         191       0.83      0.60      0.70        50\n",
      "         192       0.27      0.30      0.28        50\n",
      "         193       0.51      0.52      0.51        50\n",
      "         194       0.26      0.16      0.20        50\n",
      "         195       0.42      0.26      0.32        50\n",
      "         196       0.38      0.18      0.24        50\n",
      "         197       0.40      0.28      0.33        50\n",
      "         198       0.33      0.06      0.10        50\n",
      "         199       0.31      0.44      0.36        50\n",
      "\n",
      "    accuracy                           0.32     10000\n",
      "   macro avg       0.36      0.32      0.31     10000\n",
      "weighted avg       0.36      0.32      0.31     10000\n",
      "\n",
      "Epoch 5 - Train Loss: 1.8689440279970388 - Val Loss: 3.097762156136428 - Acc: 0.319 - F1: 0.3104615190686414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737831e557bb4a03a4330ef9e792b85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.38      0.49        50\n",
      "           1       0.08      0.18      0.11        50\n",
      "           2       0.34      0.24      0.28        50\n",
      "           3       0.27      0.46      0.34        50\n",
      "           4       0.29      0.08      0.12        50\n",
      "           5       0.18      0.04      0.07        50\n",
      "           6       0.10      0.10      0.10        50\n",
      "           7       0.68      0.30      0.42        50\n",
      "           8       0.50      0.40      0.44        50\n",
      "           9       0.13      0.10      0.11        50\n",
      "          10       0.17      0.26      0.20        50\n",
      "          11       0.45      0.26      0.33        50\n",
      "          12       0.24      0.34      0.28        50\n",
      "          13       0.31      0.18      0.23        50\n",
      "          14       0.70      0.62      0.66        50\n",
      "          15       0.31      0.30      0.30        50\n",
      "          16       0.65      0.56      0.60        50\n",
      "          17       0.29      0.36      0.32        50\n",
      "          18       0.27      0.52      0.35        50\n",
      "          19       0.21      0.44      0.28        50\n",
      "          20       0.29      0.04      0.07        50\n",
      "          21       0.58      0.42      0.49        50\n",
      "          22       0.23      0.28      0.25        50\n",
      "          23       0.74      0.40      0.52        50\n",
      "          24       0.36      0.42      0.39        50\n",
      "          25       0.38      0.18      0.24        50\n",
      "          26       0.45      0.30      0.36        50\n",
      "          27       0.22      0.42      0.29        50\n",
      "          28       0.40      0.38      0.39        50\n",
      "          29       0.48      0.20      0.28        50\n",
      "          30       0.06      0.02      0.03        50\n",
      "          31       0.21      0.32      0.25        50\n",
      "          32       0.30      0.12      0.17        50\n",
      "          33       0.29      0.22      0.25        50\n",
      "          34       0.46      0.42      0.44        50\n",
      "          35       0.87      0.40      0.55        50\n",
      "          36       0.22      0.16      0.19        50\n",
      "          37       0.33      0.30      0.32        50\n",
      "          38       0.12      0.24      0.16        50\n",
      "          39       0.27      0.46      0.34        50\n",
      "          40       0.79      0.66      0.72        50\n",
      "          41       0.24      0.24      0.24        50\n",
      "          42       0.11      0.06      0.08        50\n",
      "          43       0.53      0.58      0.55        50\n",
      "          44       0.48      0.22      0.30        50\n",
      "          45       0.16      0.28      0.20        50\n",
      "          46       0.30      0.28      0.29        50\n",
      "          47       0.40      0.20      0.27        50\n",
      "          48       0.43      0.58      0.49        50\n",
      "          49       0.21      0.22      0.22        50\n",
      "          50       0.47      0.68      0.55        50\n",
      "          51       0.52      0.56      0.54        50\n",
      "          52       0.12      0.08      0.10        50\n",
      "          53       0.11      0.08      0.09        50\n",
      "          54       0.26      0.30      0.28        50\n",
      "          55       0.40      0.34      0.37        50\n",
      "          56       0.46      0.42      0.44        50\n",
      "          57       0.16      0.18      0.17        50\n",
      "          58       0.26      0.38      0.31        50\n",
      "          59       0.36      0.24      0.29        50\n",
      "          60       0.28      0.62      0.39        50\n",
      "          61       0.45      0.28      0.35        50\n",
      "          62       0.74      0.40      0.52        50\n",
      "          63       0.38      0.40      0.39        50\n",
      "          64       0.20      0.26      0.22        50\n",
      "          65       0.57      0.50      0.53        50\n",
      "          66       0.21      0.20      0.20        50\n",
      "          67       0.45      0.20      0.28        50\n",
      "          68       0.32      0.54      0.40        50\n",
      "          69       0.24      0.44      0.31        50\n",
      "          70       0.57      0.40      0.47        50\n",
      "          71       0.33      0.50      0.40        50\n",
      "          72       0.55      0.76      0.64        50\n",
      "          73       0.15      0.20      0.17        50\n",
      "          74       0.41      0.56      0.47        50\n",
      "          75       0.41      0.56      0.47        50\n",
      "          76       0.15      0.08      0.10        50\n",
      "          77       0.29      0.20      0.24        50\n",
      "          78       0.39      0.18      0.25        50\n",
      "          79       0.24      0.12      0.16        50\n",
      "          80       0.65      0.26      0.37        50\n",
      "          81       0.50      0.74      0.60        50\n",
      "          82       0.27      0.26      0.27        50\n",
      "          83       0.50      0.12      0.19        50\n",
      "          84       0.28      0.48      0.35        50\n",
      "          85       0.55      0.22      0.31        50\n",
      "          86       0.26      0.18      0.21        50\n",
      "          87       0.27      0.32      0.29        50\n",
      "          88       0.17      0.10      0.12        50\n",
      "          89       0.38      0.26      0.31        50\n",
      "          90       0.13      0.22      0.16        50\n",
      "          91       0.20      0.38      0.26        50\n",
      "          92       0.31      0.20      0.24        50\n",
      "          93       0.20      0.14      0.16        50\n",
      "          94       0.18      0.22      0.20        50\n",
      "          95       0.37      0.38      0.37        50\n",
      "          96       0.59      0.38      0.46        50\n",
      "          97       0.56      0.60      0.58        50\n",
      "          98       0.26      0.32      0.29        50\n",
      "          99       0.26      0.36      0.30        50\n",
      "         100       0.21      0.40      0.28        50\n",
      "         101       0.22      0.14      0.17        50\n",
      "         102       0.19      0.36      0.25        50\n",
      "         103       0.47      0.44      0.45        50\n",
      "         104       0.16      0.20      0.18        50\n",
      "         105       0.47      0.44      0.45        50\n",
      "         106       0.20      0.30      0.24        50\n",
      "         107       0.22      0.08      0.12        50\n",
      "         108       0.17      0.32      0.23        50\n",
      "         109       0.42      0.10      0.16        50\n",
      "         110       0.33      0.40      0.36        50\n",
      "         111       0.86      0.12      0.21        50\n",
      "         112       0.36      0.48      0.41        50\n",
      "         113       0.21      0.08      0.12        50\n",
      "         114       0.31      0.20      0.24        50\n",
      "         115       0.54      0.52      0.53        50\n",
      "         116       0.77      0.20      0.32        50\n",
      "         117       0.28      0.18      0.22        50\n",
      "         118       0.18      0.16      0.17        50\n",
      "         119       0.22      0.08      0.12        50\n",
      "         120       0.56      0.46      0.51        50\n",
      "         121       0.30      0.06      0.10        50\n",
      "         122       0.24      0.08      0.12        50\n",
      "         123       0.10      0.08      0.09        50\n",
      "         124       0.66      0.46      0.54        50\n",
      "         125       0.40      0.34      0.37        50\n",
      "         126       0.39      0.42      0.40        50\n",
      "         127       0.27      0.18      0.22        50\n",
      "         128       0.08      0.20      0.12        50\n",
      "         129       0.47      0.14      0.22        50\n",
      "         130       0.23      0.46      0.31        50\n",
      "         131       0.24      0.30      0.27        50\n",
      "         132       0.25      0.18      0.21        50\n",
      "         133       0.09      0.10      0.10        50\n",
      "         134       0.36      0.16      0.22        50\n",
      "         135       0.39      0.26      0.31        50\n",
      "         136       0.39      0.38      0.38        50\n",
      "         137       0.74      0.28      0.41        50\n",
      "         138       0.65      0.34      0.45        50\n",
      "         139       0.12      0.20      0.15        50\n",
      "         140       0.62      0.16      0.25        50\n",
      "         141       0.27      0.14      0.18        50\n",
      "         142       0.41      0.22      0.29        50\n",
      "         143       0.47      0.30      0.37        50\n",
      "         144       0.45      0.20      0.28        50\n",
      "         145       0.28      0.28      0.28        50\n",
      "         146       0.22      0.34      0.27        50\n",
      "         147       0.34      0.80      0.47        50\n",
      "         148       0.07      0.08      0.07        50\n",
      "         149       0.21      0.16      0.18        50\n",
      "         150       0.80      0.24      0.37        50\n",
      "         151       0.59      0.48      0.53        50\n",
      "         152       0.64      0.64      0.64        50\n",
      "         153       0.31      0.18      0.23        50\n",
      "         154       0.26      0.58      0.36        50\n",
      "         155       0.86      0.12      0.21        50\n",
      "         156       0.19      0.32      0.24        50\n",
      "         157       0.24      0.36      0.29        50\n",
      "         158       0.28      0.28      0.28        50\n",
      "         159       0.43      0.30      0.35        50\n",
      "         160       0.38      0.20      0.26        50\n",
      "         161       0.19      0.44      0.27        50\n",
      "         162       0.29      0.48      0.36        50\n",
      "         163       0.10      0.16      0.12        50\n",
      "         164       0.62      0.64      0.63        50\n",
      "         165       0.00      0.00      0.00        50\n",
      "         166       0.35      0.72      0.47        50\n",
      "         167       0.39      0.34      0.36        50\n",
      "         168       0.43      0.32      0.37        50\n",
      "         169       0.21      0.14      0.17        50\n",
      "         170       0.37      0.26      0.31        50\n",
      "         171       0.81      0.50      0.62        50\n",
      "         172       0.11      0.30      0.16        50\n",
      "         173       0.23      0.28      0.25        50\n",
      "         174       0.30      0.18      0.23        50\n",
      "         175       0.45      0.42      0.43        50\n",
      "         176       0.34      0.26      0.30        50\n",
      "         177       0.21      0.46      0.29        50\n",
      "         178       0.49      0.48      0.48        50\n",
      "         179       0.26      0.16      0.20        50\n",
      "         180       0.22      0.34      0.27        50\n",
      "         181       0.34      0.64      0.44        50\n",
      "         182       0.32      0.54      0.40        50\n",
      "         183       0.15      0.18      0.16        50\n",
      "         184       0.59      0.58      0.59        50\n",
      "         185       0.33      0.36      0.35        50\n",
      "         186       0.34      0.30      0.32        50\n",
      "         187       0.79      0.66      0.72        50\n",
      "         188       0.44      0.16      0.24        50\n",
      "         189       0.18      0.04      0.07        50\n",
      "         190       0.10      0.04      0.06        50\n",
      "         191       0.55      0.66      0.60        50\n",
      "         192       0.21      0.38      0.27        50\n",
      "         193       0.65      0.56      0.60        50\n",
      "         194       0.12      0.12      0.12        50\n",
      "         195       0.29      0.20      0.24        50\n",
      "         196       0.31      0.24      0.27        50\n",
      "         197       0.22      0.34      0.27        50\n",
      "         198       0.13      0.30      0.18        50\n",
      "         199       0.54      0.38      0.45        50\n",
      "\n",
      "    accuracy                           0.31     10000\n",
      "   macro avg       0.35      0.31      0.31     10000\n",
      "weighted avg       0.35      0.31      0.31     10000\n",
      "\n",
      "Epoch 6 - Train Loss: 1.4053115366822313 - Val Loss: 3.357293267793293 - Acc: 0.3108 - F1: 0.30631604850196426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f2ded2775b4e3bad997a8ed7c0bb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.48      0.53        50\n",
      "           1       0.20      0.20      0.20        50\n",
      "           2       0.36      0.24      0.29        50\n",
      "           3       0.48      0.44      0.46        50\n",
      "           4       0.15      0.12      0.13        50\n",
      "           5       0.21      0.14      0.17        50\n",
      "           6       0.03      0.02      0.03        50\n",
      "           7       0.35      0.26      0.30        50\n",
      "           8       0.37      0.58      0.45        50\n",
      "           9       0.08      0.24      0.12        50\n",
      "          10       0.24      0.20      0.22        50\n",
      "          11       0.41      0.26      0.32        50\n",
      "          12       0.25      0.32      0.28        50\n",
      "          13       0.26      0.24      0.25        50\n",
      "          14       0.66      0.62      0.64        50\n",
      "          15       0.26      0.42      0.32        50\n",
      "          16       0.58      0.64      0.61        50\n",
      "          17       0.23      0.46      0.31        50\n",
      "          18       0.25      0.42      0.32        50\n",
      "          19       0.28      0.48      0.36        50\n",
      "          20       0.09      0.22      0.12        50\n",
      "          21       0.39      0.54      0.45        50\n",
      "          22       0.33      0.08      0.13        50\n",
      "          23       0.69      0.36      0.47        50\n",
      "          24       0.20      0.50      0.28        50\n",
      "          25       0.36      0.16      0.22        50\n",
      "          26       0.35      0.16      0.22        50\n",
      "          27       0.50      0.18      0.26        50\n",
      "          28       0.65      0.26      0.37        50\n",
      "          29       0.46      0.12      0.19        50\n",
      "          30       0.19      0.14      0.16        50\n",
      "          31       0.38      0.30      0.33        50\n",
      "          32       0.45      0.30      0.36        50\n",
      "          33       0.38      0.22      0.28        50\n",
      "          34       0.25      0.52      0.34        50\n",
      "          35       0.68      0.46      0.55        50\n",
      "          36       0.23      0.28      0.25        50\n",
      "          37       0.30      0.40      0.34        50\n",
      "          38       0.16      0.14      0.15        50\n",
      "          39       0.39      0.44      0.41        50\n",
      "          40       0.65      0.56      0.60        50\n",
      "          41       0.27      0.20      0.23        50\n",
      "          42       0.14      0.12      0.13        50\n",
      "          43       0.81      0.52      0.63        50\n",
      "          44       0.18      0.44      0.25        50\n",
      "          45       0.32      0.12      0.17        50\n",
      "          46       0.44      0.16      0.24        50\n",
      "          47       0.22      0.26      0.24        50\n",
      "          48       0.51      0.50      0.51        50\n",
      "          49       0.32      0.16      0.21        50\n",
      "          50       0.64      0.58      0.61        50\n",
      "          51       0.61      0.56      0.58        50\n",
      "          52       0.07      0.04      0.05        50\n",
      "          53       0.10      0.10      0.10        50\n",
      "          54       0.21      0.36      0.26        50\n",
      "          55       0.28      0.28      0.28        50\n",
      "          56       0.31      0.30      0.31        50\n",
      "          57       0.15      0.22      0.18        50\n",
      "          58       0.27      0.42      0.33        50\n",
      "          59       0.29      0.30      0.30        50\n",
      "          60       0.47      0.42      0.44        50\n",
      "          61       0.43      0.32      0.37        50\n",
      "          62       0.49      0.50      0.50        50\n",
      "          63       0.54      0.28      0.37        50\n",
      "          64       0.34      0.22      0.27        50\n",
      "          65       0.55      0.48      0.51        50\n",
      "          66       0.14      0.06      0.08        50\n",
      "          67       0.32      0.16      0.21        50\n",
      "          68       0.53      0.36      0.43        50\n",
      "          69       0.34      0.40      0.37        50\n",
      "          70       0.33      0.58      0.42        50\n",
      "          71       0.54      0.40      0.46        50\n",
      "          72       0.67      0.60      0.63        50\n",
      "          73       0.13      0.04      0.06        50\n",
      "          74       0.47      0.62      0.53        50\n",
      "          75       0.43      0.56      0.49        50\n",
      "          76       0.21      0.18      0.20        50\n",
      "          77       0.30      0.20      0.24        50\n",
      "          78       0.32      0.52      0.40        50\n",
      "          79       0.18      0.18      0.18        50\n",
      "          80       0.30      0.44      0.36        50\n",
      "          81       0.84      0.54      0.66        50\n",
      "          82       0.31      0.22      0.26        50\n",
      "          83       0.33      0.22      0.27        50\n",
      "          84       0.15      0.50      0.23        50\n",
      "          85       0.25      0.16      0.20        50\n",
      "          86       0.20      0.14      0.16        50\n",
      "          87       0.33      0.26      0.29        50\n",
      "          88       0.11      0.10      0.11        50\n",
      "          89       0.40      0.34      0.37        50\n",
      "          90       0.17      0.22      0.19        50\n",
      "          91       0.22      0.26      0.24        50\n",
      "          92       0.28      0.30      0.29        50\n",
      "          93       0.12      0.04      0.06        50\n",
      "          94       0.10      0.14      0.11        50\n",
      "          95       0.26      0.44      0.33        50\n",
      "          96       0.46      0.36      0.40        50\n",
      "          97       0.47      0.58      0.52        50\n",
      "          98       0.22      0.42      0.29        50\n",
      "          99       0.48      0.24      0.32        50\n",
      "         100       0.30      0.34      0.32        50\n",
      "         101       0.47      0.14      0.22        50\n",
      "         102       0.23      0.22      0.23        50\n",
      "         103       0.18      0.50      0.27        50\n",
      "         104       0.33      0.30      0.31        50\n",
      "         105       0.22      0.52      0.31        50\n",
      "         106       0.38      0.36      0.37        50\n",
      "         107       0.30      0.18      0.23        50\n",
      "         108       0.20      0.32      0.25        50\n",
      "         109       0.23      0.24      0.23        50\n",
      "         110       0.29      0.48      0.36        50\n",
      "         111       0.67      0.24      0.35        50\n",
      "         112       0.36      0.40      0.38        50\n",
      "         113       0.17      0.14      0.16        50\n",
      "         114       0.22      0.24      0.23        50\n",
      "         115       0.60      0.50      0.54        50\n",
      "         116       0.30      0.62      0.41        50\n",
      "         117       0.30      0.18      0.23        50\n",
      "         118       0.29      0.08      0.12        50\n",
      "         119       0.21      0.14      0.17        50\n",
      "         120       0.46      0.52      0.49        50\n",
      "         121       0.10      0.04      0.06        50\n",
      "         122       0.44      0.24      0.31        50\n",
      "         123       0.08      0.04      0.05        50\n",
      "         124       0.59      0.52      0.55        50\n",
      "         125       0.50      0.32      0.39        50\n",
      "         126       0.26      0.40      0.32        50\n",
      "         127       0.23      0.38      0.29        50\n",
      "         128       0.26      0.14      0.18        50\n",
      "         129       0.20      0.34      0.25        50\n",
      "         130       0.36      0.32      0.34        50\n",
      "         131       0.35      0.34      0.34        50\n",
      "         132       0.28      0.18      0.22        50\n",
      "         133       0.09      0.14      0.11        50\n",
      "         134       0.24      0.14      0.18        50\n",
      "         135       0.26      0.44      0.33        50\n",
      "         136       0.53      0.46      0.49        50\n",
      "         137       0.50      0.40      0.44        50\n",
      "         138       0.50      0.32      0.39        50\n",
      "         139       0.25      0.30      0.27        50\n",
      "         140       0.42      0.20      0.27        50\n",
      "         141       0.11      0.26      0.15        50\n",
      "         142       0.55      0.34      0.42        50\n",
      "         143       0.28      0.48      0.36        50\n",
      "         144       0.33      0.32      0.32        50\n",
      "         145       0.23      0.34      0.27        50\n",
      "         146       0.18      0.30      0.22        50\n",
      "         147       0.55      0.60      0.57        50\n",
      "         148       0.06      0.04      0.05        50\n",
      "         149       0.23      0.14      0.17        50\n",
      "         150       0.37      0.50      0.43        50\n",
      "         151       0.80      0.64      0.71        50\n",
      "         152       1.00      0.38      0.55        50\n",
      "         153       0.31      0.22      0.26        50\n",
      "         154       0.33      0.20      0.25        50\n",
      "         155       0.70      0.32      0.44        50\n",
      "         156       0.39      0.14      0.21        50\n",
      "         157       0.38      0.18      0.24        50\n",
      "         158       0.29      0.30      0.30        50\n",
      "         159       0.34      0.44      0.39        50\n",
      "         160       0.47      0.28      0.35        50\n",
      "         161       0.31      0.22      0.26        50\n",
      "         162       0.32      0.18      0.23        50\n",
      "         163       0.11      0.26      0.16        50\n",
      "         164       0.59      0.66      0.62        50\n",
      "         165       0.17      0.16      0.17        50\n",
      "         166       0.73      0.60      0.66        50\n",
      "         167       0.50      0.36      0.42        50\n",
      "         168       0.35      0.36      0.36        50\n",
      "         169       0.23      0.18      0.20        50\n",
      "         170       0.45      0.18      0.26        50\n",
      "         171       0.56      0.44      0.49        50\n",
      "         172       0.16      0.16      0.16        50\n",
      "         173       0.57      0.16      0.25        50\n",
      "         174       0.46      0.26      0.33        50\n",
      "         175       0.56      0.38      0.45        50\n",
      "         176       0.31      0.44      0.36        50\n",
      "         177       0.23      0.32      0.27        50\n",
      "         178       0.58      0.42      0.49        50\n",
      "         179       0.23      0.22      0.23        50\n",
      "         180       0.62      0.20      0.30        50\n",
      "         181       0.56      0.44      0.49        50\n",
      "         182       0.45      0.36      0.40        50\n",
      "         183       0.36      0.16      0.22        50\n",
      "         184       0.66      0.62      0.64        50\n",
      "         185       0.25      0.32      0.28        50\n",
      "         186       0.41      0.30      0.34        50\n",
      "         187       0.47      0.70      0.56        50\n",
      "         188       0.62      0.30      0.41        50\n",
      "         189       0.06      0.02      0.03        50\n",
      "         190       0.08      0.08      0.08        50\n",
      "         191       0.52      0.62      0.56        50\n",
      "         192       0.25      0.20      0.22        50\n",
      "         193       0.47      0.58      0.52        50\n",
      "         194       0.11      0.18      0.14        50\n",
      "         195       0.16      0.22      0.18        50\n",
      "         196       0.21      0.22      0.21        50\n",
      "         197       0.25      0.26      0.26        50\n",
      "         198       0.11      0.16      0.13        50\n",
      "         199       0.49      0.36      0.41        50\n",
      "\n",
      "    accuracy                           0.31     10000\n",
      "   macro avg       0.35      0.31      0.31     10000\n",
      "weighted avg       0.35      0.31      0.31     10000\n",
      "\n",
      "Epoch 7 - Train Loss: 0.9160095868665544 - Val Loss: 3.630089542533778 - Acc: 0.3138 - F1: 0.3130345633125282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7b8daecdcc41d5b2ca6e024b20e7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.60      0.62        50\n",
      "           1       0.29      0.14      0.19        50\n",
      "           2       0.32      0.32      0.32        50\n",
      "           3       0.28      0.44      0.34        50\n",
      "           4       0.16      0.24      0.19        50\n",
      "           5       0.09      0.10      0.10        50\n",
      "           6       0.11      0.08      0.09        50\n",
      "           7       0.40      0.34      0.37        50\n",
      "           8       0.36      0.42      0.39        50\n",
      "           9       0.13      0.14      0.13        50\n",
      "          10       0.29      0.28      0.28        50\n",
      "          11       0.35      0.32      0.33        50\n",
      "          12       0.30      0.32      0.31        50\n",
      "          13       0.24      0.38      0.29        50\n",
      "          14       0.64      0.74      0.69        50\n",
      "          15       0.23      0.28      0.25        50\n",
      "          16       0.66      0.58      0.62        50\n",
      "          17       0.43      0.26      0.33        50\n",
      "          18       0.36      0.32      0.34        50\n",
      "          19       0.32      0.36      0.34        50\n",
      "          20       0.08      0.14      0.10        50\n",
      "          21       0.57      0.46      0.51        50\n",
      "          22       0.12      0.12      0.12        50\n",
      "          23       0.62      0.32      0.42        50\n",
      "          24       0.26      0.48      0.33        50\n",
      "          25       0.32      0.20      0.25        50\n",
      "          26       0.27      0.30      0.29        50\n",
      "          27       0.27      0.26      0.27        50\n",
      "          28       0.50      0.48      0.49        50\n",
      "          29       0.28      0.24      0.26        50\n",
      "          30       0.07      0.10      0.08        50\n",
      "          31       0.31      0.32      0.31        50\n",
      "          32       0.28      0.28      0.28        50\n",
      "          33       0.44      0.32      0.37        50\n",
      "          34       0.49      0.42      0.45        50\n",
      "          35       0.62      0.56      0.59        50\n",
      "          36       0.22      0.16      0.19        50\n",
      "          37       0.32      0.30      0.31        50\n",
      "          38       0.18      0.12      0.14        50\n",
      "          39       0.57      0.42      0.48        50\n",
      "          40       0.64      0.70      0.67        50\n",
      "          41       0.24      0.22      0.23        50\n",
      "          42       0.07      0.06      0.07        50\n",
      "          43       0.47      0.54      0.50        50\n",
      "          44       0.30      0.26      0.28        50\n",
      "          45       0.25      0.22      0.23        50\n",
      "          46       0.38      0.28      0.32        50\n",
      "          47       0.39      0.18      0.25        50\n",
      "          48       0.54      0.62      0.58        50\n",
      "          49       0.26      0.26      0.26        50\n",
      "          50       0.82      0.54      0.65        50\n",
      "          51       0.51      0.62      0.56        50\n",
      "          52       0.05      0.04      0.04        50\n",
      "          53       0.10      0.06      0.08        50\n",
      "          54       0.19      0.42      0.26        50\n",
      "          55       0.31      0.36      0.33        50\n",
      "          56       0.39      0.40      0.40        50\n",
      "          57       0.24      0.28      0.26        50\n",
      "          58       0.28      0.46      0.35        50\n",
      "          59       0.35      0.28      0.31        50\n",
      "          60       0.59      0.40      0.48        50\n",
      "          61       0.45      0.42      0.43        50\n",
      "          62       0.43      0.62      0.51        50\n",
      "          63       0.45      0.34      0.39        50\n",
      "          64       0.25      0.32      0.28        50\n",
      "          65       0.69      0.40      0.51        50\n",
      "          66       0.24      0.16      0.19        50\n",
      "          67       0.36      0.18      0.24        50\n",
      "          68       0.61      0.38      0.47        50\n",
      "          69       0.35      0.52      0.42        50\n",
      "          70       0.37      0.58      0.45        50\n",
      "          71       0.42      0.40      0.41        50\n",
      "          72       0.66      0.58      0.62        50\n",
      "          73       0.12      0.22      0.16        50\n",
      "          74       0.37      0.46      0.41        50\n",
      "          75       0.54      0.42      0.47        50\n",
      "          76       0.21      0.18      0.20        50\n",
      "          77       0.28      0.28      0.28        50\n",
      "          78       0.27      0.34      0.30        50\n",
      "          79       0.19      0.18      0.18        50\n",
      "          80       0.32      0.50      0.39        50\n",
      "          81       0.61      0.62      0.61        50\n",
      "          82       0.24      0.32      0.27        50\n",
      "          83       0.25      0.28      0.26        50\n",
      "          84       0.30      0.12      0.17        50\n",
      "          85       0.21      0.14      0.17        50\n",
      "          86       0.13      0.18      0.15        50\n",
      "          87       0.38      0.30      0.33        50\n",
      "          88       0.17      0.14      0.15        50\n",
      "          89       0.59      0.46      0.52        50\n",
      "          90       0.18      0.22      0.20        50\n",
      "          91       0.26      0.32      0.29        50\n",
      "          92       0.34      0.40      0.37        50\n",
      "          93       0.15      0.28      0.20        50\n",
      "          94       0.08      0.06      0.07        50\n",
      "          95       0.36      0.28      0.31        50\n",
      "          96       0.59      0.32      0.42        50\n",
      "          97       0.41      0.60      0.48        50\n",
      "          98       0.33      0.22      0.27        50\n",
      "          99       0.38      0.32      0.35        50\n",
      "         100       0.32      0.34      0.33        50\n",
      "         101       0.22      0.20      0.21        50\n",
      "         102       0.21      0.18      0.20        50\n",
      "         103       0.24      0.52      0.33        50\n",
      "         104       0.27      0.26      0.26        50\n",
      "         105       0.44      0.38      0.41        50\n",
      "         106       0.49      0.34      0.40        50\n",
      "         107       0.22      0.16      0.19        50\n",
      "         108       0.30      0.26      0.28        50\n",
      "         109       0.39      0.30      0.34        50\n",
      "         110       0.46      0.42      0.44        50\n",
      "         111       0.43      0.38      0.40        50\n",
      "         112       0.49      0.50      0.50        50\n",
      "         113       0.22      0.20      0.21        50\n",
      "         114       0.29      0.30      0.30        50\n",
      "         115       0.69      0.44      0.54        50\n",
      "         116       0.31      0.60      0.41        50\n",
      "         117       0.26      0.20      0.22        50\n",
      "         118       0.21      0.24      0.22        50\n",
      "         119       0.13      0.12      0.12        50\n",
      "         120       0.45      0.50      0.48        50\n",
      "         121       0.14      0.10      0.12        50\n",
      "         122       0.26      0.16      0.20        50\n",
      "         123       0.20      0.08      0.11        50\n",
      "         124       0.55      0.62      0.58        50\n",
      "         125       0.42      0.28      0.34        50\n",
      "         126       0.56      0.38      0.45        50\n",
      "         127       0.29      0.16      0.21        50\n",
      "         128       0.42      0.22      0.29        50\n",
      "         129       0.25      0.30      0.27        50\n",
      "         130       0.28      0.32      0.30        50\n",
      "         131       0.48      0.28      0.35        50\n",
      "         132       0.31      0.20      0.24        50\n",
      "         133       0.04      0.14      0.07        50\n",
      "         134       0.36      0.26      0.30        50\n",
      "         135       0.43      0.20      0.27        50\n",
      "         136       0.46      0.46      0.46        50\n",
      "         137       0.42      0.44      0.43        50\n",
      "         138       0.39      0.48      0.43        50\n",
      "         139       0.41      0.18      0.25        50\n",
      "         140       0.44      0.36      0.40        50\n",
      "         141       0.30      0.30      0.30        50\n",
      "         142       0.45      0.40      0.43        50\n",
      "         143       0.45      0.34      0.39        50\n",
      "         144       0.21      0.42      0.28        50\n",
      "         145       0.39      0.14      0.21        50\n",
      "         146       0.18      0.24      0.21        50\n",
      "         147       0.41      0.60      0.48        50\n",
      "         148       0.12      0.12      0.12        50\n",
      "         149       0.11      0.14      0.13        50\n",
      "         150       0.53      0.64      0.58        50\n",
      "         151       0.79      0.62      0.70        50\n",
      "         152       0.49      0.72      0.58        50\n",
      "         153       0.35      0.30      0.32        50\n",
      "         154       0.38      0.46      0.41        50\n",
      "         155       0.42      0.34      0.38        50\n",
      "         156       0.30      0.20      0.24        50\n",
      "         157       0.41      0.26      0.32        50\n",
      "         158       0.25      0.22      0.23        50\n",
      "         159       0.37      0.38      0.37        50\n",
      "         160       0.28      0.28      0.28        50\n",
      "         161       0.26      0.44      0.33        50\n",
      "         162       0.32      0.42      0.36        50\n",
      "         163       0.26      0.14      0.18        50\n",
      "         164       0.79      0.68      0.73        50\n",
      "         165       0.16      0.10      0.12        50\n",
      "         166       0.68      0.60      0.64        50\n",
      "         167       0.38      0.46      0.42        50\n",
      "         168       0.41      0.42      0.42        50\n",
      "         169       0.35      0.22      0.27        50\n",
      "         170       0.42      0.34      0.38        50\n",
      "         171       0.47      0.62      0.53        50\n",
      "         172       0.13      0.16      0.14        50\n",
      "         173       0.32      0.32      0.32        50\n",
      "         174       0.28      0.32      0.30        50\n",
      "         175       0.50      0.40      0.44        50\n",
      "         176       0.26      0.36      0.31        50\n",
      "         177       0.35      0.44      0.39        50\n",
      "         178       0.35      0.58      0.43        50\n",
      "         179       0.21      0.16      0.18        50\n",
      "         180       0.30      0.22      0.25        50\n",
      "         181       0.54      0.42      0.47        50\n",
      "         182       0.35      0.46      0.40        50\n",
      "         183       0.17      0.18      0.18        50\n",
      "         184       0.67      0.62      0.65        50\n",
      "         185       0.33      0.38      0.35        50\n",
      "         186       0.41      0.36      0.38        50\n",
      "         187       0.73      0.54      0.62        50\n",
      "         188       0.29      0.34      0.31        50\n",
      "         189       0.10      0.16      0.13        50\n",
      "         190       0.08      0.12      0.10        50\n",
      "         191       0.54      0.74      0.63        50\n",
      "         192       0.24      0.18      0.21        50\n",
      "         193       0.76      0.58      0.66        50\n",
      "         194       0.23      0.10      0.14        50\n",
      "         195       0.27      0.34      0.30        50\n",
      "         196       0.23      0.32      0.27        50\n",
      "         197       0.47      0.30      0.37        50\n",
      "         198       0.16      0.10      0.12        50\n",
      "         199       0.68      0.38      0.49        50\n",
      "\n",
      "    accuracy                           0.33     10000\n",
      "   macro avg       0.35      0.33      0.33     10000\n",
      "weighted avg       0.35      0.33      0.33     10000\n",
      "\n",
      "Epoch 8 - Train Loss: 0.5044448503562252 - Val Loss: 3.907054022897648 - Acc: 0.3326 - F1: 0.3325027143824533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6e7f52c93242b5a87865f491688e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.52      0.57        50\n",
      "           1       0.12      0.14      0.13        50\n",
      "           2       0.23      0.22      0.23        50\n",
      "           3       0.31      0.42      0.36        50\n",
      "           4       0.13      0.16      0.14        50\n",
      "           5       0.15      0.16      0.16        50\n",
      "           6       0.07      0.04      0.05        50\n",
      "           7       0.27      0.32      0.29        50\n",
      "           8       0.57      0.34      0.42        50\n",
      "           9       0.24      0.10      0.14        50\n",
      "          10       0.29      0.20      0.24        50\n",
      "          11       0.16      0.38      0.22        50\n",
      "          12       0.38      0.24      0.29        50\n",
      "          13       0.26      0.20      0.22        50\n",
      "          14       0.79      0.54      0.64        50\n",
      "          15       0.24      0.32      0.27        50\n",
      "          16       0.31      0.70      0.43        50\n",
      "          17       0.19      0.42      0.26        50\n",
      "          18       0.27      0.34      0.30        50\n",
      "          19       0.48      0.28      0.35        50\n",
      "          20       0.24      0.10      0.14        50\n",
      "          21       0.57      0.40      0.47        50\n",
      "          22       0.18      0.24      0.21        50\n",
      "          23       0.38      0.64      0.48        50\n",
      "          24       0.37      0.38      0.37        50\n",
      "          25       0.25      0.20      0.22        50\n",
      "          26       0.31      0.24      0.27        50\n",
      "          27       0.39      0.24      0.30        50\n",
      "          28       0.65      0.26      0.37        50\n",
      "          29       0.22      0.30      0.26        50\n",
      "          30       0.06      0.12      0.08        50\n",
      "          31       0.20      0.18      0.19        50\n",
      "          32       0.24      0.22      0.23        50\n",
      "          33       0.25      0.12      0.16        50\n",
      "          34       0.51      0.50      0.51        50\n",
      "          35       0.48      0.54      0.51        50\n",
      "          36       0.33      0.10      0.15        50\n",
      "          37       0.35      0.34      0.34        50\n",
      "          38       0.21      0.18      0.19        50\n",
      "          39       0.48      0.42      0.45        50\n",
      "          40       0.57      0.64      0.60        50\n",
      "          41       0.32      0.24      0.27        50\n",
      "          42       0.13      0.12      0.12        50\n",
      "          43       0.55      0.42      0.48        50\n",
      "          44       0.31      0.20      0.24        50\n",
      "          45       0.25      0.14      0.18        50\n",
      "          46       0.28      0.22      0.25        50\n",
      "          47       0.12      0.26      0.17        50\n",
      "          48       0.62      0.60      0.61        50\n",
      "          49       0.20      0.28      0.23        50\n",
      "          50       0.82      0.54      0.65        50\n",
      "          51       0.53      0.48      0.51        50\n",
      "          52       0.04      0.04      0.04        50\n",
      "          53       0.13      0.14      0.13        50\n",
      "          54       0.30      0.16      0.21        50\n",
      "          55       0.37      0.34      0.35        50\n",
      "          56       0.30      0.52      0.38        50\n",
      "          57       0.10      0.20      0.13        50\n",
      "          58       0.26      0.28      0.27        50\n",
      "          59       0.43      0.48      0.45        50\n",
      "          60       0.54      0.44      0.48        50\n",
      "          61       0.34      0.28      0.31        50\n",
      "          62       0.66      0.38      0.48        50\n",
      "          63       0.32      0.58      0.41        50\n",
      "          64       0.33      0.30      0.31        50\n",
      "          65       0.56      0.46      0.51        50\n",
      "          66       0.12      0.12      0.12        50\n",
      "          67       0.24      0.24      0.24        50\n",
      "          68       0.57      0.46      0.51        50\n",
      "          69       0.50      0.36      0.42        50\n",
      "          70       0.47      0.30      0.37        50\n",
      "          71       0.38      0.24      0.29        50\n",
      "          72       0.75      0.60      0.67        50\n",
      "          73       0.29      0.20      0.24        50\n",
      "          74       0.46      0.44      0.45        50\n",
      "          75       0.42      0.62      0.50        50\n",
      "          76       0.13      0.16      0.14        50\n",
      "          77       0.29      0.14      0.19        50\n",
      "          78       0.35      0.36      0.36        50\n",
      "          79       0.12      0.22      0.15        50\n",
      "          80       0.35      0.36      0.36        50\n",
      "          81       0.67      0.62      0.65        50\n",
      "          82       0.22      0.28      0.25        50\n",
      "          83       0.32      0.20      0.25        50\n",
      "          84       0.33      0.16      0.22        50\n",
      "          85       0.16      0.44      0.23        50\n",
      "          86       0.14      0.12      0.13        50\n",
      "          87       0.24      0.38      0.30        50\n",
      "          88       0.19      0.10      0.13        50\n",
      "          89       0.47      0.40      0.43        50\n",
      "          90       0.15      0.24      0.18        50\n",
      "          91       0.25      0.06      0.10        50\n",
      "          92       0.51      0.36      0.42        50\n",
      "          93       0.26      0.20      0.23        50\n",
      "          94       0.17      0.16      0.16        50\n",
      "          95       0.30      0.16      0.21        50\n",
      "          96       0.30      0.62      0.40        50\n",
      "          97       0.68      0.52      0.59        50\n",
      "          98       0.29      0.18      0.22        50\n",
      "          99       0.38      0.22      0.28        50\n",
      "         100       0.27      0.24      0.26        50\n",
      "         101       0.16      0.16      0.16        50\n",
      "         102       0.37      0.20      0.26        50\n",
      "         103       0.46      0.42      0.44        50\n",
      "         104       0.31      0.18      0.23        50\n",
      "         105       0.44      0.30      0.36        50\n",
      "         106       0.52      0.28      0.36        50\n",
      "         107       0.16      0.12      0.14        50\n",
      "         108       0.24      0.24      0.24        50\n",
      "         109       0.26      0.20      0.22        50\n",
      "         110       0.70      0.28      0.40        50\n",
      "         111       0.14      0.54      0.22        50\n",
      "         112       0.58      0.36      0.44        50\n",
      "         113       0.16      0.26      0.20        50\n",
      "         114       0.29      0.18      0.22        50\n",
      "         115       0.53      0.38      0.44        50\n",
      "         116       0.42      0.42      0.42        50\n",
      "         117       0.19      0.18      0.19        50\n",
      "         118       0.14      0.42      0.22        50\n",
      "         119       0.12      0.16      0.14        50\n",
      "         120       0.59      0.34      0.43        50\n",
      "         121       0.05      0.04      0.04        50\n",
      "         122       0.20      0.20      0.20        50\n",
      "         123       0.15      0.12      0.13        50\n",
      "         124       0.58      0.56      0.57        50\n",
      "         125       0.38      0.26      0.31        50\n",
      "         126       0.50      0.24      0.32        50\n",
      "         127       0.27      0.16      0.20        50\n",
      "         128       0.29      0.36      0.32        50\n",
      "         129       0.29      0.32      0.30        50\n",
      "         130       0.45      0.30      0.36        50\n",
      "         131       0.66      0.38      0.48        50\n",
      "         132       0.18      0.12      0.14        50\n",
      "         133       0.08      0.10      0.09        50\n",
      "         134       0.28      0.42      0.34        50\n",
      "         135       0.30      0.38      0.34        50\n",
      "         136       0.64      0.46      0.53        50\n",
      "         137       0.58      0.36      0.44        50\n",
      "         138       0.53      0.42      0.47        50\n",
      "         139       0.23      0.18      0.20        50\n",
      "         140       0.33      0.24      0.28        50\n",
      "         141       0.18      0.10      0.13        50\n",
      "         142       0.50      0.06      0.11        50\n",
      "         143       0.47      0.44      0.45        50\n",
      "         144       0.32      0.32      0.32        50\n",
      "         145       0.19      0.24      0.21        50\n",
      "         146       0.25      0.18      0.21        50\n",
      "         147       0.44      0.48      0.46        50\n",
      "         148       0.08      0.10      0.09        50\n",
      "         149       0.08      0.14      0.10        50\n",
      "         150       0.53      0.42      0.47        50\n",
      "         151       0.69      0.70      0.69        50\n",
      "         152       0.52      0.82      0.64        50\n",
      "         153       0.25      0.18      0.21        50\n",
      "         154       0.33      0.34      0.34        50\n",
      "         155       0.38      0.28      0.32        50\n",
      "         156       0.29      0.12      0.17        50\n",
      "         157       0.50      0.26      0.34        50\n",
      "         158       0.40      0.28      0.33        50\n",
      "         159       0.49      0.34      0.40        50\n",
      "         160       0.30      0.28      0.29        50\n",
      "         161       0.18      0.38      0.25        50\n",
      "         162       0.28      0.40      0.33        50\n",
      "         163       0.18      0.12      0.14        50\n",
      "         164       0.62      0.66      0.64        50\n",
      "         165       0.06      0.10      0.07        50\n",
      "         166       0.58      0.62      0.60        50\n",
      "         167       0.60      0.24      0.34        50\n",
      "         168       0.29      0.38      0.33        50\n",
      "         169       0.14      0.32      0.20        50\n",
      "         170       0.29      0.30      0.30        50\n",
      "         171       0.64      0.36      0.46        50\n",
      "         172       0.12      0.12      0.12        50\n",
      "         173       0.29      0.18      0.22        50\n",
      "         174       0.21      0.36      0.26        50\n",
      "         175       0.44      0.46      0.45        50\n",
      "         176       0.25      0.32      0.28        50\n",
      "         177       0.58      0.22      0.32        50\n",
      "         178       0.39      0.60      0.48        50\n",
      "         179       0.26      0.24      0.25        50\n",
      "         180       0.19      0.50      0.27        50\n",
      "         181       0.55      0.42      0.48        50\n",
      "         182       0.38      0.54      0.44        50\n",
      "         183       0.18      0.18      0.18        50\n",
      "         184       0.60      0.60      0.60        50\n",
      "         185       0.53      0.20      0.29        50\n",
      "         186       0.39      0.36      0.38        50\n",
      "         187       0.51      0.64      0.57        50\n",
      "         188       0.44      0.30      0.36        50\n",
      "         189       0.12      0.10      0.11        50\n",
      "         190       0.17      0.16      0.17        50\n",
      "         191       0.58      0.62      0.60        50\n",
      "         192       0.26      0.18      0.21        50\n",
      "         193       0.62      0.56      0.59        50\n",
      "         194       0.17      0.12      0.14        50\n",
      "         195       0.27      0.28      0.27        50\n",
      "         196       0.24      0.38      0.30        50\n",
      "         197       0.53      0.16      0.25        50\n",
      "         198       0.17      0.18      0.18        50\n",
      "         199       0.52      0.46      0.49        50\n",
      "\n",
      "    accuracy                           0.31     10000\n",
      "   macro avg       0.34      0.31      0.31     10000\n",
      "weighted avg       0.34      0.31      0.31     10000\n",
      "\n",
      "Epoch 9 - Train Loss: 0.307904827539497 - Val Loss: 4.53510104553609 - Acc: 0.3081 - F1: 0.31017578001810525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe101c990eaa4c19ae232d1fddb93b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/arrow_dataset.py:2876\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2876\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2877\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/arrow_dataset.py:2872\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/arrow_dataset.py:2857\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2855\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2856\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2857\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:112\u001b[0m, in \u001b[0;36mTorchFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    110\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 112\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    114\u001b[0m     batch[column_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate(batch[column_name])\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/utils/py_utils.py:511\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m--> 511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/utils/py_utils.py:373\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function([data_struct])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/utils/py_utils.py:484\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    483\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 484\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    486\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/utils/py_utils.py:484\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    483\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 484\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    486\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_struct\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:  \u001b[38;5;66;03m# torch tensors cannot be instantied from an array of objects\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstruct\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:95\u001b[0m, in \u001b[0;36mTorchFormatter.recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/utils/py_utils.py:484\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    483\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 484\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    486\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:92\u001b[0m, in \u001b[0;36mTorchFormatter._recursive_tensorize\u001b[0;34m(self, data_struct)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/machine-unlearning/.venv/lib64/python3.11/site-packages/datasets/formatting/torch_formatter.py:78\u001b[0m, in \u001b[0;36mTorchFormatter._tensorize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     75\u001b[0m             value \u001b[38;5;241m=\u001b[39m value[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m     77\u001b[0m         value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_tensor_kwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss.append(0)\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"]\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            outputs = model(images)\n",
    "        except Exception as e:\n",
    "            print(images.dtype)\n",
    "            raise e\n",
    "        loss_val = loss(outputs, labels)\n",
    "        train_loss[-1] += loss_val.item()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss.append(0)\n",
    "    preds = []\n",
    "    labs = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            images = batch[\"image\"]\n",
    "            labels = batch[\"label\"]\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds.append(outputs.argmax(dim=1).cpu().numpy())\n",
    "            labs.append(labels.cpu().numpy())\n",
    "            loss_val = loss(outputs, labels)\n",
    "            val_loss[-1] += loss_val.item()\n",
    "\n",
    "    train_loss[-1] /= len(train_loader)\n",
    "    val_loss[-1] /= len(val_loader)\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    labs = np.concatenate(labs)\n",
    "    acc = accuracy_score(labs, preds)\n",
    "    f1 = f1_score(labs, preds, average=\"macro\")\n",
    "    print(classification_report(labs, preds))\n",
    "    print(\n",
    "        f\"Epoch {epoch} - Train Loss: {train_loss[-1]} - Val Loss: {val_loss[-1]} - Acc: {acc} - F1: {f1}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
